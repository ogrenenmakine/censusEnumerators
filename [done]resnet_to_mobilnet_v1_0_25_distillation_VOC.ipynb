{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.contrib import nn as nn_contrib\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(y, temperature=1.0):\n",
    "    exp = nd.exp(y / temperature)\n",
    "    partition = nd.sum(exp, axis=1).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pascal Voc Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from source.NACDVOCDetection import NACDDetection\n",
    "\n",
    "train_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'train'),(2007, 'trainval'), (2012, 'trainval')])\n",
    "val_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'val'),(2007, 'val')])\n",
    "test_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'test'),(2007, 'test')])\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Val images:', len(val_dataset))\n",
    "print('Test images:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 640, 640  # suppose we use 512 as base training size\n",
    "train_transform = presets.ssd.SSDDefaultTrainTransform(width, height)\n",
    "val_transform = presets.ssd.SSDDefaultValTransform(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "\n",
    "batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset.transform(val_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset.transform(val_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(test_loader):\n",
    "    if ib > 2:\n",
    "        break\n",
    "    print('data:', batch[0].shape, 'label:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv import model_zoo\n",
    "resnet50 = model_zoo.get_model('resnet50_v2', pretrained=True, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global alpha\n",
    "alpha = 0.25\n",
    "num_filters = int(32*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sampling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_layer(nfilters, stride, expension_constant):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(nn.Conv2D(nfilters, 3, strides=stride, padding=1, groups=nfilters, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(nfilters*expension_constant, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "def s16():\n",
    "    out = nn.HybridSequential()\n",
    "    # conv_0 layer\n",
    "    out.add(nn.Conv2D(num_filters, 3, strides=2, padding=1, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_1 layer\n",
    "    out.add(dp_layer(num_filters, 1, 2))\n",
    "    # conv_2 layer\n",
    "    out.add(dp_layer(num_filters*2, 2, 2))\n",
    "    # conv_3 layer\n",
    "    out.add(dp_layer(num_filters*4, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*4, 3, strides=2, padding=1, groups=num_filters*4, use_bias=False))\n",
    "    #out.load_parameters(\"weights/mobilenet_0_25_s16_org.params\")\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def s32():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*8, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_4_layer\n",
    "    out.add(dp_layer(num_filters*8, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*8, 3, strides=2, padding=1, groups=num_filters*8, use_bias=False))\n",
    "    #out.load_parameters(\"weights/mobilenet_0_25_s32_org.params\")\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def fc():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_5_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    # conv_6_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    # conv_7_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    # conv_8_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    # conv_9_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    # conv_10_layer\n",
    "    out.add(dp_layer(num_filters*16, 2, 2))\n",
    "    # conv_11_layer\n",
    "    out.add(dp_layer(num_filters*32, 1, 1))\n",
    "    out.add(nn.GlobalAvgPool2D())\n",
    "    out.add(nn.Flatten())\n",
    "    out.add(nn.Dense(1000))\n",
    "    #out.load_parameters(\"weights/mobilenet_0_25_fc_org.params\")\n",
    "    out.hybridize()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mobile(x, s16, s32, fc):\n",
    "    x = s16(x)\n",
    "    x = s32(x)\n",
    "    x = fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnet(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(mnet, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.s16 = s16()\n",
    "            self.s32 = s32()\n",
    "            self.fc = fc()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return forward_mobile(x, self.s16, self.s32, self.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sce = mx.gluon.loss.SoftmaxCrossEntropyLoss(from_logits=True, sparse_label=False)\n",
    "#l2 = mx.gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from mxnet import autograd as ag\n",
    "net_mobile = mnet()\n",
    "#net_mobile.save_parameters('process/net_mobile_temp_%d_init.params' % temperature)\n",
    "for temperature in range(12,24,4):\n",
    "    #for optimizer in (\"SGD\",\"RMSProp\",\"Adam\"):\n",
    "    for optimizer in (\"SGD\",):\n",
    "        for lr in range(1,2,1):\n",
    "            net_mobile.load_parameters('process/net_mobile_init.params')\n",
    "            print('temperature=%d #### optimizer=%s #### lr=%.3f ####\\n' % (temperature, optimizer, 0.1**lr))\n",
    "            net_mobile.collect_params().reset_ctx(ctx)\n",
    "            trainer = gluon.Trainer(net_mobile.collect_params(), optimizer, {'learning_rate': 0.1**lr, 'wd': 4e-5})\n",
    "            for epoch in range(start_epoch, epochs):\n",
    "                # reset iterator and tick\n",
    "                tic = time.time()\n",
    "                # iterate through all batch\n",
    "                train_loss = 0\n",
    "                train_mae = mx.metric.MAE()\n",
    "                for i, batch in enumerate(train_loader):\n",
    "                    x = batch[0].as_in_context(ctx)\n",
    "                    slbl = softmax(resnet50(x),temperature=temperature).detach()\n",
    "                    # record gradients\n",
    "                    with ag.record():\n",
    "                        p = softmax(net_mobile(x),temperature=temperature)\n",
    "                        rloss = sce(nd.log(p), slbl)\n",
    "                        train_loss += nd.sum(rloss).asscalar()\n",
    "                        train_mae.update(preds=p, labels=slbl)\n",
    "                        # backpropagate\n",
    "                        rloss.backward()\n",
    "                    # apply \n",
    "                    trainer.step(batch_size)\n",
    "                btic = time.time()\n",
    "                # iterate through all batch\n",
    "                val_loss = 0\n",
    "                val_mae = mx.metric.MAE()\n",
    "                for i, batch in enumerate(val_loader):\n",
    "                    x = batch[0].as_in_context(ctx)\n",
    "                    slbl = softmax(resnet50(x),temperature=1)\n",
    "                    p = softmax(net_mobile(x),temperature=1)\n",
    "                    rloss = sce(nd.log(p), slbl)\n",
    "                    val_loss += nd.sum(rloss).asscalar()\n",
    "                    val_mae.update(preds=p, labels=slbl)\n",
    "                # iterate through all batch\n",
    "                test_loss = 0\n",
    "                test_mae = mx.metric.MAE()\n",
    "                for i, batch in enumerate(test_loader):\n",
    "                    x = batch[0].as_in_context(ctx)\n",
    "                    slbl = softmax(resnet50(x),temperature=1)\n",
    "                    p = softmax(net_mobile(x),temperature=1)\n",
    "                    rloss = sce(nd.log(p), slbl)\n",
    "                    test_loss += nd.sum(rloss).asscalar()\n",
    "                    test_mae.update(preds=p, labels=slbl)\n",
    "                print(\"%3d;Loss:%f;Val_loss:%f;Test_loss:%f;Speed:%s;Train_mae:%.6e;Val_mae:%.6e;Test_mae:%.6e\" % (epoch, train_loss/len(train_dataset), val_loss/len(val_dataset), test_loss/len(test_dataset), round(len(train_dataset)/(btic-tic)), train_mae.get()[1], val_mae.get()[1], test_mae.get()[1]))\n",
    "                # we can save the trained parameters to disk\n",
    "                net_mobile.save_parameters('process/net_mobile_temp_%d_opt_%s_lr_%d_epoch_%d.params' % (temperature, optimizer, lr, epoch))\n",
    "            train_mae.reset()\n",
    "            val_mae.reset()\n",
    "            test_mae.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
