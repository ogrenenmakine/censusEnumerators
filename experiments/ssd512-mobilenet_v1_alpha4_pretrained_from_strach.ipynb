{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.contrib.ndarray import MultiBoxPrior\n",
    "from mxnet.gluon.contrib import nn as nn_contrib\n",
    "from mxnet.gluon import nn\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict classes\n",
    "- channel `i*(num_class+1)` store the scores for this box contains only background\n",
    "- channel `i*(num_class+1)+1+j` store the scores for this box contains an object from the *j*-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predictor(num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict anchor boxes\n",
    "- $t_x = (Y_x - b_x) / b_{width}$\n",
    "- $t_y = (Y_y - b_y) / b_{height}$\n",
    "- $t_{width} = (Y_{width} - b_{width}) / b_{width}$\n",
    "- $t_{height} = (Y_{height} - b_{height}) / b_{height}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_predictor(num_anchors):\n",
    "    return nn.Conv2D(num_anchors * 4, 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage preditions from multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    return nd.flatten(nd.transpose(pred, axes=(0, 2, 3, 1)))\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    return nd.concat(*preds, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_layer(nfilters, stride, expension_constant):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(nn.Conv2D(nfilters, 3, strides=stride, padding=1, groups=nfilters, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(nfilters*expension_constant, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global alpha\n",
    "alpha = 0.25\n",
    "num_filters = int(32*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "def s16():\n",
    "    out = nn.HybridSequential()\n",
    "    # conv_0 layer\n",
    "    out.add(nn.Conv2D(num_filters, 3, strides=2, padding=1, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_1 layer\n",
    "    out.add(dp_layer(num_filters, 1, 2))\n",
    "    # conv_2 layer\n",
    "    out.add(dp_layer(num_filters*2, 2, 2))\n",
    "    # conv_3 layer\n",
    "    out.add(dp_layer(num_filters*4, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*4, 3, strides=2, padding=1, groups=num_filters*4, use_bias=False))\n",
    "    out.load_parameters(\"weights/mobilenet_0_25_s16_org.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def s32():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*8, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_4_layer\n",
    "    out.add(dp_layer(num_filters*8, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*8, 3, strides=2, padding=1, groups=num_filters*8, use_bias=False))\n",
    "    out.load_parameters(\"weights/mobilenet_0_25_s32_org.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b1():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_6_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b2():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_7_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b3():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_8_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b4():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_9_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_model(num_anchors, num_classes):\n",
    "    class_preds = nn.Sequential()\n",
    "    box_preds = nn.Sequential()\n",
    "    \n",
    "    for scale in range(6):\n",
    "        class_preds.add(class_predictor(num_anchors, num_classes))\n",
    "        box_preds.add(box_predictor(num_anchors))\n",
    "    \n",
    "    class_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    box_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    return s16(), s32(), b1(), b2(), b3(), b4(), class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_forward(x, s16, s32, b1, b2, b3, b4, class_preds, box_preds, sizes, ratios):\n",
    "    default_anchors = []\n",
    "    predicted_boxes = []  \n",
    "    predicted_classes = []\n",
    "\n",
    "    x = s16(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[0], ratios=ratios[0]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[0](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[0](x)))\n",
    "    \n",
    "    x = s32(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[1], ratios=ratios[1]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[1](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[1](x)))\n",
    "    \n",
    "    x = b1(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[2], ratios=ratios[2]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[2](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[2](x)))\n",
    "    \n",
    "    x = b2(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[3], ratios=ratios[3]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[3](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[3](x)))\n",
    "    \n",
    "    x = b3(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[4], ratios=ratios[4]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[4](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[4](x)))\n",
    "    \n",
    "    x = b4(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[5], ratios=ratios[5]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[5](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[5](x)))\n",
    "\n",
    "    return default_anchors, predicted_classes, predicted_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "class SSD(gluon.Block):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(SSD, self).__init__(**kwargs)\n",
    "        self.anchor_sizes = [[0.04, 0.1],[0.1,0.26],[0.26,0.42],[0.42,0.58],[0.58,0.74],[0.74,0.9],[0.9,1.06]]\n",
    "        self.anchor_ratios = [[1, 2, .5]] * 6\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.s16, self.s32, self.b1, self.b2, self.b3, self.b4, self.class_preds, self.box_preds = ssd_model(4, num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        default_anchors, predicted_classes, predicted_boxes = ssd_forward(x, self.s16, self.s32, self.b1, self.b2, self.b3, self.b4,\n",
    "            self.class_preds, self.box_preds, self.anchor_sizes, self.anchor_ratios)\n",
    "        anchors = concat_predictions(default_anchors)\n",
    "        box_preds = concat_predictions(predicted_boxes)\n",
    "        class_preds = concat_predictions(predicted_classes)\n",
    "        class_preds = nd.reshape(class_preds, shape=(0, -1, self.num_classes + 1))\n",
    "        \n",
    "        return anchors, class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Outputs:', 'anchors', (1L, 21840L, 4L), 'class prediction', (1L, 21840L, 3L), 'box prediction', (1L, 87360L))\n"
     ]
    }
   ],
   "source": [
    "net = SSD(2)\n",
    "#net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "#net.load_parameters(\"process/ssd_99.params\",ctx=ctx)\n",
    "x = nd.zeros((1, 3, 512, 512),ctx=ctx)\n",
    "default_anchors, class_predictions, box_predictions = net(x)\n",
    "print('Outputs:', 'anchors', default_anchors.shape, 'class prediction', class_predictions.shape, 'box prediction', box_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.NACDDetection import NACDDetection\n",
    "\n",
    "train_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'train')])\n",
    "test_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'test')])\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Test images:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source import NACDTransform\n",
    "width, height = 512, 512\n",
    "train_transform = NACDTransform.NACDDefaultTransform(width, height, False)\n",
    "test_transform = NACDTransform.NACDDefaultTransform(width, height, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd\n",
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = test_dataset[0]\n",
    "bboxes = train_label[:, :4]\n",
    "cids = train_label[:, 4:5]\n",
    "print('image:', train_image.shape)\n",
    "print('bboxes:', bboxes.shape, 'class ids:', cids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2, train_label2 = train_transform(train_image, train_label)\n",
    "print('tensor shape:', train_image2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset.transform(test_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(test_loader):\n",
    "    if ib > 3:\n",
    "        break\n",
    "    print('data:', batch[0].shape, 'label:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array((0.485, 0.456, 0.406))\n",
    "train_image2 = (train_image2 * 255).clip(0, 255)\n",
    "ax = viz.plot_bbox(train_image2.asnumpy(), train_label2[:, :4],\n",
    "                   labels=train_label2[:, 4:5],\n",
    "                   class_names=train_dataset.classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxTarget\n",
    "def training_targets(default_anchors, class_predicts, labels):\n",
    "    class_predicts = nd.transpose(class_predicts, axes=(0, 2, 1))\n",
    "    z = MultiBoxTarget(anchor=default_anchors.as_in_context(mx.cpu()), label=labels.as_in_context(mx.cpu()), cls_pred=class_predicts.as_in_context(mx.cpu()))\n",
    "    box_target = z[0].as_in_context(ctx)  # box offset target for (x, y, width, height)\n",
    "    box_mask = z[1].as_in_context(ctx)  # mask is used to ignore box offsets we don't want to penalize, e.g. negative samples\n",
    "    cls_target = z[2].as_in_context(ctx)  # cls_target is an array of labels for all anchors boxes\n",
    "    return box_target, box_mask, cls_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertlbl(y):\n",
    "    mtrx = y[:,:,0:4]\n",
    "    mtrx = mtrx.asnumpy()\n",
    "    mtrx[mtrx == -1] = -width\n",
    "    mtrx = mtrx/512\n",
    "    return mx.nd.concat(nd.expand_dims(y[:,:,4],2),mx.nd.array(mtrx),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(gluon.loss.Loss):\n",
    "    def __init__(self, axis=-1, alpha=0.25, gamma=2, batch_axis=0, **kwargs):\n",
    "        super(FocalLoss, self).__init__(None, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label):\n",
    "        output = F.softmax(output)\n",
    "        pt = F.pick(output, label, axis=self._axis, keepdims=True)\n",
    "        loss = -self._alpha * ((1 - pt) ** self._gamma) * F.log(pt)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "# cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "cls_loss = FocalLoss()\n",
    "print(cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(gluon.loss.Loss):\n",
    "    def __init__(self, batch_axis=0, **kwargs):\n",
    "        super(SmoothL1Loss, self).__init__(None, batch_axis, **kwargs)\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label, mask):\n",
    "        loss = F.smooth_l1((output - label) * mask, scalar=1.0)\n",
    "        return F.mean(loss, self._batch_axis, exclude=True)\n",
    "\n",
    "box_loss = SmoothL1Loss()\n",
    "print(box_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mxnet import autograd as ag\n",
    "from gluoncv.loss import SSDMultiBoxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop params\n",
    "epochs = 351\n",
    "start_epoch = 1\n",
    "\n",
    "# initialize trainer\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 1e-1, 'wd': 4e-5})\n",
    "\n",
    "# evaluation metrics\n",
    "cls_metric = mx.metric.Accuracy()\n",
    "box_metric = mx.metric.MAE()\n",
    "cls_metric_test = mx.metric.Accuracy()\n",
    "box_metric_test = mx.metric.MAE()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # reset iterator and tick\n",
    "    #train_data.reset()\n",
    "    cls_metric.reset()\n",
    "    box_metric.reset()\n",
    "    tic = time.time()\n",
    "    train_loss = 0\n",
    "    # iterate through all batch\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # record gradients\n",
    "        with ag.record():\n",
    "            x = batch[0].as_in_context(ctx)\n",
    "            y = batch[1].as_in_context(ctx)\n",
    "            lbl = convertlbl(batch[1])\n",
    "            default_anchors, class_predictions, box_predictions = net(x)\n",
    "            box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "            # losses\n",
    "            loss1 = cls_loss(class_predictions, cls_target)\n",
    "            loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "            # sum all losses\n",
    "            loss = loss1 + loss2\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "        # apply \n",
    "        trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        # update metrics\n",
    "        cls_metric.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_train, val1_train = cls_metric.get()\n",
    "    name2_train, val2_train = box_metric.get()\n",
    "\n",
    "    cls_metric_test.reset()\n",
    "    box_metric_test.reset()\n",
    "    tic = time.time()\n",
    "    test_loss = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        # record gradients\n",
    "        x = batch[0].as_in_context(ctx)\n",
    "        y = batch[1].as_in_context(ctx)\n",
    "        lbl = convertlbl(batch[1])\n",
    "        default_anchors, class_predictions, box_predictions = net(x)\n",
    "        box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "        # losses\n",
    "        loss1 = cls_loss(class_predictions, cls_target)\n",
    "        loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "        # sum all losses\n",
    "        loss = loss1 + loss2\n",
    "        test_loss += nd.sum(loss).asscalar()\n",
    "        # update metrics\n",
    "        cls_metric_test.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric_test.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_test, val1_test = cls_metric_test.get()\n",
    "    name2_test, val2_test = box_metric_test.get()\n",
    "    print('epoch:%3d;\\t train:%.6e;%f;%.6e;\\t test:%.6e;%f;%.6e'\n",
    "          %(epoch, train_loss/len(train_dataset), val1_train, val2_train, test_loss/len(test_dataset), val1_test, val2_test))\n",
    "\n",
    "    net.save_parameters('process/ssd_%d.params' % epoch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch:  1;\t train:9.915915e-03;0.984248;2.036356e-03;\t test:5.662337e-03;0.994727;1.982166e-03\n",
    "epoch:  2;\t train:5.041475e-03;0.996049;2.012134e-03;\t test:4.065077e-03;0.996796;1.946245e-03\n",
    "epoch:  3;\t train:4.026770e-03;0.997003;1.996785e-03;\t test:3.552532e-03;0.997218;1.987236e-03\n",
    "epoch:  4;\t train:3.580692e-03;0.997336;2.003301e-03;\t test:3.194752e-03;0.997495;1.968955e-03\n",
    "epoch:  5;\t train:3.296646e-03;0.997584;1.986838e-03;\t test:2.976476e-03;0.997690;1.959656e-03\n",
    "epoch:  6;\t train:3.127968e-03;0.997723;2.002749e-03;\t test:2.840425e-03;0.997814;1.946090e-03\n",
    "epoch:  7;\t train:2.968606e-03;0.997834;1.986935e-03;\t test:2.687024e-03;0.997884;1.946150e-03\n",
    "epoch:  8;\t train:2.862159e-03;0.997894;1.985720e-03;\t test:2.553406e-03;0.997972;1.899725e-03\n",
    "epoch:  9;\t train:2.764990e-03;0.997932;1.981117e-03;\t test:2.514132e-03;0.997960;1.930248e-03\n",
    "epoch: 10;\t train:2.703905e-03;0.997951;1.988112e-03;\t test:2.463196e-03;0.997988;1.941222e-03\n",
    "epoch: 11;\t train:2.649946e-03;0.997973;1.985149e-03;\t test:2.408850e-03;0.997987;1.933829e-03\n",
    "epoch: 12;\t train:2.587259e-03;0.998001;1.973181e-03;\t test:2.392028e-03;0.997982;1.948976e-03\n",
    "epoch: 13;\t train:2.561463e-03;0.998004;1.982478e-03;\t test:2.338929e-03;0.998034;1.921408e-03\n",
    "epoch: 14;\t train:2.510963e-03;0.998034;1.961080e-03;\t test:2.306400e-03;0.998031;1.922906e-03\n",
    "epoch: 15;\t train:2.500861e-03;0.998028;1.983403e-03;\t test:2.284681e-03;0.998045;1.922682e-03\n",
    "epoch: 16;\t train:2.460926e-03;0.998054;1.965158e-03;\t test:2.227659e-03;0.998095;1.893857e-03\n",
    "epoch: 17;\t train:2.437713e-03;0.998063;1.967082e-03;\t test:2.250980e-03;0.998064;1.927637e-03\n",
    "epoch: 18;\t train:2.414690e-03;0.998077;1.961378e-03;\t test:2.205622e-03;0.998104;1.897701e-03\n",
    "epoch: 19;\t train:2.401447e-03;0.998082;1.963269e-03;\t test:2.169014e-03;0.998093;1.871711e-03\n",
    "epoch: 20;\t train:2.364268e-03;0.998106;1.947215e-03;\t test:2.155032e-03;0.998141;1.876826e-03\n",
    "epoch: 21;\t train:2.351022e-03;0.998111;1.949115e-03;\t test:2.163376e-03;0.998065;1.917326e-03\n",
    "epoch: 22;\t train:2.345119e-03;0.998111;1.955072e-03;\t test:2.148358e-03;0.998123;1.907023e-03\n",
    "epoch: 23;\t train:2.330475e-03;0.998112;1.959593e-03;\t test:2.149186e-03;0.998092;1.914328e-03\n",
    "epoch: 24;\t train:2.314760e-03;0.998124;1.950172e-03;\t test:2.117703e-03;0.998128;1.900555e-03\n",
    "epoch: 25;\t train:2.307660e-03;0.998116;1.959925e-03;\t test:2.101004e-03;0.998153;1.892699e-03\n",
    "epoch: 26;\t train:2.296005e-03;0.998118;1.960533e-03;\t test:2.086683e-03;0.998134;1.897294e-03\n",
    "epoch: 27;\t train:2.282183e-03;0.998133;1.950645e-03;\t test:2.070907e-03;0.998163;1.888425e-03\n",
    "epoch: 28;\t train:2.253669e-03;0.998145;1.936853e-03;\t test:2.082636e-03;0.998138;1.910202e-03\n",
    "epoch: 29;\t train:2.261041e-03;0.998131;1.954309e-03;\t test:2.039832e-03;0.998149;1.868632e-03\n",
    "epoch: 30;\t train:2.240591e-03;0.998144;1.941008e-03;\t test:2.007629e-03;0.998175;1.860610e-03\n",
    "epoch: 31;\t train:2.235702e-03;0.998135;1.947637e-03;\t test:2.024012e-03;0.998148;1.885972e-03\n",
    "epoch: 32;\t train:2.212788e-03;0.998155;1.927829e-03;\t test:2.000703e-03;0.998165;1.877092e-03\n",
    "epoch: 33;\t train:2.204836e-03;0.998150;1.933987e-03;\t test:1.966472e-03;0.998190;1.843492e-03\n",
    "epoch: 34;\t train:2.192430e-03;0.998155;1.925231e-03;\t test:2.001079e-03;0.998130;1.896860e-03\n",
    "epoch: 35;\t train:2.193085e-03;0.998148;1.935097e-03;\t test:1.955434e-03;0.998195;1.837578e-03\n",
    "epoch: 36;\t train:2.180715e-03;0.998152;1.930223e-03;\t test:1.956363e-03;0.998174;1.855132e-03\n",
    "epoch: 37;\t train:2.158154e-03;0.998162;1.915500e-03;\t test:1.968254e-03;0.998180;1.867483e-03\n",
    "epoch: 38;\t train:2.169852e-03;0.998153;1.928765e-03;\t test:1.967339e-03;0.998163;1.882916e-03\n",
    "epoch: 39;\t train:2.142363e-03;0.998167;1.910354e-03;\t test:1.927743e-03;0.998181;1.867759e-03\n",
    "epoch: 40;\t train:2.145203e-03;0.998165;1.917443e-03;\t test:1.906969e-03;0.998199;1.846701e-03\n",
    "epoch: 41;\t train:2.134904e-03;0.998159;1.915681e-03;\t test:1.928167e-03;0.998159;1.867939e-03\n",
    "epoch: 42;\t train:2.126251e-03;0.998167;1.906554e-03;\t test:1.902380e-03;0.998194;1.837939e-03\n",
    "epoch: 43;\t train:2.119428e-03;0.998165;1.910050e-03;\t test:1.889990e-03;0.998197;1.831288e-03\n",
    "epoch: 44;\t train:2.114098e-03;0.998164;1.910200e-03;\t test:1.893676e-03;0.998203;1.850409e-03\n",
    "epoch: 45;\t train:2.115241e-03;0.998158;1.917094e-03;\t test:1.876565e-03;0.998219;1.835008e-03\n",
    "epoch: 46;\t train:2.108774e-03;0.998160;1.911690e-03;\t test:1.883950e-03;0.998195;1.848246e-03\n",
    "epoch: 47;\t train:2.098011e-03;0.998159;1.908191e-03;\t test:1.867154e-03;0.998201;1.856103e-03\n",
    "epoch: 48;\t train:2.093297e-03;0.998164;1.905666e-03;\t test:1.851451e-03;0.998208;1.832443e-03\n",
    "epoch: 49;\t train:2.080139e-03;0.998166;1.898025e-03;\t test:1.841309e-03;0.998218;1.826438e-03\n",
    "epoch: 50;\t train:2.078092e-03;0.998162;1.904999e-03;\t test:1.843487e-03;0.998209;1.833231e-03\n",
    "epoch: 51;\t train:2.072922e-03;0.998161;1.900652e-03;\t test:1.841007e-03;0.998220;1.829398e-03\n",
    "epoch: 52;\t train:2.065368e-03;0.998166;1.898582e-03;\t test:1.811715e-03;0.998235;1.811452e-03\n",
    "epoch: 53;\t train:2.054386e-03;0.998168;1.892416e-03;\t test:1.837347e-03;0.998222;1.836529e-03\n",
    "epoch: 54;\t train:2.061404e-03;0.998160;1.898004e-03;\t test:1.792988e-03;0.998227;1.803610e-03\n",
    "epoch: 55;\t train:2.050576e-03;0.998163;1.897176e-03;\t test:1.813392e-03;0.998221;1.818538e-03\n",
    "epoch: 56;\t train:2.037827e-03;0.998175;1.885027e-03;\t test:1.820720e-03;0.998207;1.840251e-03\n",
    "epoch: 57;\t train:2.035247e-03;0.998169;1.887554e-03;\t test:1.814256e-03;0.998217;1.827995e-03\n",
    "epoch: 58;\t train:2.031311e-03;0.998167;1.888133e-03;\t test:1.802264e-03;0.998172;1.822689e-03\n",
    "epoch: 59;\t train:2.029135e-03;0.998170;1.883824e-03;\t test:1.768979e-03;0.998248;1.777481e-03\n",
    "epoch: 60;\t train:2.028328e-03;0.998162;1.891952e-03;\t test:1.761036e-03;0.998252;1.778428e-03\n",
    "epoch: 61;\t train:2.001216e-03;0.998182;1.866262e-03;\t test:1.791676e-03;0.998207;1.820730e-03\n",
    "epoch: 62;\t train:2.010155e-03;0.998168;1.875911e-03;\t test:1.779200e-03;0.998206;1.813795e-03\n",
    "epoch: 63;\t train:2.009333e-03;0.998160;1.881681e-03;\t test:1.772457e-03;0.998220;1.809628e-03\n",
    "epoch: 64;\t train:2.012092e-03;0.998158;1.886222e-03;\t test:1.774535e-03;0.998217;1.811606e-03\n",
    "epoch: 65;\t train:1.983672e-03;0.998182;1.861996e-03;\t test:1.765657e-03;0.998228;1.799229e-03\n",
    "epoch: 66;\t train:1.990664e-03;0.998165;1.873696e-03;\t test:1.732713e-03;0.998256;1.774749e-03\n",
    "epoch: 67;\t train:1.992960e-03;0.998169;1.873790e-03;\t test:1.744511e-03;0.998209;1.794858e-03\n",
    "epoch: 68;\t train:1.979678e-03;0.998174;1.864427e-03;\t test:1.738526e-03;0.998231;1.783427e-03\n",
    "epoch: 69;\t train:1.975876e-03;0.998173;1.862080e-03;\t test:1.760699e-03;0.998209;1.814429e-03\n",
    "epoch: 70;\t train:1.973931e-03;0.998180;1.860710e-03;\t test:1.740439e-03;0.998225;1.797798e-03\n",
    "epoch: 71;\t train:1.966780e-03;0.998174;1.858922e-03;\t test:1.730416e-03;0.998229;1.789653e-03\n",
    "epoch: 72;\t train:1.982209e-03;0.998158;1.875281e-03;\t test:1.718463e-03;0.998239;1.762776e-03\n",
    "epoch: 73;\t train:1.954917e-03;0.998180;1.851766e-03;\t test:1.723684e-03;0.998236;1.778238e-03\n",
    "epoch: 74;\t train:1.949924e-03;0.998184;1.851537e-03;\t test:1.728884e-03;0.998229;1.803351e-03\n",
    "epoch: 75;\t train:1.944785e-03;0.998184;1.841696e-03;\t test:1.707451e-03;0.998247;1.762655e-03\n",
    "epoch: 76;\t train:1.956444e-03;0.998168;1.862348e-03;\t test:1.681841e-03;0.998277;1.734060e-03\n",
    "epoch: 77;\t train:1.945784e-03;0.998172;1.853028e-03;\t test:1.694073e-03;0.998270;1.758066e-03\n",
    "epoch: 78;\t train:1.944101e-03;0.998176;1.850786e-03;\t test:1.706625e-03;0.998255;1.779756e-03\n",
    "epoch: 79;\t train:1.938024e-03;0.998179;1.845239e-03;\t test:1.712276e-03;0.998225;1.784630e-03\n",
    "epoch: 80;\t train:1.937941e-03;0.998178;1.846926e-03;\t test:1.678997e-03;0.998253;1.754671e-03\n",
    "epoch: 81;\t train:1.932482e-03;0.998174;1.848364e-03;\t test:1.681257e-03;0.998242;1.754797e-03\n",
    "epoch: 82;\t train:1.920837e-03;0.998178;1.836061e-03;\t test:1.672644e-03;0.998255;1.752821e-03\n",
    "epoch: 83;\t train:1.926700e-03;0.998179;1.839438e-03;\t test:1.635619e-03;0.998303;1.705406e-03\n",
    "epoch: 84;\t train:1.915173e-03;0.998186;1.833119e-03;\t test:1.682793e-03;0.998242;1.767956e-03\n",
    "epoch: 85;\t train:1.914149e-03;0.998176;1.838608e-03;\t test:1.653887e-03;0.998276;1.723044e-03\n",
    "epoch: 86;\t train:1.917039e-03;0.998173;1.838825e-03;\t test:1.678846e-03;0.998252;1.763602e-03\n",
    "epoch: 87;\t train:1.906446e-03;0.998180;1.832379e-03;\t test:1.654108e-03;0.998267;1.729533e-03\n",
    "epoch: 88;\t train:1.903285e-03;0.998185;1.823286e-03;\t test:1.672584e-03;0.998264;1.744849e-03\n",
    "epoch: 89;\t train:1.909336e-03;0.998174;1.837028e-03;\t test:1.672275e-03;0.998249;1.764654e-03\n",
    "epoch: 90;\t train:1.908337e-03;0.998164;1.840708e-03;\t test:1.651808e-03;0.998231;1.749405e-03\n",
    "epoch: 91;\t train:1.906907e-03;0.998174;1.833012e-03;\t test:1.651304e-03;0.998255;1.733261e-03\n",
    "epoch: 92;\t train:1.900101e-03;0.998176;1.830526e-03;\t test:1.646111e-03;0.998274;1.731398e-03\n",
    "epoch: 93;\t train:1.889958e-03;0.998179;1.824966e-03;\t test:1.643874e-03;0.998272;1.734397e-03\n",
    "epoch: 94;\t train:1.892142e-03;0.998182;1.820752e-03;\t test:1.617179e-03;0.998257;1.706448e-03\n",
    "epoch: 95;\t train:1.887961e-03;0.998178;1.823690e-03;\t test:1.633763e-03;0.998269;1.723681e-03\n",
    "epoch: 96;\t train:1.886297e-03;0.998183;1.819385e-03;\t test:1.619906e-03;0.998264;1.703017e-03\n",
    "epoch: 97;\t train:1.874065e-03;0.998185;1.813634e-03;\t test:1.633011e-03;0.998240;1.730672e-03\n",
    "epoch: 98;\t train:1.880075e-03;0.998178;1.821366e-03;\t test:1.651482e-03;0.998241;1.757571e-03\n",
    "epoch: 99;\t train:1.878608e-03;0.998181;1.814840e-03;\t test:1.615057e-03;0.998290;1.702968e-03\n",
    "epoch:100;\t train:1.874953e-03;0.998186;1.812103e-03;\t test:1.625413e-03;0.998279;1.722595e-03\n",
    "epoch:101;\t train:1.866284e-03;0.998183;1.809518e-03;\t test:1.601257e-03;0.998287;1.692689e-03\n",
    "epoch:102;\t train:1.867688e-03;0.998179;1.811962e-03;\t test:1.603580e-03;0.998283;1.697030e-03\n",
    "epoch:103;\t train:1.874396e-03;0.998171;1.822456e-03;\t test:1.623712e-03;0.998264;1.721051e-03\n",
    "epoch:104;\t train:1.867188e-03;0.998179;1.811830e-03;\t test:1.601089e-03;0.998285;1.695577e-03\n",
    "epoch:105;\t train:1.853147e-03;0.998188;1.800012e-03;\t test:1.607832e-03;0.998267;1.710073e-03\n",
    "epoch:106;\t train:1.844160e-03;0.998196;1.792644e-03;\t test:1.615833e-03;0.998249;1.721202e-03\n",
    "epoch:107;\t train:1.861963e-03;0.998176;1.812018e-03;\t test:1.590093e-03;0.998297;1.694198e-03\n",
    "epoch:108;\t train:1.846821e-03;0.998194;1.799272e-03;\t test:1.604253e-03;0.998263;1.700088e-03\n",
    "epoch:109;\t train:1.852660e-03;0.998180;1.804638e-03;\t test:1.593526e-03;0.998292;1.690154e-03\n",
    "epoch:110;\t train:1.846038e-03;0.998189;1.795517e-03;\t test:1.571151e-03;0.998304;1.674275e-03\n",
    "epoch:111;\t train:1.848520e-03;0.998178;1.805409e-03;\t test:1.588501e-03;0.998297;1.687297e-03\n",
    "epoch:112;\t train:1.838689e-03;0.998193;1.790534e-03;\t test:1.552133e-03;0.998301;1.652937e-03\n",
    "epoch:113;\t train:1.830052e-03;0.998193;1.787616e-03;\t test:1.602527e-03;0.998260;1.709611e-03\n",
    "epoch:114;\t train:1.841977e-03;0.998180;1.798163e-03;\t test:1.605771e-03;0.998260;1.711465e-03\n",
    "epoch:115;\t train:1.832887e-03;0.998197;1.786164e-03;\t test:1.601103e-03;0.998256;1.714027e-03\n",
    "epoch:116;\t train:1.834837e-03;0.998187;1.792966e-03;\t test:1.550222e-03;0.998285;1.650500e-03\n",
    "epoch:117;\t train:1.823840e-03;0.998195;1.782600e-03;\t test:1.581533e-03;0.998273;1.689898e-03\n",
    "epoch:118;\t train:1.825218e-03;0.998191;1.782832e-03;\t test:1.572525e-03;0.998286;1.674091e-03\n",
    "epoch:119;\t train:1.827404e-03;0.998189;1.790178e-03;\t test:1.540938e-03;0.998321;1.634410e-03\n",
    "epoch:120;\t train:1.828906e-03;0.998186;1.789030e-03;\t test:1.560018e-03;0.998282;1.668570e-03\n",
    "epoch:121;\t train:1.815302e-03;0.998192;1.778394e-03;\t test:1.559923e-03;0.998295;1.669083e-03\n",
    "epoch:122;\t train:1.811503e-03;0.998199;1.775449e-03;\t test:1.586664e-03;0.998280;1.695180e-03\n",
    "epoch:123;\t train:1.813703e-03;0.998197;1.776706e-03;\t test:1.556206e-03;0.998301;1.659894e-03\n",
    "epoch:124;\t train:1.813745e-03;0.998192;1.777159e-03;\t test:1.537935e-03;0.998312;1.643552e-03\n",
    "epoch:125;\t train:1.817134e-03;0.998188;1.785607e-03;\t test:1.564043e-03;0.998263;1.673567e-03\n",
    "epoch:126;\t train:1.810625e-03;0.998192;1.774826e-03;\t test:1.582708e-03;0.998262;1.697899e-03\n",
    "epoch:127;\t train:1.805411e-03;0.998191;1.774849e-03;\t test:1.559270e-03;0.998297;1.669908e-03\n",
    "epoch:128;\t train:1.807871e-03;0.998191;1.776414e-03;\t test:1.537277e-03;0.998312;1.652232e-03\n",
    "epoch:129;\t train:1.794501e-03;0.998199;1.763580e-03;\t test:1.572912e-03;0.998269;1.676128e-03\n",
    "epoch:130;\t train:1.795843e-03;0.998200;1.767030e-03;\t test:1.547097e-03;0.998290;1.666961e-03\n",
    "epoch:131;\t train:1.802003e-03;0.998199;1.772504e-03;\t test:1.556854e-03;0.998261;1.670166e-03\n",
    "epoch:132;\t train:1.786848e-03;0.998203;1.760205e-03;\t test:1.538512e-03;0.998292;1.653171e-03\n",
    "epoch:133;\t train:1.795175e-03;0.998198;1.767415e-03;\t test:1.553438e-03;0.998272;1.672498e-03\n",
    "epoch:134;\t train:1.795252e-03;0.998196;1.764939e-03;\t test:1.559552e-03;0.998253;1.670416e-03\n",
    "epoch:135;\t train:1.797590e-03;0.998192;1.772697e-03;\t test:1.544102e-03;0.998278;1.647661e-03\n",
    "epoch:136;\t train:1.777798e-03;0.998210;1.752614e-03;\t test:1.551727e-03;0.998282;1.667088e-03\n",
    "epoch:137;\t train:1.774051e-03;0.998214;1.749084e-03;\t test:1.536883e-03;0.998294;1.644635e-03\n",
    "epoch:138;\t train:1.791747e-03;0.998197;1.765577e-03;\t test:1.547307e-03;0.998274;1.665650e-03\n",
    "epoch:139;\t train:1.787813e-03;0.998201;1.759380e-03;\t test:1.567472e-03;0.998248;1.693031e-03\n",
    "epoch:140;\t train:1.784473e-03;0.998195;1.764766e-03;\t test:1.535156e-03;0.998285;1.649445e-03\n",
    "epoch:141;\t train:1.786018e-03;0.998195;1.764564e-03;\t test:1.526985e-03;0.998299;1.638069e-03\n",
    "epoch:142;\t train:1.785204e-03;0.998194;1.763737e-03;\t test:1.516905e-03;0.998285;1.630339e-03\n",
    "epoch:143;\t train:1.782993e-03;0.998198;1.763370e-03;\t test:1.527071e-03;0.998284;1.651567e-03\n",
    "epoch:144;\t train:1.779332e-03;0.998199;1.758802e-03;\t test:1.526478e-03;0.998280;1.645557e-03\n",
    "epoch:145;\t train:1.768216e-03;0.998204;1.752610e-03;\t test:1.509516e-03;0.998280;1.625071e-03\n",
    "epoch:146;\t train:1.769078e-03;0.998203;1.752635e-03;\t test:1.508766e-03;0.998325;1.628903e-03\n",
    "epoch:147;\t train:1.762420e-03;0.998214;1.742918e-03;\t test:1.531465e-03;0.998281;1.657888e-03\n",
    "epoch:148;\t train:1.772841e-03;0.998196;1.759199e-03;\t test:1.504457e-03;0.998324;1.627557e-03\n",
    "epoch:149;\t train:1.757611e-03;0.998218;1.740828e-03;\t test:1.520942e-03;0.998293;1.645694e-03\n",
    "epoch:150;\t train:1.767098e-03;0.998204;1.750210e-03;\t test:1.522512e-03;0.998287;1.657559e-03\n",
    "epoch:151;\t train:1.760646e-03;0.998212;1.744450e-03;\t test:1.534090e-03;0.998280;1.659501e-03\n",
    "epoch:152;\t train:1.764503e-03;0.998204;1.748372e-03;\t test:1.513255e-03;0.998310;1.626797e-03\n",
    "epoch:153;\t train:1.759172e-03;0.998201;1.747946e-03;\t test:1.513072e-03;0.998292;1.628669e-03\n",
    "epoch:154;\t train:1.766163e-03;0.998197;1.754192e-03;\t test:1.528305e-03;0.998251;1.659559e-03\n",
    "epoch:155;\t train:1.758323e-03;0.998207;1.744970e-03;\t test:1.498619e-03;0.998299;1.622132e-03\n",
    "epoch:156;\t train:1.768341e-03;0.998196;1.759181e-03;\t test:1.515352e-03;0.998274;1.631322e-03\n",
    "epoch:157;\t train:1.762240e-03;0.998199;1.753580e-03;\t test:1.497713e-03;0.998306;1.620259e-03\n",
    "epoch:158;\t train:1.750407e-03;0.998219;1.735240e-03;\t test:1.522111e-03;0.998282;1.643171e-03\n",
    "epoch:159;\t train:1.770133e-03;0.998186;1.761952e-03;\t test:1.521202e-03;0.998274;1.646961e-03\n",
    "epoch:160;\t train:1.743517e-03;0.998220;1.727946e-03;\t test:1.518290e-03;0.998294;1.635348e-03\n",
    "epoch:161;\t train:1.749059e-03;0.998210;1.740938e-03;\t test:1.500891e-03;0.998288;1.633076e-03\n",
    "epoch:162;\t train:1.752015e-03;0.998205;1.745912e-03;\t test:1.499206e-03;0.998303;1.618715e-03\n",
    "epoch:163;\t train:1.747553e-03;0.998211;1.737930e-03;\t test:1.502693e-03;0.998304;1.629583e-03\n",
    "epoch:164;\t train:1.754733e-03;0.998205;1.746612e-03;\t test:1.505343e-03;0.998285;1.630401e-03\n",
    "epoch:165;\t train:1.745557e-03;0.998207;1.740151e-03;\t test:1.483734e-03;0.998312;1.603068e-03\n",
    "epoch:166;\t train:1.750949e-03;0.998207;1.740310e-03;\t test:1.513967e-03;0.998275;1.641301e-03\n",
    "epoch:167;\t train:1.741747e-03;0.998209;1.741126e-03;\t test:1.488431e-03;0.998306;1.615928e-03\n",
    "epoch:168;\t train:1.734614e-03;0.998214;1.732457e-03;\t test:1.500418e-03;0.998307;1.631621e-03\n",
    "epoch:169;\t train:1.731534e-03;0.998217;1.730205e-03;\t test:1.466817e-03;0.998338;1.583287e-03\n",
    "epoch:170;\t train:1.732806e-03;0.998216;1.731486e-03;\t test:1.500388e-03;0.998290;1.625964e-03\n",
    "epoch:171;\t train:1.750161e-03;0.998198;1.747113e-03;\t test:1.500925e-03;0.998290;1.623980e-03\n",
    "epoch:172;\t train:1.728982e-03;0.998218;1.730305e-03;\t test:1.496434e-03;0.998310;1.621680e-03\n",
    "epoch:173;\t train:1.735995e-03;0.998217;1.732717e-03;\t test:1.472130e-03;0.998321;1.601948e-03\n",
    "epoch:174;\t train:1.734287e-03;0.998213;1.733804e-03;\t test:1.488148e-03;0.998300;1.614268e-03\n",
    "epoch:175;\t train:1.727612e-03;0.998221;1.726472e-03;\t test:1.491136e-03;0.998314;1.618318e-03\n",
    "epoch:176;\t train:1.734442e-03;0.998210;1.733491e-03;\t test:1.477725e-03;0.998314;1.605247e-03\n",
    "epoch:177;\t train:1.713657e-03;0.998225;1.716667e-03;\t test:1.480905e-03;0.998311;1.614349e-03\n",
    "epoch:178;\t train:1.722900e-03;0.998216;1.727038e-03;\t test:1.481646e-03;0.998301;1.616127e-03\n",
    "epoch:179;\t train:1.723246e-03;0.998218;1.723579e-03;\t test:1.490344e-03;0.998302;1.616696e-03\n",
    "epoch:180;\t train:1.723928e-03;0.998208;1.730073e-03;\t test:1.471414e-03;0.998309;1.600087e-03\n",
    "epoch:181;\t train:1.723999e-03;0.998208;1.732209e-03;\t test:1.465288e-03;0.998290;1.597965e-03\n",
    "epoch:182;\t train:1.715235e-03;0.998217;1.719800e-03;\t test:1.470826e-03;0.998306;1.605611e-03\n",
    "epoch:183;\t train:1.720330e-03;0.998213;1.727104e-03;\t test:1.487680e-03;0.998292;1.619193e-03\n",
    "epoch:184;\t train:1.711414e-03;0.998223;1.715816e-03;\t test:1.458339e-03;0.998339;1.571408e-03\n",
    "epoch:185;\t train:1.715388e-03;0.998220;1.719743e-03;\t test:1.490483e-03;0.998294;1.629532e-03\n",
    "epoch:186;\t train:1.708729e-03;0.998219;1.716306e-03;\t test:1.479525e-03;0.998271;1.618887e-03\n",
    "epoch:187;\t train:1.711963e-03;0.998219;1.717569e-03;\t test:1.490641e-03;0.998287;1.628494e-03\n",
    "epoch:188;\t train:1.703847e-03;0.998223;1.712156e-03;\t test:1.448888e-03;0.998325;1.566666e-03\n",
    "epoch:189;\t train:1.708377e-03;0.998222;1.715651e-03;\t test:1.489060e-03;0.998287;1.621495e-03\n",
    "epoch:190;\t train:1.716749e-03;0.998213;1.724095e-03;\t test:1.468411e-03;0.998317;1.611832e-03\n",
    "epoch:191;\t train:1.713229e-03;0.998211;1.724885e-03;\t test:1.475834e-03;0.998314;1.608046e-03\n",
    "epoch:192;\t train:1.713985e-03;0.998216;1.723312e-03;\t test:1.483912e-03;0.998289;1.619121e-03\n",
    "epoch:193;\t train:1.707245e-03;0.998212;1.719193e-03;\t test:1.451707e-03;0.998332;1.587533e-03\n",
    "epoch:194;\t train:1.717990e-03;0.998206;1.731719e-03;\t test:1.485249e-03;0.998294;1.623614e-03\n",
    "epoch:195;\t train:1.704470e-03;0.998226;1.714621e-03;\t test:1.475850e-03;0.998305;1.604636e-03\n",
    "epoch:196;\t train:1.709630e-03;0.998215;1.721507e-03;\t test:1.478278e-03;0.998308;1.617735e-03\n",
    "epoch:197;\t train:1.711424e-03;0.998213;1.726987e-03;\t test:1.486048e-03;0.998278;1.626748e-03\n",
    "epoch:198;\t train:1.700367e-03;0.998225;1.711595e-03;\t test:1.452894e-03;0.998326;1.581211e-03\n",
    "epoch:199;\t train:1.695354e-03;0.998227;1.710059e-03;\t test:1.476068e-03;0.998300;1.614810e-03\n",
    "epoch:200;\t train:1.703423e-03;0.998216;1.714799e-03;\t test:1.475329e-03;0.998290;1.607823e-03\n",
    "epoch:201;\t train:1.705641e-03;0.998212;1.724426e-03;\t test:1.455498e-03;0.998317;1.593790e-03\n",
    "epoch:202;\t train:1.701082e-03;0.998213;1.720629e-03;\t test:1.448240e-03;0.998331;1.583167e-03\n",
    "epoch:203;\t train:1.699311e-03;0.998217;1.715013e-03;\t test:1.444248e-03;0.998332;1.578975e-03\n",
    "epoch:204;\t train:1.700106e-03;0.998214;1.717821e-03;\t test:1.471582e-03;0.998298;1.603699e-03\n",
    "epoch:205;\t train:1.685352e-03;0.998231;1.701627e-03;\t test:1.463662e-03;0.998288;1.593272e-03\n",
    "epoch:206;\t train:1.696082e-03;0.998220;1.712631e-03;\t test:1.437443e-03;0.998350;1.566040e-03\n",
    "epoch:207;\t train:1.684709e-03;0.998230;1.703808e-03;\t test:1.441572e-03;0.998325;1.579033e-03\n",
    "epoch:208;\t train:1.693268e-03;0.998225;1.709142e-03;\t test:1.460326e-03;0.998282;1.599252e-03\n",
    "epoch:209;\t train:1.686733e-03;0.998228;1.703354e-03;\t test:1.449637e-03;0.998319;1.583323e-03\n",
    "epoch:210;\t train:1.688657e-03;0.998226;1.708293e-03;\t test:1.463794e-03;0.998310;1.602271e-03\n",
    "epoch:211;\t train:1.697201e-03;0.998211;1.717762e-03;\t test:1.453619e-03;0.998331;1.587109e-03\n",
    "epoch:212;\t train:1.679517e-03;0.998230;1.698330e-03;\t test:1.442643e-03;0.998329;1.570822e-03\n",
    "epoch:213;\t train:1.694351e-03;0.998212;1.717503e-03;\t test:1.449720e-03;0.998317;1.588594e-03\n",
    "epoch:214;\t train:1.671282e-03;0.998237;1.690326e-03;\t test:1.445196e-03;0.998333;1.575561e-03\n",
    "epoch:215;\t train:1.667967e-03;0.998235;1.694197e-03;\t test:1.446421e-03;0.998340;1.585459e-03\n",
    "epoch:216;\t train:1.679070e-03;0.998225;1.703132e-03;\t test:1.417635e-03;0.998342;1.547252e-03\n",
    "epoch:217;\t train:1.679173e-03;0.998226;1.704523e-03;\t test:1.456290e-03;0.998318;1.595015e-03\n",
    "epoch:218;\t train:1.674125e-03;0.998226;1.701006e-03;\t test:1.445491e-03;0.998327;1.588651e-03\n",
    "epoch:219;\t train:1.677354e-03;0.998224;1.703258e-03;\t test:1.457734e-03;0.998298;1.601274e-03\n",
    "epoch:220;\t train:1.687390e-03;0.998207;1.712829e-03;\t test:1.442271e-03;0.998316;1.587087e-03\n",
    "epoch:221;\t train:1.672216e-03;0.998232;1.695066e-03;\t test:1.447561e-03;0.998312;1.585737e-03\n",
    "epoch:222;\t train:1.674649e-03;0.998228;1.701420e-03;\t test:1.435836e-03;0.998329;1.574138e-03\n",
    "epoch:223;\t train:1.675526e-03;0.998218;1.703696e-03;\t test:1.445912e-03;0.998317;1.592618e-03\n",
    "epoch:224;\t train:1.670527e-03;0.998232;1.694262e-03;\t test:1.436072e-03;0.998337;1.568408e-03\n",
    "epoch:225;\t train:1.668296e-03;0.998234;1.695353e-03;\t test:1.416441e-03;0.998327;1.551261e-03\n",
    "epoch:226;\t train:1.673690e-03;0.998224;1.702494e-03;\t test:1.423946e-03;0.998330;1.568823e-03\n",
    "epoch:227;\t train:1.685262e-03;0.998218;1.711333e-03;\t test:1.437474e-03;0.998328;1.582469e-03\n",
    "epoch:228;\t train:1.667386e-03;0.998226;1.694979e-03;\t test:1.449459e-03;0.998301;1.595287e-03\n",
    "epoch:229;\t train:1.672344e-03;0.998221;1.704370e-03;\t test:1.441807e-03;0.998296;1.582852e-03\n",
    "epoch:230;\t train:1.665204e-03;0.998232;1.691657e-03;\t test:1.443257e-03;0.998317;1.585715e-03\n",
    "epoch:231;\t train:1.657684e-03;0.998238;1.687655e-03;\t test:1.447467e-03;0.998328;1.587299e-03\n",
    "epoch:232;\t train:1.664885e-03;0.998236;1.690560e-03;\t test:1.437710e-03;0.998331;1.577601e-03\n",
    "epoch:233;\t train:1.652599e-03;0.998246;1.681608e-03;\t test:1.409387e-03;0.998354;1.547077e-03\n",
    "epoch:234;\t train:1.656707e-03;0.998238;1.689650e-03;\t test:1.425824e-03;0.998323;1.567146e-03\n",
    "epoch:235;\t train:1.675326e-03;0.998217;1.705877e-03;\t test:1.435080e-03;0.998316;1.564970e-03\n",
    "epoch:236;\t train:1.654070e-03;0.998239;1.685216e-03;\t test:1.432023e-03;0.998310;1.575948e-03\n",
    "epoch:237;\t train:1.651685e-03;0.998236;1.684267e-03;\t test:1.415954e-03;0.998348;1.551621e-03\n",
    "epoch:238;\t train:1.656372e-03;0.998237;1.686260e-03;\t test:1.438218e-03;0.998311;1.572819e-03\n",
    "epoch:239;\t train:1.656797e-03;0.998230;1.690640e-03;\t test:1.411121e-03;0.998353;1.553678e-03\n",
    "epoch:240;\t train:1.653620e-03;0.998234;1.683157e-03;\t test:1.403965e-03;0.998367;1.555534e-03\n",
    "epoch:241;\t train:1.655850e-03;0.998239;1.690225e-03;\t test:1.419456e-03;0.998348;1.558872e-03\n",
    "epoch:242;\t train:1.643802e-03;0.998243;1.674367e-03;\t test:1.427974e-03;0.998306;1.560760e-03\n",
    "epoch:243;\t train:1.664923e-03;0.998225;1.699342e-03;\t test:1.394473e-03;0.998370;1.532271e-03\n",
    "epoch:244;\t train:1.647877e-03;0.998239;1.685121e-03;\t test:1.426052e-03;0.998356;1.565550e-03\n",
    "epoch:245;\t train:1.656262e-03;0.998231;1.692848e-03;\t test:1.425271e-03;0.998330;1.565739e-03\n",
    "epoch:246;\t train:1.657908e-03;0.998228;1.690333e-03;\t test:1.389550e-03;0.998359;1.532976e-03\n",
    "epoch:247;\t train:1.643574e-03;0.998243;1.679645e-03;\t test:1.416794e-03;0.998323;1.562440e-03\n",
    "epoch:248;\t train:1.650590e-03;0.998244;1.682914e-03;\t test:1.423435e-03;0.998339;1.572022e-03\n",
    "epoch:249;\t train:1.645682e-03;0.998235;1.683498e-03;\t test:1.407725e-03;0.998322;1.550999e-03\n",
    "epoch:250;\t train:1.641408e-03;0.998240;1.679990e-03;\t test:1.425327e-03;0.998325;1.572174e-03\n",
    "epoch:251;\t train:1.654111e-03;0.998231;1.692366e-03;\t test:1.418937e-03;0.998340;1.564306e-03\n",
    "epoch:252;\t train:1.643378e-03;0.998239;1.682551e-03;\t test:1.418929e-03;0.998326;1.559519e-03\n",
    "epoch:253;\t train:1.643181e-03;0.998237;1.683886e-03;\t test:1.420611e-03;0.998342;1.565686e-03\n",
    "epoch:254;\t train:1.643513e-03;0.998239;1.680531e-03;\t test:1.405401e-03;0.998347;1.547648e-03\n",
    "epoch:255;\t train:1.641228e-03;0.998241;1.680498e-03;\t test:1.420321e-03;0.998336;1.567622e-03\n",
    "epoch:256;\t train:1.634806e-03;0.998244;1.676281e-03;\t test:1.403017e-03;0.998356;1.545708e-03\n",
    "epoch:257;\t train:1.645616e-03;0.998235;1.686349e-03;\t test:1.419455e-03;0.998333;1.565414e-03\n",
    "epoch:258;\t train:1.635819e-03;0.998242;1.675610e-03;\t test:1.406290e-03;0.998350;1.552771e-03\n",
    "epoch:259;\t train:1.628754e-03;0.998254;1.666936e-03;\t test:1.411178e-03;0.998357;1.556054e-03\n",
    "epoch:260;\t train:1.640932e-03;0.998241;1.681651e-03;\t test:1.383521e-03;0.998357;1.529279e-03\n",
    "epoch:261;\t train:1.633902e-03;0.998241;1.675909e-03;\t test:1.398486e-03;0.998360;1.540001e-03\n",
    "epoch:262;\t train:1.635806e-03;0.998244;1.675716e-03;\t test:1.424579e-03;0.998329;1.577889e-03\n",
    "epoch:263;\t train:1.638327e-03;0.998237;1.685783e-03;\t test:1.400585e-03;0.998360;1.543792e-03\n",
    "epoch:264;\t train:1.634375e-03;0.998242;1.676536e-03;\t test:1.396372e-03;0.998342;1.542112e-03\n",
    "epoch:265;\t train:1.637087e-03;0.998240;1.681511e-03;\t test:1.421243e-03;0.998331;1.579732e-03\n",
    "epoch:266;\t train:1.633503e-03;0.998237;1.678463e-03;\t test:1.414349e-03;0.998339;1.563236e-03\n",
    "epoch:267;\t train:1.631347e-03;0.998248;1.671369e-03;\t test:1.394611e-03;0.998342;1.540888e-03\n",
    "epoch:268;\t train:1.627483e-03;0.998250;1.671508e-03;\t test:1.407253e-03;0.998343;1.557412e-03\n",
    "epoch:269;\t train:1.628028e-03;0.998247;1.671178e-03;\t test:1.395845e-03;0.998337;1.537964e-03\n",
    "epoch:270;\t train:1.626509e-03;0.998244;1.670196e-03;\t test:1.413332e-03;0.998339;1.564274e-03\n",
    "epoch:271;\t train:1.636474e-03;0.998238;1.683535e-03;\t test:1.394327e-03;0.998382;1.536060e-03\n",
    "epoch:272;\t train:1.620193e-03;0.998254;1.664151e-03;\t test:1.393123e-03;0.998357;1.538919e-03\n",
    "epoch:273;\t train:1.618817e-03;0.998252;1.665225e-03;\t test:1.387754e-03;0.998373;1.528793e-03\n",
    "epoch:274;\t train:1.626165e-03;0.998247;1.675829e-03;\t test:1.399803e-03;0.998364;1.549808e-03\n",
    "epoch:275;\t train:1.630216e-03;0.998240;1.678850e-03;\t test:1.402298e-03;0.998345;1.555032e-03\n",
    "epoch:276;\t train:1.620656e-03;0.998248;1.669202e-03;\t test:1.404396e-03;0.998342;1.560918e-03\n",
    "epoch:277;\t train:1.612527e-03;0.998259;1.661846e-03;\t test:1.372243e-03;0.998369;1.518874e-03\n",
    "epoch:278;\t train:1.614755e-03;0.998260;1.658496e-03;\t test:1.383635e-03;0.998376;1.532814e-03\n",
    "epoch:279;\t train:1.607864e-03;0.998262;1.655128e-03;\t test:1.375148e-03;0.998374;1.523565e-03\n",
    "epoch:280;\t train:1.615390e-03;0.998258;1.662699e-03;\t test:1.379993e-03;0.998322;1.531902e-03\n",
    "epoch:281;\t train:1.598006e-03;0.998267;1.646711e-03;\t test:1.401012e-03;0.998340;1.552453e-03\n",
    "epoch:282;\t train:1.604615e-03;0.998261;1.655118e-03;\t test:1.386075e-03;0.998369;1.525017e-03\n",
    "epoch:283;\t train:1.610608e-03;0.998261;1.657411e-03;\t test:1.388142e-03;0.998362;1.544350e-03\n",
    "epoch:284;\t train:1.608278e-03;0.998251;1.664319e-03;\t test:1.395298e-03;0.998354;1.548516e-03\n",
    "epoch:285;\t train:1.604283e-03;0.998261;1.653410e-03;\t test:1.407892e-03;0.998316;1.563377e-03\n",
    "epoch:286;\t train:1.607097e-03;0.998258;1.657966e-03;\t test:1.387021e-03;0.998360;1.528410e-03\n",
    "epoch:287;\t train:1.610771e-03;0.998248;1.666247e-03;\t test:1.387916e-03;0.998349;1.534173e-03\n",
    "epoch:288;\t train:1.610166e-03;0.998257;1.663164e-03;\t test:1.365820e-03;0.998387;1.520605e-03\n",
    "epoch:289;\t train:1.593356e-03;0.998273;1.644315e-03;\t test:1.373437e-03;0.998376;1.513251e-03\n",
    "epoch:290;\t train:1.602618e-03;0.998255;1.659238e-03;\t test:1.369489e-03;0.998380;1.513361e-03\n",
    "epoch:291;\t train:1.617769e-03;0.998243;1.673367e-03;\t test:1.377886e-03;0.998351;1.537239e-03\n",
    "epoch:292;\t train:1.608244e-03;0.998254;1.663050e-03;\t test:1.406923e-03;0.998333;1.563990e-03\n",
    "epoch:293;\t train:1.599127e-03;0.998262;1.656221e-03;\t test:1.379223e-03;0.998350;1.528450e-03\n",
    "epoch:294;\t train:1.603525e-03;0.998265;1.655800e-03;\t test:1.393518e-03;0.998342;1.555665e-03\n",
    "epoch:295;\t train:1.603791e-03;0.998259;1.660959e-03;\t test:1.372708e-03;0.998357;1.524909e-03\n",
    "epoch:296;\t train:1.594265e-03;0.998270;1.647867e-03;\t test:1.384306e-03;0.998374;1.528939e-03\n",
    "epoch:297;\t train:1.600929e-03;0.998255;1.659407e-03;\t test:1.355039e-03;0.998372;1.505770e-03\n",
    "epoch:298;\t train:1.603558e-03;0.998260;1.660210e-03;\t test:1.395679e-03;0.998354;1.545938e-03\n",
    "epoch:299;\t train:1.591490e-03;0.998269;1.648149e-03;\t test:1.365717e-03;0.998361;1.521799e-03\n",
    "epoch:300;\t train:1.592906e-03;0.998260;1.653434e-03;\t test:1.370247e-03;0.998369;1.524302e-03\n",
    "epoch:301;\t train:1.596927e-03;0.998264;1.657762e-03;\t test:1.391387e-03;0.998375;1.548151e-03\n",
    "epoch:302;\t train:1.590225e-03;0.998269;1.646942e-03;\t test:1.376115e-03;0.998351;1.531693e-03\n",
    "epoch:303;\t train:1.605081e-03;0.998250;1.664326e-03;\t test:1.375193e-03;0.998345;1.533989e-03\n",
    "epoch:304;\t train:1.599104e-03;0.998263;1.657556e-03;\t test:1.358766e-03;0.998380;1.513341e-03\n",
    "epoch:305;\t train:1.587643e-03;0.998270;1.648986e-03;\t test:1.374157e-03;0.998393;1.530975e-03\n",
    "epoch:306;\t train:1.598770e-03;0.998257;1.661172e-03;\t test:1.345786e-03;0.998382;1.499503e-03\n",
    "epoch:307;\t train:1.596184e-03;0.998256;1.656073e-03;\t test:1.361767e-03;0.998376;1.524276e-03\n",
    "epoch:308;\t train:1.582903e-03;0.998274;1.645114e-03;\t test:1.371787e-03;0.998383;1.529040e-03\n",
    "epoch:309;\t train:1.586096e-03;0.998266;1.649948e-03;\t test:1.369102e-03;0.998372;1.531576e-03\n",
    "epoch:310;\t train:1.601601e-03;0.998259;1.662936e-03;\t test:1.347240e-03;0.998394;1.502866e-03\n",
    "epoch:311;\t train:1.592624e-03;0.998260;1.655799e-03;\t test:1.368380e-03;0.998382;1.525898e-03\n",
    "epoch:312;\t train:1.589571e-03;0.998269;1.649439e-03;\t test:1.387218e-03;0.998355;1.545239e-03\n",
    "epoch:313;\t train:1.584222e-03;0.998268;1.650391e-03;\t test:1.346071e-03;0.998404;1.500155e-03\n",
    "epoch:314;\t train:1.585283e-03;0.998274;1.647912e-03;\t test:1.383712e-03;0.998344;1.547088e-03\n",
    "epoch:315;\t train:1.578408e-03;0.998276;1.638354e-03;\t test:1.330969e-03;0.998419;1.478797e-03\n",
    "epoch:316;\t train:1.578897e-03;0.998273;1.641746e-03;\t test:1.357158e-03;0.998393;1.517469e-03\n",
    "epoch:317;\t train:1.585979e-03;0.998266;1.653053e-03;\t test:1.363929e-03;0.998369;1.530453e-03\n",
    "epoch:318;\t train:1.584572e-03;0.998274;1.645406e-03;\t test:1.339731e-03;0.998406;1.495573e-03\n",
    "epoch:319;\t train:1.578564e-03;0.998272;1.644533e-03;\t test:1.376337e-03;0.998368;1.543107e-03\n",
    "epoch:320;\t train:1.576737e-03;0.998276;1.640362e-03;\t test:1.353755e-03;0.998363;1.511107e-03\n",
    "epoch:321;\t train:1.582898e-03;0.998270;1.648683e-03;\t test:1.368156e-03;0.998387;1.529887e-03\n",
    "epoch:322;\t train:1.572718e-03;0.998277;1.642065e-03;\t test:1.334963e-03;0.998417;1.487650e-03\n",
    "epoch:323;\t train:1.578073e-03;0.998271;1.644370e-03;\t test:1.339034e-03;0.998411;1.495175e-03\n",
    "epoch:324;\t train:1.582885e-03;0.998267;1.649619e-03;\t test:1.365454e-03;0.998361;1.524782e-03\n",
    "epoch:325;\t train:1.572263e-03;0.998277;1.638533e-03;\t test:1.365501e-03;0.998362;1.535481e-03\n",
    "epoch:326;\t train:1.561324e-03;0.998285;1.625206e-03;\t test:1.347064e-03;0.998366;1.509403e-03\n",
    "epoch:327;\t train:1.575267e-03;0.998270;1.644404e-03;\t test:1.348328e-03;0.998411;1.503229e-03\n",
    "epoch:328;\t train:1.572273e-03;0.998278;1.637837e-03;\t test:1.342330e-03;0.998350;1.501151e-03\n",
    "epoch:329;\t train:1.578587e-03;0.998266;1.649271e-03;\t test:1.336420e-03;0.998411;1.494390e-03\n",
    "epoch:330;\t train:1.577276e-03;0.998270;1.648762e-03;\t test:1.344389e-03;0.998383;1.506215e-03\n",
    "epoch:331;\t train:1.567425e-03;0.998284;1.631968e-03;\t test:1.336872e-03;0.998390;1.499157e-03\n",
    "epoch:332;\t train:1.563783e-03;0.998281;1.633105e-03;\t test:1.343383e-03;0.998379;1.503749e-03\n",
    "epoch:333;\t train:1.572873e-03;0.998271;1.642605e-03;\t test:1.353550e-03;0.998365;1.515814e-03\n",
    "epoch:334;\t train:1.568275e-03;0.998282;1.637343e-03;\t test:1.361684e-03;0.998349;1.526319e-03\n",
    "epoch:335;\t train:1.559062e-03;0.998288;1.628444e-03;\t test:1.359845e-03;0.998388;1.520992e-03\n",
    "epoch:336;\t train:1.569957e-03;0.998278;1.640597e-03;\t test:1.354448e-03;0.998389;1.515691e-03\n",
    "epoch:337;\t train:1.564214e-03;0.998278;1.633245e-03;\t test:1.346820e-03;0.998365;1.513445e-03\n",
    "epoch:338;\t train:1.576530e-03;0.998267;1.651495e-03;\t test:1.342770e-03;0.998388;1.508621e-03\n",
    "epoch:339;\t train:1.571879e-03;0.998275;1.641416e-03;\t test:1.337725e-03;0.998363;1.494664e-03\n",
    "epoch:340;\t train:1.566179e-03;0.998274;1.637453e-03;\t test:1.342055e-03;0.998373;1.509350e-03\n",
    "epoch:341;\t train:1.558766e-03;0.998284;1.632856e-03;\t test:1.333819e-03;0.998395;1.500459e-03\n",
    "epoch:342;\t train:1.563988e-03;0.998285;1.634174e-03;\t test:1.340566e-03;0.998383;1.500006e-03\n",
    "epoch:343;\t train:1.556101e-03;0.998284;1.630315e-03;\t test:1.341609e-03;0.998384;1.505076e-03\n",
    "epoch:344;\t train:1.559959e-03;0.998282;1.631955e-03;\t test:1.340092e-03;0.998405;1.501261e-03\n",
    "epoch:345;\t train:1.552390e-03;0.998293;1.623843e-03;\t test:1.338597e-03;0.998378;1.499050e-03\n",
    "epoch:346;\t train:1.564561e-03;0.998271;1.642242e-03;\t test:1.336884e-03;0.998391;1.499148e-03\n",
    "epoch:347;\t train:1.552458e-03;0.998293;1.624484e-03;\t test:1.333455e-03;0.998401;1.494834e-03\n",
    "epoch:348;\t train:1.554068e-03;0.998276;1.628348e-03;\t test:1.360319e-03;0.998376;1.530314e-03\n",
    "epoch:349;\t train:1.553915e-03;0.998282;1.627691e-03;\t test:1.355187e-03;0.998371;1.520228e-03\n",
    "epoch:350;\t train:1.549806e-03;0.998294;1.620867e-03;\t test:1.338100e-03;0.998396;1.500731e-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = test_dataset[0]\n",
    "test_image2, test_label2 = train_transform(test_image, test_label)\n",
    "test_image2 = nd.expand_dims(test_image2,0)\n",
    "print('tensor shape:', test_image2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, cls_preds, box_preds = net(test_image2.as_in_context(ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predictions to real object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxDetection\n",
    "cls_probs = nd.SoftmaxActivation(nd.transpose(cls_preds, (0, 2, 1)), mode='channel')\n",
    "output = MultiBoxDetection(cls_prob=cls_probs, loc_pred=box_preds, anchor=anchors, force_suppress=True, clip=True, nms_topk=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('cluster')\n",
    "def display(img, out, thresh=0.5):\n",
    "    import random\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    mpl.rcParams['figure.figsize'] = (10,10)\n",
    "    img = img.asnumpy()\n",
    "    img = np.transpose(img,(2,3,1,0))\n",
    "    img = np.squeeze(img)\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    for det in out:\n",
    "        cid = int(det[0])\n",
    "        if cid == 0:\n",
    "            continue\n",
    "        score = det[1]\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        scales = [img.shape[1], img.shape[0]] * 2\n",
    "        xmin, ymin, xmax, ymax = [int(p * s) for p, s in zip(det[2:6].tolist(), scales)]\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False,\n",
    "                             edgecolor='red', linewidth=3)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[cid]\n",
    "        plt.gca().text(xmin, ymin-2, '{:s} {:.3f}'.format(text, score),\n",
    "                       bbox=dict(facecolor='red', alpha=0.5),\n",
    "                       fontsize=12, color='white')\n",
    "\n",
    "display(test_image2, output[0].asnumpy(), thresh=0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
