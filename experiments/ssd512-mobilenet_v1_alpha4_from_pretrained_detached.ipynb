{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.contrib.ndarray import MultiBoxPrior\n",
    "from mxnet.gluon.contrib import nn as nn_contrib\n",
    "from mxnet.gluon import nn\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict classes\n",
    "- channel `i*(num_class+1)` store the scores for this box contains only background\n",
    "- channel `i*(num_class+1)+1+j` store the scores for this box contains an object from the *j*-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predictor(num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict anchor boxes\n",
    "- $t_x = (Y_x - b_x) / b_{width}$\n",
    "- $t_y = (Y_y - b_y) / b_{height}$\n",
    "- $t_{width} = (Y_{width} - b_{width}) / b_{width}$\n",
    "- $t_{height} = (Y_{height} - b_{height}) / b_{height}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_predictor(num_anchors):\n",
    "    return nn.Conv2D(num_anchors * 4, 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage preditions from multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    return nd.flatten(nd.transpose(pred, axes=(0, 2, 3, 1)))\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    return nd.concat(*preds, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_layer(nfilters, stride, expension_constant):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(nn.Conv2D(nfilters, 3, strides=stride, padding=1, groups=nfilters, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(nfilters*expension_constant, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global alpha\n",
    "alpha = 0.25\n",
    "num_filters = int(32*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "def s16():\n",
    "    out = nn.HybridSequential()\n",
    "    # conv_0 layer\n",
    "    out.add(nn.Conv2D(num_filters, 3, strides=2, padding=1, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_1 layer\n",
    "    out.add(dp_layer(num_filters, 1, 2))\n",
    "    # conv_2 layer\n",
    "    out.add(dp_layer(num_filters*2, 2, 2))\n",
    "    # conv_3 layer\n",
    "    out.add(dp_layer(num_filters*4, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*4, 3, strides=2, padding=1, groups=num_filters*4, use_bias=False))\n",
    "    out.load_parameters(\"weights/mobilenet_0_25_s16_org.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def s32():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*8, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_4_layer\n",
    "    out.add(dp_layer(num_filters*8, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*8, 3, strides=2, padding=1, groups=num_filters*8, use_bias=False))\n",
    "    out.load_parameters(\"weights/mobilenet_0_25_s32_org.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b1():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_6_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b2():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_7_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b3():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_8_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b4():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_9_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_model(num_anchors, num_classes):\n",
    "    class_preds = nn.Sequential()\n",
    "    box_preds = nn.Sequential()\n",
    "    \n",
    "    for scale in range(6):\n",
    "        class_preds.add(class_predictor(num_anchors, num_classes))\n",
    "        box_preds.add(box_predictor(num_anchors))\n",
    "    \n",
    "    class_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    box_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    return s16(), s32(), b1(), b2(), b3(), b4(), class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_forward(x, s16, s32, b1, b2, b3, b4, class_preds, box_preds, sizes, ratios):\n",
    "    default_anchors = []\n",
    "    predicted_boxes = []  \n",
    "    predicted_classes = []\n",
    "\n",
    "    x = s16(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[0], ratios=ratios[0]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[0](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[0](x)))\n",
    "    \n",
    "    x = s32(x).detach()\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[1], ratios=ratios[1]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[1](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[1](x)))\n",
    "    \n",
    "    x = b1(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[2], ratios=ratios[2]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[2](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[2](x)))\n",
    "    \n",
    "    x = b2(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[3], ratios=ratios[3]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[3](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[3](x)))\n",
    "    \n",
    "    x = b3(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[4], ratios=ratios[4]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[4](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[4](x)))\n",
    "    \n",
    "    x = b4(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[5], ratios=ratios[5]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[5](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[5](x)))\n",
    "\n",
    "    return default_anchors, predicted_classes, predicted_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "class SSD(gluon.Block):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(SSD, self).__init__(**kwargs)\n",
    "        self.anchor_sizes = [[0.04, 0.1],[0.1,0.26],[0.26,0.42],[0.42,0.58],[0.58,0.74],[0.74,0.9],[0.9,1.06]]\n",
    "        self.anchor_ratios = [[1, 2, .5]] * 6\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.s16, self.s32, self.b1, self.b2, self.b3, self.b4, self.class_preds, self.box_preds = ssd_model(4, num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        default_anchors, predicted_classes, predicted_boxes = ssd_forward(x, self.s16, self.s32, self.b1, self.b2, self.b3, self.b4,\n",
    "            self.class_preds, self.box_preds, self.anchor_sizes, self.anchor_ratios)\n",
    "        anchors = concat_predictions(default_anchors)\n",
    "        box_preds = concat_predictions(predicted_boxes)\n",
    "        class_preds = concat_predictions(predicted_classes)\n",
    "        class_preds = nd.reshape(class_preds, shape=(0, -1, self.num_classes + 1))\n",
    "        \n",
    "        return anchors, class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = SSD(2)\n",
    "#net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "net.load_parameters(\"process/ssd_324.params\",ctx=ctx)\n",
    "x = nd.zeros((1, 3, 512, 512),ctx=ctx)\n",
    "default_anchors, class_predictions, box_predictions = net(x)\n",
    "print('Outputs:', 'anchors', default_anchors.shape, 'class prediction', class_predictions.shape, 'box prediction', box_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.NACDDetection import NACDDetection\n",
    "\n",
    "train_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'train')])\n",
    "test_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'test')])\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Test images:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source import NACDTransform\n",
    "width, height = 512, 512\n",
    "train_transform = NACDTransform.NACDDefaultTransform(width, height, False)\n",
    "test_transform = NACDTransform.NACDDefaultTransform(width, height, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd\n",
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = test_dataset[0]\n",
    "bboxes = train_label[:, :4]\n",
    "cids = train_label[:, 4:5]\n",
    "print('image:', train_image.shape)\n",
    "print('bboxes:', bboxes.shape, 'class ids:', cids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2, train_label2 = train_transform(train_image, train_label)\n",
    "print('tensor shape:', train_image2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset.transform(test_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(test_loader):\n",
    "    if ib > 3:\n",
    "        break\n",
    "    print('data:', batch[0].shape, 'label:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array((0.485, 0.456, 0.406))\n",
    "train_image2 = (train_image2 * 255).clip(0, 255)\n",
    "ax = viz.plot_bbox(train_image2.asnumpy(), train_label2[:, :4],\n",
    "                   labels=train_label2[:, 4:5],\n",
    "                   class_names=train_dataset.classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxTarget\n",
    "def training_targets(default_anchors, class_predicts, labels):\n",
    "    class_predicts = nd.transpose(class_predicts, axes=(0, 2, 1))\n",
    "    z = MultiBoxTarget(anchor=default_anchors.as_in_context(mx.cpu()), label=labels.as_in_context(mx.cpu()), cls_pred=class_predicts.as_in_context(mx.cpu()))\n",
    "    box_target = z[0].as_in_context(ctx)  # box offset target for (x, y, width, height)\n",
    "    box_mask = z[1].as_in_context(ctx)  # mask is used to ignore box offsets we don't want to penalize, e.g. negative samples\n",
    "    cls_target = z[2].as_in_context(ctx)  # cls_target is an array of labels for all anchors boxes\n",
    "    return box_target, box_mask, cls_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertlbl(y):\n",
    "    mtrx = y[:,:,0:4]\n",
    "    mtrx = mtrx.asnumpy()\n",
    "    mtrx[mtrx == -1] = -width\n",
    "    mtrx = mtrx/512\n",
    "    return mx.nd.concat(nd.expand_dims(y[:,:,4],2),mx.nd.array(mtrx),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(gluon.loss.Loss):\n",
    "    def __init__(self, axis=-1, alpha=0.25, gamma=2, batch_axis=0, **kwargs):\n",
    "        super(FocalLoss, self).__init__(None, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label):\n",
    "        output = F.softmax(output)\n",
    "        pt = F.pick(output, label, axis=self._axis, keepdims=True)\n",
    "        loss = -self._alpha * ((1 - pt) ** self._gamma) * F.log(pt)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "# cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "cls_loss = FocalLoss()\n",
    "print(cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(gluon.loss.Loss):\n",
    "    def __init__(self, batch_axis=0, **kwargs):\n",
    "        super(SmoothL1Loss, self).__init__(None, batch_axis, **kwargs)\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label, mask):\n",
    "        loss = F.smooth_l1((output - label) * mask, scalar=1.0)\n",
    "        return F.mean(loss, self._batch_axis, exclude=True)\n",
    "\n",
    "box_loss = SmoothL1Loss()\n",
    "print(box_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mxnet import autograd as ag\n",
    "from gluoncv.loss import SSDMultiBoxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop params\n",
    "epochs = 351\n",
    "start_epoch = 325\n",
    "\n",
    "# initialize trainer\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 1e-1, 'wd': 4e-5})\n",
    "\n",
    "# evaluation metrics\n",
    "cls_metric = mx.metric.Accuracy()\n",
    "box_metric = mx.metric.MAE()\n",
    "cls_metric_test = mx.metric.Accuracy()\n",
    "box_metric_test = mx.metric.MAE()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # reset iterator and tick\n",
    "    #train_data.reset()\n",
    "    cls_metric.reset()\n",
    "    box_metric.reset()\n",
    "    tic = time.time()\n",
    "    train_loss = 0\n",
    "    # iterate through all batch\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # record gradients\n",
    "        with ag.record():\n",
    "            x = batch[0].as_in_context(ctx)\n",
    "            y = batch[1].as_in_context(ctx)\n",
    "            lbl = convertlbl(batch[1])\n",
    "            default_anchors, class_predictions, box_predictions = net(x)\n",
    "            box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "            # losses\n",
    "            loss1 = cls_loss(class_predictions, cls_target)\n",
    "            loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "            # sum all losses\n",
    "            loss = loss1 + loss2\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "        # apply \n",
    "        trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        # update metrics\n",
    "        cls_metric.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_train, val1_train = cls_metric.get()\n",
    "    name2_train, val2_train = box_metric.get()\n",
    "\n",
    "    cls_metric_test.reset()\n",
    "    box_metric_test.reset()\n",
    "    tic = time.time()\n",
    "    test_loss = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        # record gradients\n",
    "        x = batch[0].as_in_context(ctx)\n",
    "        y = batch[1].as_in_context(ctx)\n",
    "        lbl = convertlbl(batch[1])\n",
    "        default_anchors, class_predictions, box_predictions = net(x)\n",
    "        box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "        # losses\n",
    "        loss1 = cls_loss(class_predictions, cls_target)\n",
    "        loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "        # sum all losses\n",
    "        loss = loss1 + loss2\n",
    "        test_loss += nd.sum(loss).asscalar()\n",
    "        # update metrics\n",
    "        cls_metric_test.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric_test.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_test, val1_test = cls_metric_test.get()\n",
    "    name2_test, val2_test = box_metric_test.get()\n",
    "    print('epoch:%3d;\\t train:%.6e;%f;%.6e;\\t test:%.6e;%f;%.6e'\n",
    "          %(epoch, train_loss/len(train_dataset), val1_train, val2_train, test_loss/len(test_dataset), val1_test, val2_test))\n",
    "\n",
    "    net.save_parameters('process/ssd_%d.params' % epoch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch:350;\t train:1.690522e-03;0.998209;1.713183e-03;\t test:1.453554e-03;0.998308;1.591727e-03\n",
    "epoch:  1;\t train:1.002903e-02;0.984111;2.047690e-03;\t test:5.746536e-03;0.994749;1.942699e-03\n",
    "epoch:  2;\t train:5.122083e-03;0.996052;2.014267e-03;\t test:4.158919e-03;0.996777;1.945308e-03\n",
    "epoch:  3;\t train:4.115885e-03;0.996998;2.006182e-03;\t test:3.586820e-03;0.997243;1.957495e-03\n",
    "epoch:  4;\t train:3.642940e-03;0.997353;1.992412e-03;\t test:3.257105e-03;0.997548;1.933163e-03\n",
    "epoch:  5;\t train:3.361986e-03;0.997591;1.986672e-03;\t test:3.050271e-03;0.997731;1.927993e-03\n",
    "epoch:  6;\t train:3.179704e-03;0.997742;1.993188e-03;\t test:2.870556e-03;0.997832;1.948751e-03\n",
    "epoch:  7;\t train:3.026552e-03;0.997836;1.994354e-03;\t test:2.762228e-03;0.997871;1.973043e-03\n",
    "epoch:  8;\t train:2.918137e-03;0.997896;1.995535e-03;\t test:2.639039e-03;0.997969;1.926807e-03\n",
    "epoch:  9;\t train:2.800025e-03;0.997958;1.968611e-03;\t test:2.560860e-03;0.998000;1.916212e-03\n",
    "epoch: 10;\t train:2.745866e-03;0.997966;1.988452e-03;\t test:2.477135e-03;0.998011;1.926637e-03\n",
    "epoch: 11;\t train:2.685534e-03;0.997989;1.981464e-03;\t test:2.429989e-03;0.998039;1.905448e-03\n",
    "epoch: 12;\t train:2.629513e-03;0.998009;1.978693e-03;\t test:2.409681e-03;0.998040;1.932247e-03\n",
    "epoch: 13;\t train:2.590268e-03;0.998022;1.978630e-03;\t test:2.363134e-03;0.998049;1.923806e-03\n",
    "epoch: 14;\t train:2.555017e-03;0.998037;1.976086e-03;\t test:2.316772e-03;0.998084;1.899406e-03\n",
    "epoch: 15;\t train:2.518372e-03;0.998055;1.968113e-03;\t test:2.322547e-03;0.998059;1.935502e-03\n",
    "epoch: 16;\t train:2.482499e-03;0.998078;1.954324e-03;\t test:2.248151e-03;0.998131;1.875119e-03\n",
    "epoch: 17;\t train:2.464820e-03;0.998079;1.962848e-03;\t test:2.234394e-03;0.998117;1.896303e-03\n",
    "epoch: 18;\t train:2.434244e-03;0.998093;1.954790e-03;\t test:2.210113e-03;0.998115;1.880998e-03\n",
    "epoch: 19;\t train:2.426601e-03;0.998088;1.968211e-03;\t test:2.218708e-03;0.998102;1.904877e-03\n",
    "epoch: 20;\t train:2.394774e-03;0.998111;1.951217e-03;\t test:2.195206e-03;0.998108;1.912405e-03\n",
    "epoch: 21;\t train:2.396055e-03;0.998094;1.975308e-03;\t test:2.176532e-03;0.998117;1.904790e-03\n",
    "epoch: 22;\t train:2.359146e-03;0.998123;1.946865e-03;\t test:2.163917e-03;0.998123;1.901380e-03\n",
    "epoch: 23;\t train:2.348468e-03;0.998119;1.951500e-03;\t test:2.147160e-03;0.998132;1.904521e-03\n",
    "epoch: 24;\t train:2.340762e-03;0.998125;1.950650e-03;\t test:2.120751e-03;0.998146;1.895721e-03\n",
    "epoch: 25;\t train:2.323498e-03;0.998126;1.951553e-03;\t test:2.097069e-03;0.998153;1.882868e-03\n",
    "epoch: 26;\t train:2.305909e-03;0.998135;1.944785e-03;\t test:2.105668e-03;0.998112;1.896440e-03\n",
    "epoch: 27;\t train:2.297457e-03;0.998134;1.945246e-03;\t test:2.031822e-03;0.998185;1.823785e-03\n",
    "epoch: 28;\t train:2.281690e-03;0.998136;1.943818e-03;\t test:2.099101e-03;0.998123;1.907131e-03\n",
    "epoch: 29;\t train:2.283539e-03;0.998128;1.954131e-03;\t test:2.035385e-03;0.998181;1.841522e-03\n",
    "epoch: 30;\t train:2.264766e-03;0.998140;1.939487e-03;\t test:2.048578e-03;0.998138;1.873057e-03\n",
    "epoch: 31;\t train:2.255104e-03;0.998140;1.943164e-03;\t test:2.031664e-03;0.998185;1.874914e-03\n",
    "epoch: 32;\t train:2.240545e-03;0.998147;1.933459e-03;\t test:2.014790e-03;0.998176;1.866488e-03\n",
    "epoch: 33;\t train:2.230103e-03;0.998152;1.932350e-03;\t test:2.018795e-03;0.998157;1.871436e-03\n",
    "epoch: 34;\t train:2.230093e-03;0.998139;1.940400e-03;\t test:2.000770e-03;0.998148;1.865446e-03\n",
    "epoch: 35;\t train:2.206527e-03;0.998158;1.921172e-03;\t test:1.998501e-03;0.998146;1.878028e-03\n",
    "epoch: 36;\t train:2.204732e-03;0.998145;1.932354e-03;\t test:1.999186e-03;0.998160;1.874025e-03\n",
    "epoch: 37;\t train:2.200118e-03;0.998143;1.935968e-03;\t test:2.009997e-03;0.998142;1.905375e-03\n",
    "epoch: 38;\t train:2.194652e-03;0.998150;1.927348e-03;\t test:1.985388e-03;0.998164;1.889806e-03\n",
    "epoch: 39;\t train:2.172741e-03;0.998156;1.917582e-03;\t test:1.951411e-03;0.998185;1.852844e-03\n",
    "epoch: 40;\t train:2.179562e-03;0.998149;1.927582e-03;\t test:1.940012e-03;0.998185;1.847507e-03\n",
    "epoch: 41;\t train:2.169941e-03;0.998150;1.925222e-03;\t test:1.931786e-03;0.998193;1.845692e-03\n",
    "epoch: 42;\t train:2.168150e-03;0.998141;1.932707e-03;\t test:1.952712e-03;0.998138;1.871275e-03\n",
    "epoch: 43;\t train:2.149794e-03;0.998158;1.913032e-03;\t test:1.932799e-03;0.998154;1.864463e-03\n",
    "epoch: 44;\t train:2.142715e-03;0.998153;1.916753e-03;\t test:1.931318e-03;0.998165;1.876147e-03\n",
    "epoch: 45;\t train:2.131359e-03;0.998156;1.910873e-03;\t test:1.922069e-03;0.998166;1.854750e-03\n",
    "epoch: 46;\t train:2.132360e-03;0.998155;1.909670e-03;\t test:1.916823e-03;0.998147;1.859603e-03\n",
    "epoch: 47;\t train:2.130809e-03;0.998146;1.920291e-03;\t test:1.902708e-03;0.998166;1.858602e-03\n",
    "epoch: 48;\t train:2.128023e-03;0.998152;1.917381e-03;\t test:1.905555e-03;0.998168;1.857427e-03\n",
    "epoch: 49;\t train:2.119254e-03;0.998149;1.917468e-03;\t test:1.880664e-03;0.998149;1.839734e-03\n",
    "epoch: 50;\t train:2.107825e-03;0.998151;1.910405e-03;\t test:1.887577e-03;0.998154;1.849507e-03\n",
    "epoch: 51;\t train:2.108101e-03;0.998156;1.909219e-03;\t test:1.868453e-03;0.998181;1.835962e-03\n",
    "epoch: 52;\t train:2.098503e-03;0.998155;1.905297e-03;\t test:1.854214e-03;0.998190;1.829656e-03\n",
    "epoch: 53;\t train:2.096823e-03;0.998147;1.912300e-03;\t test:1.859028e-03;0.998188;1.839276e-03\n",
    "epoch: 54;\t train:2.089872e-03;0.998154;1.906001e-03;\t test:1.859818e-03;0.998172;1.848861e-03\n",
    "epoch: 55;\t train:2.087610e-03;0.998148;1.907084e-03;\t test:1.864877e-03;0.998173;1.858938e-03\n",
    "epoch: 56;\t train:2.080489e-03;0.998158;1.897917e-03;\t test:1.815416e-03;0.998198;1.801192e-03\n",
    "epoch: 57;\t train:2.064108e-03;0.998164;1.890293e-03;\t test:1.828326e-03;0.998197;1.811865e-03\n",
    "epoch: 58;\t train:2.057797e-03;0.998164;1.888865e-03;\t test:1.869446e-03;0.998161;1.858957e-03\n",
    "epoch: 59;\t train:2.059070e-03;0.998163;1.888297e-03;\t test:1.820740e-03;0.998190;1.813991e-03\n",
    "epoch: 60;\t train:2.051837e-03;0.998163;1.887794e-03;\t test:1.808352e-03;0.998199;1.823497e-03\n",
    "epoch: 61;\t train:2.045992e-03;0.998164;1.886633e-03;\t test:1.810524e-03;0.998215;1.815451e-03\n",
    "epoch: 62;\t train:2.052842e-03;0.998155;1.894207e-03;\t test:1.818566e-03;0.998187;1.831964e-03\n",
    "epoch: 63;\t train:2.040597e-03;0.998161;1.885673e-03;\t test:1.818533e-03;0.998175;1.812608e-03\n",
    "epoch: 64;\t train:2.040920e-03;0.998165;1.886023e-03;\t test:1.827099e-03;0.998160;1.840776e-03\n",
    "epoch: 65;\t train:2.040792e-03;0.998157;1.890589e-03;\t test:1.794417e-03;0.998209;1.811197e-03\n",
    "epoch: 66;\t train:2.026203e-03;0.998165;1.881130e-03;\t test:1.808094e-03;0.998193;1.813372e-03\n",
    "epoch: 67;\t train:2.035408e-03;0.998157;1.886945e-03;\t test:1.805638e-03;0.998182;1.835888e-03\n",
    "epoch: 68;\t train:2.032018e-03;0.998155;1.890282e-03;\t test:1.811074e-03;0.998169;1.839679e-03\n",
    "epoch: 69;\t train:2.028810e-03;0.998156;1.888173e-03;\t test:1.793968e-03;0.998185;1.812900e-03\n",
    "epoch: 70;\t train:2.023154e-03;0.998154;1.885309e-03;\t test:1.771787e-03;0.998227;1.796238e-03\n",
    "epoch: 71;\t train:2.021503e-03;0.998154;1.887850e-03;\t test:1.792615e-03;0.998188;1.826357e-03\n",
    "epoch: 72;\t train:2.022849e-03;0.998149;1.889344e-03;\t test:1.757431e-03;0.998224;1.792543e-03\n",
    "epoch: 73;\t train:2.015562e-03;0.998157;1.884330e-03;\t test:1.775190e-03;0.998223;1.804870e-03\n",
    "epoch: 74;\t train:2.006983e-03;0.998155;1.882903e-03;\t test:1.763017e-03;0.998211;1.800456e-03\n",
    "epoch: 75;\t train:2.020821e-03;0.998145;1.892094e-03;\t test:1.750710e-03;0.998229;1.787568e-03\n",
    "epoch: 76;\t train:2.003226e-03;0.998159;1.879741e-03;\t test:1.813162e-03;0.998158;1.845570e-03\n",
    "epoch: 77;\t train:1.998807e-03;0.998166;1.873810e-03;\t test:1.772389e-03;0.998205;1.810063e-03\n",
    "epoch: 78;\t train:1.997219e-03;0.998160;1.870373e-03;\t test:1.753388e-03;0.998214;1.797341e-03\n",
    "epoch: 79;\t train:1.995659e-03;0.998163;1.870302e-03;\t test:1.780137e-03;0.998179;1.822682e-03\n",
    "epoch: 80;\t train:1.990720e-03;0.998170;1.866116e-03;\t test:1.742971e-03;0.998219;1.776438e-03\n",
    "epoch: 81;\t train:1.981999e-03;0.998167;1.862254e-03;\t test:1.762739e-03;0.998200;1.797690e-03\n",
    "epoch: 82;\t train:1.991986e-03;0.998157;1.871911e-03;\t test:1.764326e-03;0.998192;1.808403e-03\n",
    "epoch: 83;\t train:1.997812e-03;0.998155;1.875582e-03;\t test:1.758811e-03;0.998206;1.794046e-03\n",
    "epoch: 84;\t train:1.989937e-03;0.998158;1.870033e-03;\t test:1.777595e-03;0.998178;1.820049e-03\n",
    "epoch: 85;\t train:1.987321e-03;0.998154;1.871462e-03;\t test:1.728841e-03;0.998219;1.768889e-03\n",
    "epoch: 86;\t train:1.982335e-03;0.998161;1.862928e-03;\t test:1.748810e-03;0.998185;1.775822e-03\n",
    "epoch: 87;\t train:1.980397e-03;0.998155;1.865127e-03;\t test:1.757714e-03;0.998152;1.793016e-03\n",
    "epoch: 88;\t train:1.989974e-03;0.998149;1.869405e-03;\t test:1.755392e-03;0.998174;1.792334e-03\n",
    "epoch: 89;\t train:1.990662e-03;0.998145;1.873287e-03;\t test:1.729684e-03;0.998203;1.766682e-03\n",
    "epoch: 90;\t train:1.962742e-03;0.998163;1.848667e-03;\t test:1.751899e-03;0.998178;1.792605e-03\n",
    "epoch: 91;\t train:1.990244e-03;0.998137;1.874612e-03;\t test:1.753982e-03;0.998150;1.796528e-03\n",
    "epoch: 92;\t train:1.980691e-03;0.998143;1.863357e-03;\t test:1.730277e-03;0.998180;1.772049e-03\n",
    "epoch: 93;\t train:1.961053e-03;0.998153;1.852505e-03;\t test:1.733255e-03;0.998083;1.778938e-03\n",
    "epoch: 94;\t train:1.963429e-03;0.998155;1.853799e-03;\t test:1.711934e-03;0.998179;1.752221e-03\n",
    "epoch: 95;\t train:1.966424e-03;0.998145;1.858364e-03;\t test:1.721451e-03;0.998181;1.770960e-03\n",
    "epoch: 96;\t train:1.952300e-03;0.998156;1.843327e-03;\t test:1.719370e-03;0.998142;1.757622e-03\n",
    "epoch: 97;\t train:1.960189e-03;0.998142;1.856211e-03;\t test:1.682989e-03;0.998192;1.721403e-03\n",
    "epoch: 98;\t train:1.958252e-03;0.998139;1.854754e-03;\t test:1.731103e-03;0.998117;1.782502e-03\n",
    "epoch: 99;\t train:1.954638e-03;0.998146;1.851667e-03;\t test:1.725498e-03;0.998126;1.781678e-03\n",
    "epoch:100;\t train:1.953322e-03;0.998133;1.856186e-03;\t test:1.694644e-03;0.998159;1.744757e-03\n",
    "epoch:101;\t train:1.943179e-03;0.998144;1.844535e-03;\t test:1.715612e-03;0.998132;1.764899e-03\n",
    "epoch:102;\t train:1.954241e-03;0.998135;1.855306e-03;\t test:1.695352e-03;0.998150;1.749191e-03\n",
    "epoch:103;\t train:1.945139e-03;0.998137;1.851659e-03;\t test:1.717216e-03;0.998123;1.776697e-03\n",
    "epoch:104;\t train:1.931355e-03;0.998152;1.836794e-03;\t test:1.685371e-03;0.998156;1.738005e-03\n",
    "epoch:105;\t train:1.936551e-03;0.998142;1.843660e-03;\t test:1.690761e-03;0.998121;1.747775e-03\n",
    "epoch:106;\t train:1.921081e-03;0.998154;1.830559e-03;\t test:1.709978e-03;0.998156;1.770144e-03\n",
    "epoch:107;\t train:1.936845e-03;0.998136;1.845122e-03;\t test:1.686148e-03;0.998157;1.744587e-03\n",
    "epoch:108;\t train:1.929318e-03;0.998139;1.839771e-03;\t test:1.675502e-03;0.998151;1.732806e-03\n",
    "epoch:109;\t train:1.939895e-03;0.998133;1.853766e-03;\t test:1.690016e-03;0.998150;1.754226e-03\n",
    "epoch:110;\t train:1.929960e-03;0.998135;1.841760e-03;\t test:1.713079e-03;0.998132;1.776281e-03\n",
    "epoch:111;\t train:1.929822e-03;0.998131;1.844770e-03;\t test:1.694265e-03;0.998095;1.760949e-03\n",
    "epoch:112;\t train:1.917093e-03;0.998147;1.830715e-03;\t test:1.691566e-03;0.998087;1.759626e-03\n",
    "epoch:113;\t train:1.916312e-03;0.998141;1.834098e-03;\t test:1.666531e-03;0.998102;1.726122e-03\n",
    "epoch:114;\t train:1.911074e-03;0.998147;1.830532e-03;\t test:1.663442e-03;0.998162;1.729892e-03\n",
    "epoch:115;\t train:1.911402e-03;0.998151;1.823040e-03;\t test:1.658092e-03;0.998196;1.721349e-03\n",
    "epoch:116;\t train:1.918303e-03;0.998137;1.838066e-03;\t test:1.673793e-03;0.998134;1.748406e-03\n",
    "epoch:117;\t train:1.911440e-03;0.998140;1.832160e-03;\t test:1.692754e-03;0.998118;1.767667e-03\n",
    "epoch:118;\t train:1.916825e-03;0.998136;1.836078e-03;\t test:1.677202e-03;0.998094;1.743358e-03\n",
    "epoch:119;\t train:1.920097e-03;0.998121;1.848503e-03;\t test:1.666351e-03;0.998135;1.746627e-03\n",
    "epoch:120;\t train:1.914424e-03;0.998128;1.837948e-03;\t test:1.651306e-03;0.998142;1.723421e-03\n",
    "epoch:121;\t train:1.896957e-03;0.998148;1.818855e-03;\t test:1.658245e-03;0.998147;1.734311e-03\n",
    "epoch:122;\t train:1.908549e-03;0.998128;1.838986e-03;\t test:1.657721e-03;0.998131;1.729026e-03\n",
    "epoch:123;\t train:1.896982e-03;0.998142;1.819781e-03;\t test:1.647780e-03;0.998135;1.729232e-03\n",
    "epoch:124;\t train:1.895827e-03;0.998142;1.823920e-03;\t test:1.661999e-03;0.998135;1.734519e-03\n",
    "epoch:125;\t train:1.887153e-03;0.998149;1.816414e-03;\t test:1.644670e-03;0.998151;1.731617e-03\n",
    "epoch:126;\t train:1.897712e-03;0.998139;1.827147e-03;\t test:1.639329e-03;0.998169;1.715418e-03\n",
    "epoch:127;\t train:1.895814e-03;0.998142;1.824432e-03;\t test:1.627874e-03;0.998186;1.706438e-03\n",
    "epoch:128;\t train:1.895799e-03;0.998138;1.823287e-03;\t test:1.658539e-03;0.998149;1.738379e-03\n",
    "epoch:129;\t train:1.878531e-03;0.998150;1.811950e-03;\t test:1.649398e-03;0.998168;1.734297e-03\n",
    "epoch:130;\t train:1.885450e-03;0.998141;1.821387e-03;\t test:1.638303e-03;0.998107;1.714537e-03\n",
    "epoch:131;\t train:1.880693e-03;0.998149;1.811147e-03;\t test:1.618041e-03;0.998184;1.694441e-03\n",
    "epoch:132;\t train:1.874019e-03;0.998150;1.810857e-03;\t test:1.629968e-03;0.998200;1.707716e-03\n",
    "epoch:133;\t train:1.868994e-03;0.998159;1.801395e-03;\t test:1.628756e-03;0.998158;1.714573e-03\n",
    "epoch:134;\t train:1.881033e-03;0.998145;1.816334e-03;\t test:1.634526e-03;0.998173;1.716954e-03\n",
    "epoch:135;\t train:1.868711e-03;0.998150;1.805109e-03;\t test:1.630372e-03;0.998167;1.718356e-03\n",
    "epoch:136;\t train:1.864303e-03;0.998157;1.798962e-03;\t test:1.617469e-03;0.998187;1.698802e-03\n",
    "epoch:137;\t train:1.867939e-03;0.998149;1.808744e-03;\t test:1.613583e-03;0.998199;1.698332e-03\n",
    "epoch:138;\t train:1.871557e-03;0.998145;1.812230e-03;\t test:1.630715e-03;0.998190;1.716891e-03\n",
    "epoch:139;\t train:1.872445e-03;0.998145;1.812277e-03;\t test:1.610615e-03;0.998143;1.690451e-03\n",
    "epoch:140;\t train:1.854811e-03;0.998159;1.796019e-03;\t test:1.621604e-03;0.998153;1.712296e-03\n",
    "epoch:141;\t train:1.860801e-03;0.998151;1.805317e-03;\t test:1.602724e-03;0.998148;1.684232e-03\n",
    "epoch:142;\t train:1.860644e-03;0.998156;1.799364e-03;\t test:1.598672e-03;0.998175;1.665290e-03\n",
    "epoch:143;\t train:1.864659e-03;0.998148;1.807127e-03;\t test:1.585327e-03;0.998228;1.674713e-03\n",
    "epoch:144;\t train:1.853835e-03;0.998161;1.796249e-03;\t test:1.618917e-03;0.998146;1.708035e-03\n",
    "epoch:145;\t train:1.864695e-03;0.998145;1.808573e-03;\t test:1.634555e-03;0.998141;1.734841e-03\n",
    "epoch:146;\t train:1.861672e-03;0.998149;1.809963e-03;\t test:1.596537e-03;0.998221;1.689481e-03\n",
    "epoch:147;\t train:1.865247e-03;0.998145;1.806675e-03;\t test:1.598794e-03;0.998182;1.683632e-03\n",
    "epoch:148;\t train:1.850418e-03;0.998158;1.798633e-03;\t test:1.589635e-03;0.998204;1.684057e-03\n",
    "epoch:149;\t train:1.858032e-03;0.998144;1.803210e-03;\t test:1.614763e-03;0.998148;1.712959e-03\n",
    "epoch:150;\t train:1.860636e-03;0.998144;1.808382e-03;\t test:1.607246e-03;0.998203;1.704250e-03\n",
    "epoch:151;\t train:1.849296e-03;0.998157;1.797872e-03;\t test:1.613020e-03;0.998181;1.718088e-03\n",
    "epoch:152;\t train:1.840938e-03;0.998158;1.790994e-03;\t test:1.584873e-03;0.998202;1.682799e-03\n",
    "epoch:153;\t train:1.841827e-03;0.998152;1.794244e-03;\t test:1.609125e-03;0.998153;1.707358e-03\n",
    "epoch:154;\t train:1.841306e-03;0.998156;1.794335e-03;\t test:1.583778e-03;0.998220;1.670688e-03\n",
    "epoch:155;\t train:1.846844e-03;0.998154;1.795180e-03;\t test:1.593126e-03;0.998171;1.692642e-03\n",
    "epoch:156;\t train:1.839674e-03;0.998151;1.795880e-03;\t test:1.600195e-03;0.998163;1.693688e-03\n",
    "epoch:157;\t train:1.824583e-03;0.998162;1.782262e-03;\t test:1.595608e-03;0.998196;1.700926e-03\n",
    "epoch:158;\t train:1.838203e-03;0.998159;1.790671e-03;\t test:1.593838e-03;0.998193;1.695073e-03\n",
    "epoch:159;\t train:1.827051e-03;0.998160;1.785893e-03;\t test:1.615855e-03;0.998151;1.719201e-03\n",
    "epoch:160;\t train:1.851142e-03;0.998140;1.806569e-03;\t test:1.578987e-03;0.998187;1.680664e-03\n",
    "epoch:161;\t train:1.831937e-03;0.998161;1.789712e-03;\t test:1.588019e-03;0.998192;1.681004e-03\n",
    "epoch:162;\t train:1.844163e-03;0.998152;1.800990e-03;\t test:1.591232e-03;0.998219;1.692265e-03\n",
    "epoch:163;\t train:1.842043e-03;0.998162;1.790950e-03;\t test:1.601324e-03;0.998198;1.699747e-03\n",
    "epoch:164;\t train:1.837969e-03;0.998160;1.792163e-03;\t test:1.603356e-03;0.998207;1.690942e-03\n",
    "epoch:165;\t train:1.846346e-03;0.998149;1.803853e-03;\t test:1.587721e-03;0.998213;1.677376e-03\n",
    "epoch:166;\t train:1.837297e-03;0.998158;1.792761e-03;\t test:1.568567e-03;0.998237;1.665224e-03\n",
    "epoch:167;\t train:1.835512e-03;0.998161;1.791447e-03;\t test:1.589327e-03;0.998207;1.688690e-03\n",
    "epoch:168;\t train:1.831186e-03;0.998162;1.788966e-03;\t test:1.575018e-03;0.998225;1.666063e-03\n",
    "epoch:169;\t train:1.842132e-03;0.998154;1.801439e-03;\t test:1.571954e-03;0.998232;1.673233e-03\n",
    "epoch:170;\t train:1.828322e-03;0.998162;1.789940e-03;\t test:1.580853e-03;0.998227;1.687307e-03\n",
    "epoch:171;\t train:1.820319e-03;0.998170;1.779084e-03;\t test:1.579977e-03;0.998210;1.680598e-03\n",
    "epoch:172;\t train:1.830836e-03;0.998155;1.793155e-03;\t test:1.562688e-03;0.998241;1.668171e-03\n",
    "epoch:173;\t train:1.820198e-03;0.998168;1.782636e-03;\t test:1.568740e-03;0.998229;1.671266e-03\n",
    "epoch:174;\t train:1.819606e-03;0.998166;1.780707e-03;\t test:1.582507e-03;0.998217;1.693650e-03\n",
    "epoch:175;\t train:1.823897e-03;0.998162;1.785678e-03;\t test:1.569664e-03;0.998238;1.664093e-03\n",
    "epoch:176;\t train:1.827798e-03;0.998158;1.790407e-03;\t test:1.581193e-03;0.998212;1.682039e-03\n",
    "epoch:177;\t train:1.820898e-03;0.998165;1.785456e-03;\t test:1.586496e-03;0.998184;1.690340e-03\n",
    "epoch:178;\t train:1.807368e-03;0.998175;1.773954e-03;\t test:1.572898e-03;0.998206;1.669350e-03\n",
    "epoch:179;\t train:1.810130e-03;0.998177;1.774074e-03;\t test:1.535620e-03;0.998246;1.626742e-03\n",
    "epoch:180;\t train:1.814994e-03;0.998167;1.784587e-03;\t test:1.560427e-03;0.998198;1.670854e-03\n",
    "epoch:181;\t train:1.807340e-03;0.998174;1.775075e-03;\t test:1.571160e-03;0.998207;1.672838e-03\n",
    "epoch:182;\t train:1.811167e-03;0.998172;1.777658e-03;\t test:1.547320e-03;0.998237;1.653546e-03\n",
    "epoch:183;\t train:1.800482e-03;0.998176;1.771176e-03;\t test:1.569835e-03;0.998205;1.678245e-03\n",
    "epoch:184;\t train:1.801886e-03;0.998183;1.770331e-03;\t test:1.581770e-03;0.998194;1.691732e-03\n",
    "epoch:185;\t train:1.797343e-03;0.998181;1.770372e-03;\t test:1.567957e-03;0.998229;1.676617e-03\n",
    "epoch:186;\t train:1.809617e-03;0.998161;1.780887e-03;\t test:1.566822e-03;0.998225;1.677553e-03\n",
    "epoch:187;\t train:1.813314e-03;0.998160;1.782719e-03;\t test:1.563890e-03;0.998229;1.676464e-03\n",
    "epoch:188;\t train:1.796374e-03;0.998180;1.765604e-03;\t test:1.578847e-03;0.998185;1.677232e-03\n",
    "epoch:189;\t train:1.801822e-03;0.998168;1.775462e-03;\t test:1.568840e-03;0.998225;1.676276e-03\n",
    "epoch:190;\t train:1.798121e-03;0.998174;1.771072e-03;\t test:1.575585e-03;0.998192;1.693066e-03\n",
    "epoch:191;\t train:1.803812e-03;0.998163;1.779220e-03;\t test:1.558996e-03;0.998215;1.666251e-03\n",
    "epoch:192;\t train:1.803097e-03;0.998174;1.776263e-03;\t test:1.564459e-03;0.998222;1.669167e-03\n",
    "epoch:193;\t train:1.788690e-03;0.998179;1.767124e-03;\t test:1.524013e-03;0.998269;1.627497e-03\n",
    "epoch:194;\t train:1.788202e-03;0.998185;1.763345e-03;\t test:1.562719e-03;0.998211;1.672955e-03\n",
    "epoch:195;\t train:1.798360e-03;0.998172;1.773742e-03;\t test:1.557714e-03;0.998190;1.666660e-03\n",
    "epoch:196;\t train:1.794779e-03;0.998172;1.774666e-03;\t test:1.541646e-03;0.998232;1.646481e-03\n",
    "epoch:197;\t train:1.793054e-03;0.998173;1.773343e-03;\t test:1.544265e-03;0.998237;1.662242e-03\n",
    "epoch:198;\t train:1.784633e-03;0.998178;1.763463e-03;\t test:1.537685e-03;0.998250;1.639491e-03\n",
    "epoch:199;\t train:1.782220e-03;0.998181;1.762068e-03;\t test:1.544523e-03;0.998234;1.654601e-03\n",
    "epoch:200;\t train:1.790027e-03;0.998177;1.768274e-03;\t test:1.565101e-03;0.998189;1.678540e-03\n",
    "epoch:201;\t train:1.782100e-03;0.998183;1.760761e-03;\t test:1.542808e-03;0.998218;1.658197e-03\n",
    "epoch:202;\t train:1.783949e-03;0.998174;1.769176e-03;\t test:1.525767e-03;0.998271;1.625498e-03\n",
    "epoch:203;\t train:1.773732e-03;0.998192;1.752769e-03;\t test:1.543774e-03;0.998245;1.644498e-03\n",
    "epoch:204;\t train:1.784780e-03;0.998173;1.768209e-03;\t test:1.534188e-03;0.998255;1.640780e-03\n",
    "epoch:205;\t train:1.782438e-03;0.998180;1.764778e-03;\t test:1.537249e-03;0.998239;1.648456e-03\n",
    "epoch:206;\t train:1.785681e-03;0.998173;1.767851e-03;\t test:1.556616e-03;0.998211;1.673054e-03\n",
    "epoch:207;\t train:1.769575e-03;0.998191;1.753902e-03;\t test:1.550521e-03;0.998231;1.672052e-03\n",
    "epoch:208;\t train:1.771592e-03;0.998188;1.752227e-03;\t test:1.543502e-03;0.998244;1.652457e-03\n",
    "epoch:209;\t train:1.779689e-03;0.998182;1.761850e-03;\t test:1.552362e-03;0.998214;1.663456e-03\n",
    "epoch:210;\t train:1.762726e-03;0.998189;1.748592e-03;\t test:1.542146e-03;0.998251;1.657081e-03\n",
    "epoch:211;\t train:1.776202e-03;0.998177;1.761656e-03;\t test:1.515583e-03;0.998271;1.626596e-03\n",
    "epoch:212;\t train:1.761587e-03;0.998194;1.747275e-03;\t test:1.538395e-03;0.998252;1.656818e-03\n",
    "epoch:213;\t train:1.774969e-03;0.998178;1.765673e-03;\t test:1.515524e-03;0.998214;1.620721e-03\n",
    "epoch:214;\t train:1.768297e-03;0.998186;1.754121e-03;\t test:1.522365e-03;0.998243;1.637208e-03\n",
    "epoch:215;\t train:1.769923e-03;0.998176;1.760021e-03;\t test:1.536656e-03;0.998250;1.652982e-03\n",
    "epoch:216;\t train:1.763209e-03;0.998189;1.750210e-03;\t test:1.510286e-03;0.998260;1.621560e-03\n",
    "epoch:217;\t train:1.759772e-03;0.998197;1.747933e-03;\t test:1.506590e-03;0.998264;1.617763e-03\n",
    "epoch:218;\t train:1.765212e-03;0.998179;1.759524e-03;\t test:1.540150e-03;0.998247;1.663033e-03\n",
    "epoch:219;\t train:1.764837e-03;0.998186;1.750593e-03;\t test:1.533048e-03;0.998241;1.647866e-03\n",
    "epoch:220;\t train:1.770505e-03;0.998178;1.761365e-03;\t test:1.514563e-03;0.998275;1.626674e-03\n",
    "epoch:221;\t train:1.764589e-03;0.998186;1.752670e-03;\t test:1.518871e-03;0.998244;1.636653e-03\n",
    "epoch:222;\t train:1.759257e-03;0.998194;1.749770e-03;\t test:1.528623e-03;0.998253;1.651838e-03\n",
    "epoch:223;\t train:1.772357e-03;0.998171;1.765834e-03;\t test:1.507142e-03;0.998239;1.623362e-03\n",
    "epoch:224;\t train:1.767743e-03;0.998175;1.758732e-03;\t test:1.511740e-03;0.998245;1.633168e-03\n",
    "epoch:225;\t train:1.756408e-03;0.998192;1.748705e-03;\t test:1.540565e-03;0.998202;1.666358e-03\n",
    "epoch:226;\t train:1.759115e-03;0.998191;1.751559e-03;\t test:1.532813e-03;0.998244;1.661859e-03\n",
    "epoch:227;\t train:1.766721e-03;0.998182;1.759395e-03;\t test:1.525869e-03;0.998212;1.645211e-03\n",
    "epoch:228;\t train:1.756700e-03;0.998190;1.749363e-03;\t test:1.514568e-03;0.998275;1.627639e-03\n",
    "epoch:229;\t train:1.750771e-03;0.998194;1.745312e-03;\t test:1.537116e-03;0.998240;1.654483e-03\n",
    "epoch:230;\t train:1.756508e-03;0.998192;1.746640e-03;\t test:1.531464e-03;0.998226;1.649596e-03\n",
    "epoch:231;\t train:1.753310e-03;0.998190;1.746965e-03;\t test:1.530078e-03;0.998230;1.648386e-03\n",
    "epoch:232;\t train:1.749862e-03;0.998197;1.743219e-03;\t test:1.522035e-03;0.998250;1.642521e-03\n",
    "epoch:233;\t train:1.747920e-03;0.998192;1.744932e-03;\t test:1.528220e-03;0.998231;1.644647e-03\n",
    "epoch:234;\t train:1.750593e-03;0.998195;1.747346e-03;\t test:1.502619e-03;0.998247;1.615382e-03\n",
    "epoch:235;\t train:1.745652e-03;0.998198;1.741136e-03;\t test:1.516985e-03;0.998241;1.638728e-03\n",
    "epoch:236;\t train:1.747731e-03;0.998192;1.744451e-03;\t test:1.511205e-03;0.998265;1.636904e-03\n",
    "epoch:237;\t train:1.752947e-03;0.998186;1.752889e-03;\t test:1.507697e-03;0.998260;1.626231e-03\n",
    "epoch:238;\t train:1.750315e-03;0.998188;1.751641e-03;\t test:1.515517e-03;0.998251;1.645671e-03\n",
    "epoch:239;\t train:1.758188e-03;0.998176;1.762006e-03;\t test:1.499984e-03;0.998279;1.620583e-03\n",
    "epoch:240;\t train:1.741097e-03;0.998199;1.736835e-03;\t test:1.501151e-03;0.998276;1.620477e-03\n",
    "epoch:241;\t train:1.755820e-03;0.998183;1.754742e-03;\t test:1.512338e-03;0.998260;1.642116e-03\n",
    "epoch:242;\t train:1.748151e-03;0.998187;1.749270e-03;\t test:1.494697e-03;0.998280;1.612001e-03\n",
    "epoch:243;\t train:1.748464e-03;0.998187;1.747916e-03;\t test:1.515316e-03;0.998265;1.641833e-03\n",
    "epoch:244;\t train:1.736677e-03;0.998195;1.739648e-03;\t test:1.489246e-03;0.998294;1.594339e-03\n",
    "epoch:245;\t train:1.739324e-03;0.998194;1.743384e-03;\t test:1.511279e-03;0.998268;1.633787e-03\n",
    "epoch:246;\t train:1.744560e-03;0.998196;1.744363e-03;\t test:1.506844e-03;0.998265;1.632869e-03\n",
    "epoch:247;\t train:1.727671e-03;0.998205;1.732327e-03;\t test:1.516826e-03;0.998256;1.646511e-03\n",
    "epoch:248;\t train:1.748222e-03;0.998187;1.751451e-03;\t test:1.518737e-03;0.998252;1.641955e-03\n",
    "epoch:249;\t train:1.730107e-03;0.998199;1.735139e-03;\t test:1.493198e-03;0.998288;1.617430e-03\n",
    "epoch:250;\t train:1.734853e-03;0.998197;1.740667e-03;\t test:1.494601e-03;0.998257;1.611479e-03\n",
    "epoch:251;\t train:1.740504e-03;0.998197;1.740127e-03;\t test:1.513000e-03;0.998265;1.633415e-03\n",
    "epoch:252;\t train:1.725787e-03;0.998203;1.730456e-03;\t test:1.510131e-03;0.998270;1.636260e-03\n",
    "epoch:253;\t train:1.737424e-03;0.998192;1.743171e-03;\t test:1.517713e-03;0.998242;1.642425e-03\n",
    "epoch:254;\t train:1.737813e-03;0.998198;1.741971e-03;\t test:1.497292e-03;0.998264;1.626532e-03\n",
    "epoch:255;\t train:1.721901e-03;0.998209;1.727228e-03;\t test:1.486742e-03;0.998275;1.606073e-03\n",
    "epoch:256;\t train:1.735315e-03;0.998194;1.742418e-03;\t test:1.478950e-03;0.998289;1.596982e-03\n",
    "epoch:257;\t train:1.735663e-03;0.998190;1.744903e-03;\t test:1.508304e-03;0.998256;1.635225e-03\n",
    "epoch:258;\t train:1.727748e-03;0.998202;1.737778e-03;\t test:1.512544e-03;0.998265;1.642215e-03\n",
    "epoch:259;\t train:1.735615e-03;0.998195;1.743521e-03;\t test:1.479673e-03;0.998272;1.609953e-03\n",
    "epoch:260;\t train:1.728460e-03;0.998198;1.737001e-03;\t test:1.501165e-03;0.998271;1.635690e-03\n",
    "epoch:261;\t train:1.724097e-03;0.998203;1.732617e-03;\t test:1.500162e-03;0.998274;1.625471e-03\n",
    "epoch:262;\t train:1.727098e-03;0.998203;1.734544e-03;\t test:1.486292e-03;0.998277;1.614052e-03\n",
    "epoch:263;\t train:1.732422e-03;0.998191;1.743687e-03;\t test:1.486124e-03;0.998275;1.614217e-03\n",
    "epoch:264;\t train:1.734065e-03;0.998193;1.740004e-03;\t test:1.468039e-03;0.998295;1.597907e-03\n",
    "epoch:265;\t train:1.721202e-03;0.998208;1.730076e-03;\t test:1.496593e-03;0.998261;1.628217e-03\n",
    "epoch:266;\t train:1.732135e-03;0.998187;1.745647e-03;\t test:1.504721e-03;0.998276;1.633013e-03\n",
    "epoch:267;\t train:1.737181e-03;0.998191;1.743491e-03;\t test:1.502030e-03;0.998274;1.631648e-03\n",
    "epoch:268;\t train:1.723108e-03;0.998199;1.733735e-03;\t test:1.493242e-03;0.998268;1.627118e-03\n",
    "epoch:269;\t train:1.712658e-03;0.998209;1.721611e-03;\t test:1.492462e-03;0.998260;1.629615e-03\n",
    "epoch:270;\t train:1.712663e-03;0.998215;1.722629e-03;\t test:1.473566e-03;0.998275;1.600146e-03\n",
    "epoch:271;\t train:1.723317e-03;0.998200;1.734313e-03;\t test:1.484998e-03;0.998277;1.625569e-03\n",
    "epoch:272;\t train:1.726096e-03;0.998203;1.736678e-03;\t test:1.490627e-03;0.998251;1.614519e-03\n",
    "epoch:273;\t train:1.715039e-03;0.998204;1.729898e-03;\t test:1.493233e-03;0.998272;1.626302e-03\n",
    "epoch:274;\t train:1.719467e-03;0.998197;1.738528e-03;\t test:1.495575e-03;0.998249;1.623592e-03\n",
    "epoch:275;\t train:1.718860e-03;0.998205;1.728558e-03;\t test:1.464888e-03;0.998305;1.593555e-03\n",
    "epoch:276;\t train:1.708713e-03;0.998220;1.719586e-03;\t test:1.488383e-03;0.998284;1.619673e-03\n",
    "epoch:277;\t train:1.720038e-03;0.998193;1.738609e-03;\t test:1.478964e-03;0.998290;1.603411e-03\n",
    "epoch:278;\t train:1.705383e-03;0.998214;1.715765e-03;\t test:1.502552e-03;0.998263;1.634052e-03\n",
    "epoch:279;\t train:1.713317e-03;0.998204;1.728372e-03;\t test:1.476420e-03;0.998265;1.598345e-03\n",
    "epoch:280;\t train:1.720861e-03;0.998200;1.734183e-03;\t test:1.475476e-03;0.998280;1.605159e-03\n",
    "epoch:281;\t train:1.716353e-03;0.998204;1.727918e-03;\t test:1.499335e-03;0.998263;1.630077e-03\n",
    "epoch:282;\t train:1.708103e-03;0.998208;1.725635e-03;\t test:1.463342e-03;0.998301;1.594468e-03\n",
    "epoch:283;\t train:1.716875e-03;0.998201;1.728233e-03;\t test:1.474717e-03;0.998303;1.600247e-03\n",
    "epoch:284;\t train:1.707965e-03;0.998202;1.725201e-03;\t test:1.479114e-03;0.998273;1.612473e-03\n",
    "epoch:285;\t train:1.700165e-03;0.998217;1.716853e-03;\t test:1.493742e-03;0.998257;1.630073e-03\n",
    "epoch:286;\t train:1.711047e-03;0.998213;1.721274e-03;\t test:1.493769e-03;0.998261;1.629992e-03\n",
    "epoch:287;\t train:1.721614e-03;0.998196;1.740886e-03;\t test:1.492526e-03;0.998262;1.622047e-03\n",
    "epoch:288;\t train:1.694250e-03;0.998224;1.709222e-03;\t test:1.482465e-03;0.998276;1.622500e-03\n",
    "epoch:289;\t train:1.714463e-03;0.998200;1.733325e-03;\t test:1.481889e-03;0.998246;1.611339e-03\n",
    "epoch:290;\t train:1.710268e-03;0.998205;1.728722e-03;\t test:1.476468e-03;0.998280;1.613813e-03\n",
    "epoch:291;\t train:1.709093e-03;0.998209;1.725786e-03;\t test:1.496927e-03;0.998281;1.627892e-03\n",
    "epoch:292;\t train:1.708086e-03;0.998203;1.724853e-03;\t test:1.471951e-03;0.998287;1.605950e-03\n",
    "epoch:293;\t train:1.702664e-03;0.998213;1.719984e-03;\t test:1.481399e-03;0.998285;1.620282e-03\n",
    "epoch:294;\t train:1.710846e-03;0.998203;1.727016e-03;\t test:1.479177e-03;0.998271;1.613800e-03\n",
    "epoch:295;\t train:1.698220e-03;0.998218;1.712226e-03;\t test:1.470495e-03;0.998282;1.603821e-03\n",
    "epoch:296;\t train:1.693369e-03;0.998226;1.709776e-03;\t test:1.470896e-03;0.998277;1.609143e-03\n",
    "epoch:297;\t train:1.704308e-03;0.998208;1.723817e-03;\t test:1.481272e-03;0.998281;1.616074e-03\n",
    "epoch:298;\t train:1.696581e-03;0.998211;1.715954e-03;\t test:1.476016e-03;0.998272;1.620732e-03\n",
    "epoch:299;\t train:1.700200e-03;0.998217;1.717664e-03;\t test:1.468535e-03;0.998310;1.599350e-03\n",
    "epoch:300;\t train:1.688691e-03;0.998220;1.707975e-03;\t test:1.466914e-03;0.998294;1.609419e-03\n",
    "epoch:301;\t train:1.697746e-03;0.998216;1.714966e-03;\t test:1.490144e-03;0.998267;1.629452e-03\n",
    "epoch:302;\t train:1.706851e-03;0.998205;1.724987e-03;\t test:1.456948e-03;0.998323;1.590456e-03\n",
    "epoch:303;\t train:1.695905e-03;0.998217;1.714231e-03;\t test:1.464514e-03;0.998312;1.591037e-03\n",
    "epoch:304;\t train:1.704366e-03;0.998206;1.723073e-03;\t test:1.467042e-03;0.998274;1.607543e-03\n",
    "epoch:305;\t train:1.707695e-03;0.998202;1.728926e-03;\t test:1.475726e-03;0.998286;1.615330e-03\n",
    "epoch:306;\t train:1.697208e-03;0.998215;1.717072e-03;\t test:1.449574e-03;0.998314;1.583419e-03\n",
    "epoch:307;\t train:1.703937e-03;0.998209;1.720562e-03;\t test:1.451568e-03;0.998308;1.589529e-03\n",
    "epoch:308;\t train:1.687787e-03;0.998214;1.712258e-03;\t test:1.477177e-03;0.998271;1.613430e-03\n",
    "epoch:309;\t train:1.698141e-03;0.998207;1.719945e-03;\t test:1.449940e-03;0.998273;1.580673e-03\n",
    "epoch:310;\t train:1.688955e-03;0.998219;1.709603e-03;\t test:1.473573e-03;0.998295;1.613446e-03\n",
    "epoch:311;\t train:1.687321e-03;0.998217;1.710732e-03;\t test:1.477962e-03;0.998268;1.619766e-03\n",
    "epoch:312;\t train:1.696849e-03;0.998210;1.716833e-03;\t test:1.463550e-03;0.998296;1.609615e-03\n",
    "epoch:313;\t train:1.700370e-03;0.998211;1.721545e-03;\t test:1.462109e-03;0.998295;1.591714e-03\n",
    "epoch:314;\t train:1.695737e-03;0.998211;1.717277e-03;\t test:1.468115e-03;0.998283;1.607053e-03\n",
    "epoch:315;\t train:1.693677e-03;0.998214;1.714301e-03;\t test:1.452522e-03;0.998308;1.581940e-03\n",
    "epoch:316;\t train:1.692536e-03;0.998214;1.714729e-03;\t test:1.476848e-03;0.998272;1.614955e-03\n",
    "epoch:317;\t train:1.695764e-03;0.998207;1.720378e-03;\t test:1.464917e-03;0.998285;1.606639e-03\n",
    "epoch:318;\t train:1.691780e-03;0.998216;1.711456e-03;\t test:1.466303e-03;0.998286;1.610021e-03\n",
    "epoch:319;\t train:1.694001e-03;0.998204;1.721679e-03;\t test:1.451916e-03;0.998288;1.583345e-03\n",
    "epoch:320;\t train:1.692636e-03;0.998216;1.713844e-03;\t test:1.469993e-03;0.998261;1.617050e-03\n",
    "epoch:321;\t train:1.696718e-03;0.998207;1.721394e-03;\t test:1.444189e-03;0.998291;1.574271e-03\n",
    "epoch:322;\t train:1.696515e-03;0.998211;1.718971e-03;\t test:1.475555e-03;0.998287;1.616216e-03\n",
    "epoch:323;\t train:1.696543e-03;0.998209;1.716088e-03;\t test:1.432193e-03;0.998302;1.563030e-03\n",
    "epoch:324;\t train:1.688733e-03;0.998208;1.715734e-03;\t test:1.458030e-03;0.998300;1.602408e-03\n",
    "epoch:325;\t train:1.683513e-03;0.998217;1.709446e-03;\t test:1.462941e-03;0.998286;1.603038e-03\n",
    "epoch:326;\t train:1.692647e-03;0.998210;1.717163e-03;\t test:1.494189e-03;0.998256;1.638100e-03\n",
    "epoch:327;\t train:1.697801e-03;0.998203;1.722118e-03;\t test:1.469854e-03;0.998285;1.603140e-03\n",
    "epoch:328;\t train:1.686171e-03;0.998219;1.709097e-03;\t test:1.450530e-03;0.998309;1.586677e-03\n",
    "epoch:329;\t train:1.698100e-03;0.998203;1.722908e-03;\t test:1.436033e-03;0.998307;1.575146e-03\n",
    "epoch:330;\t train:1.693503e-03;0.998211;1.714727e-03;\t test:1.464249e-03;0.998282;1.602250e-03\n",
    "epoch:331;\t train:1.692069e-03;0.998207;1.716616e-03;\t test:1.453070e-03;0.998284;1.596257e-03\n",
    "epoch:332;\t train:1.684249e-03;0.998219;1.704309e-03;\t test:1.469315e-03;0.998256;1.609721e-03\n",
    "epoch:333;\t train:1.676337e-03;0.998220;1.701844e-03;\t test:1.455768e-03;0.998307;1.601079e-03\n",
    "epoch:334;\t train:1.692025e-03;0.998204;1.718688e-03;\t test:1.451843e-03;0.998321;1.584840e-03\n",
    "epoch:335;\t train:1.680130e-03;0.998225;1.696307e-03;\t test:1.457884e-03;0.998295;1.599819e-03\n",
    "epoch:336;\t train:1.700423e-03;0.998199;1.729404e-03;\t test:1.469465e-03;0.998290;1.613481e-03\n",
    "epoch:337;\t train:1.698432e-03;0.998202;1.723320e-03;\t test:1.475346e-03;0.998280;1.612394e-03\n",
    "epoch:338;\t train:1.693624e-03;0.998207;1.717820e-03;\t test:1.468785e-03;0.998286;1.608553e-03\n",
    "epoch:339;\t train:1.685530e-03;0.998211;1.711475e-03;\t test:1.488974e-03;0.998239;1.640106e-03\n",
    "epoch:340;\t train:1.686630e-03;0.998219;1.704949e-03;\t test:1.459070e-03;0.998288;1.592901e-03\n",
    "epoch:341;\t train:1.691229e-03;0.998206;1.715123e-03;\t test:1.475629e-03;0.998286;1.617119e-03\n",
    "epoch:342;\t train:1.672970e-03;0.998227;1.695362e-03;\t test:1.445647e-03;0.998302;1.586430e-03\n",
    "epoch:343;\t train:1.690053e-03;0.998213;1.710549e-03;\t test:1.475845e-03;0.998276;1.611638e-03\n",
    "epoch:344;\t train:1.687097e-03;0.998207;1.713212e-03;\t test:1.453264e-03;0.998281;1.582451e-03\n",
    "epoch:345;\t train:1.673643e-03;0.998216;1.700988e-03;\t test:1.457038e-03;0.998296;1.591221e-03\n",
    "epoch:346;\t train:1.688564e-03;0.998207;1.709171e-03;\t test:1.456258e-03;0.998292;1.588611e-03\n",
    "epoch:347;\t train:1.684670e-03;0.998209;1.713422e-03;\t test:1.470463e-03;0.998261;1.609584e-03\n",
    "epoch:348;\t train:1.680706e-03;0.998216;1.702627e-03;\t test:1.471646e-03;0.998283;1.614284e-03\n",
    "epoch:349;\t train:1.692918e-03;0.998204;1.717960e-03;\t test:1.458994e-03;0.998292;1.599588e-03\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = test_dataset[0]\n",
    "test_image2, test_label2 = train_transform(test_image, test_label)\n",
    "test_image2 = nd.expand_dims(test_image2,0)\n",
    "print('tensor shape:', test_image2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, cls_preds, box_preds = net(test_image2.as_in_context(ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predictions to real object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxDetection\n",
    "cls_probs = nd.SoftmaxActivation(nd.transpose(cls_preds, (0, 2, 1)), mode='channel')\n",
    "output = MultiBoxDetection(cls_prob=cls_probs, loc_pred=box_preds, anchor=anchors, force_suppress=True, clip=True, nms_topk=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('cluster')\n",
    "def display(img, out, thresh=0.5):\n",
    "    import random\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    mpl.rcParams['figure.figsize'] = (10,10)\n",
    "    img = img.asnumpy()\n",
    "    img = np.transpose(img,(2,3,1,0))\n",
    "    img = np.squeeze(img)\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    for det in out:\n",
    "        cid = int(det[0])\n",
    "        if cid == 0:\n",
    "            continue\n",
    "        score = det[1]\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        scales = [img.shape[1], img.shape[0]] * 2\n",
    "        xmin, ymin, xmax, ymax = [int(p * s) for p, s in zip(det[2:6].tolist(), scales)]\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False,\n",
    "                             edgecolor='red', linewidth=3)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[cid]\n",
    "        plt.gca().text(xmin, ymin-2, '{:s} {:.3f}'.format(text, score),\n",
    "                       bbox=dict(facecolor='red', alpha=0.5),\n",
    "                       fontsize=12, color='white')\n",
    "\n",
    "display(test_image2, output[0].asnumpy(), thresh=0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
