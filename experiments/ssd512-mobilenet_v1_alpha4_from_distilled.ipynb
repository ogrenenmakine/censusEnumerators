{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.contrib.ndarray import MultiBoxPrior\n",
    "from mxnet.gluon.contrib import nn as nn_contrib\n",
    "from mxnet.gluon import nn\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict classes\n",
    "- channel `i*(num_class+1)` store the scores for this box contains only background\n",
    "- channel `i*(num_class+1)+1+j` store the scores for this box contains an object from the *j*-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predictor(num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict anchor boxes\n",
    "- $t_x = (Y_x - b_x) / b_{width}$\n",
    "- $t_y = (Y_y - b_y) / b_{height}$\n",
    "- $t_{width} = (Y_{width} - b_{width}) / b_{width}$\n",
    "- $t_{height} = (Y_{height} - b_{height}) / b_{height}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_predictor(num_anchors):\n",
    "    return nn.Conv2D(num_anchors * 4, 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage preditions from multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    return nd.flatten(nd.transpose(pred, axes=(0, 2, 3, 1)))\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    return nd.concat(*preds, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_layer(nfilters, stride, expension_constant):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(nn.Conv2D(nfilters, 3, strides=stride, padding=1, groups=nfilters, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(nfilters*expension_constant, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global alpha\n",
    "alpha = 0.25\n",
    "num_filters = int(32*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "def s16():\n",
    "    out = nn.HybridSequential()\n",
    "    # conv_0 layer\n",
    "    out.add(nn.Conv2D(num_filters, 3, strides=2, padding=1, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_1 layer\n",
    "    out.add(dp_layer(num_filters, 1, 2))\n",
    "    # conv_2 layer\n",
    "    out.add(dp_layer(num_filters*2, 2, 2))\n",
    "    # conv_3 layer\n",
    "    out.add(dp_layer(num_filters*4, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*4, 3, strides=2, padding=1, groups=num_filters*4, use_bias=False))\n",
    "    out.load_parameters(\"weights/mobilenet_0_25_s16_dist.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def s32():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*8, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_4_layer\n",
    "    out.add(dp_layer(num_filters*8, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*8, 3, strides=2, padding=1, groups=num_filters*8, use_bias=False))\n",
    "    out.load_parameters(\"weights/mobilenet_0_25_s32_dist.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b1():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_6_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b2():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_7_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b3():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_8_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b4():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_9_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_model(num_anchors, num_classes):\n",
    "    class_preds = nn.Sequential()\n",
    "    box_preds = nn.Sequential()\n",
    "    \n",
    "    for scale in range(6):\n",
    "        class_preds.add(class_predictor(num_anchors, num_classes))\n",
    "        box_preds.add(box_predictor(num_anchors))\n",
    "    \n",
    "    class_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    box_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    return s16(), s32(), b1(), b2(), b3(), b4(), class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_forward(x, s16, s32, b1, b2, b3, b4, class_preds, box_preds, sizes, ratios):\n",
    "    default_anchors = []\n",
    "    predicted_boxes = []  \n",
    "    predicted_classes = []\n",
    "\n",
    "    x = s16(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[0], ratios=ratios[0]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[0](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[0](x)))\n",
    "    \n",
    "    x = s32(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[1], ratios=ratios[1]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[1](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[1](x)))\n",
    "    \n",
    "    x = b1(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[2], ratios=ratios[2]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[2](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[2](x)))\n",
    "    \n",
    "    x = b2(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[3], ratios=ratios[3]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[3](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[3](x)))\n",
    "    \n",
    "    x = b3(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[4], ratios=ratios[4]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[4](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[4](x)))\n",
    "    \n",
    "    x = b4(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[5], ratios=ratios[5]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[5](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[5](x)))\n",
    "\n",
    "    return default_anchors, predicted_classes, predicted_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "class SSD(gluon.Block):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(SSD, self).__init__(**kwargs)\n",
    "        self.anchor_sizes = [[0.04, 0.1],[0.1,0.26],[0.26,0.42],[0.42,0.58],[0.58,0.74],[0.74,0.9],[0.9,1.06]]\n",
    "        self.anchor_ratios = [[1, 2, .5]] * 6\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.s16, self.s32, self.b1, self.b2, self.b3, self.b4, self.class_preds, self.box_preds = ssd_model(4, num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        default_anchors, predicted_classes, predicted_boxes = ssd_forward(x, self.s16, self.s32, self.b1, self.b2, self.b3, self.b4,\n",
    "            self.class_preds, self.box_preds, self.anchor_sizes, self.anchor_ratios)\n",
    "        anchors = concat_predictions(default_anchors)\n",
    "        box_preds = concat_predictions(predicted_boxes)\n",
    "        class_preds = concat_predictions(predicted_classes)\n",
    "        class_preds = nd.reshape(class_preds, shape=(0, -1, self.num_classes + 1))\n",
    "        \n",
    "        return anchors, class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SSD(2)\n",
    "#net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "net.load_parameters(\"process/ssd_300.params\",ctx=ctx)\n",
    "x = nd.zeros((1, 3, 512, 512),ctx=ctx)\n",
    "default_anchors, class_predictions, box_predictions = net(x)\n",
    "print('Outputs:', 'anchors', default_anchors.shape, 'class prediction', class_predictions.shape, 'box prediction', box_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.NACDDetection import NACDDetection\n",
    "\n",
    "train_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'train')])\n",
    "test_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'test')])\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Test images:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source import NACDTransform\n",
    "width, height = 512, 512\n",
    "train_transform = NACDTransform.NACDDefaultTransform(width, height, False)\n",
    "test_transform = NACDTransform.NACDDefaultTransform(width, height, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd\n",
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = test_dataset[0]\n",
    "bboxes = train_label[:, :4]\n",
    "cids = train_label[:, 4:5]\n",
    "print('image:', train_image.shape)\n",
    "print('bboxes:', bboxes.shape, 'class ids:', cids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2, train_label2 = train_transform(train_image, train_label)\n",
    "print('tensor shape:', train_image2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset.transform(test_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(test_loader):\n",
    "    if ib > 3:\n",
    "        break\n",
    "    print('data:', batch[0].shape, 'label:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array((0.485, 0.456, 0.406))\n",
    "train_image2 = (train_image2 * 255).clip(0, 255)\n",
    "ax = viz.plot_bbox(train_image2.asnumpy(), train_label2[:, :4],\n",
    "                   labels=train_label2[:, 4:5],\n",
    "                   class_names=train_dataset.classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxTarget\n",
    "def training_targets(default_anchors, class_predicts, labels):\n",
    "    class_predicts = nd.transpose(class_predicts, axes=(0, 2, 1))\n",
    "    z = MultiBoxTarget(anchor=default_anchors.as_in_context(mx.cpu()), label=labels.as_in_context(mx.cpu()), cls_pred=class_predicts.as_in_context(mx.cpu()))\n",
    "    box_target = z[0].as_in_context(ctx)  # box offset target for (x, y, width, height)\n",
    "    box_mask = z[1].as_in_context(ctx)  # mask is used to ignore box offsets we don't want to penalize, e.g. negative samples\n",
    "    cls_target = z[2].as_in_context(ctx)  # cls_target is an array of labels for all anchors boxes\n",
    "    return box_target, box_mask, cls_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertlbl(y):\n",
    "    mtrx = y[:,:,0:4]\n",
    "    mtrx = mtrx.asnumpy()\n",
    "    mtrx[mtrx == -1] = -width\n",
    "    mtrx = mtrx/512\n",
    "    return mx.nd.concat(nd.expand_dims(y[:,:,4],2),mx.nd.array(mtrx),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(gluon.loss.Loss):\n",
    "    def __init__(self, axis=-1, alpha=0.25, gamma=2, batch_axis=0, **kwargs):\n",
    "        super(FocalLoss, self).__init__(None, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label):\n",
    "        output = F.softmax(output)\n",
    "        pt = F.pick(output, label, axis=self._axis, keepdims=True)\n",
    "        loss = -self._alpha * ((1 - pt) ** self._gamma) * F.log(pt)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "# cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "cls_loss = FocalLoss()\n",
    "print(cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(gluon.loss.Loss):\n",
    "    def __init__(self, batch_axis=0, **kwargs):\n",
    "        super(SmoothL1Loss, self).__init__(None, batch_axis, **kwargs)\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label, mask):\n",
    "        loss = F.smooth_l1((output - label) * mask, scalar=1.0)\n",
    "        return F.mean(loss, self._batch_axis, exclude=True)\n",
    "\n",
    "box_loss = SmoothL1Loss()\n",
    "print(box_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mxnet import autograd as ag\n",
    "from gluoncv.loss import SSDMultiBoxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop params\n",
    "epochs = 100\n",
    "start_epoch = 1\n",
    "\n",
    "# initialize trainer\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 1e-1, 'wd': 4e-5})\n",
    "\n",
    "# evaluation metrics\n",
    "cls_metric = mx.metric.Accuracy()\n",
    "box_metric = mx.metric.MAE()\n",
    "cls_metric_test = mx.metric.Accuracy()\n",
    "box_metric_test = mx.metric.MAE()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # reset iterator and tick\n",
    "    #train_data.reset()\n",
    "    cls_metric.reset()\n",
    "    box_metric.reset()\n",
    "    tic = time.time()\n",
    "    train_loss = 0\n",
    "    # iterate through all batch\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # record gradients\n",
    "        with ag.record():\n",
    "            x = batch[0].as_in_context(ctx)\n",
    "            y = batch[1].as_in_context(ctx)\n",
    "            lbl = convertlbl(batch[1])\n",
    "            default_anchors, class_predictions, box_predictions = net(x)\n",
    "            box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "            # losses\n",
    "            loss1 = cls_loss(class_predictions, cls_target)\n",
    "            loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "            # sum all losses\n",
    "            loss = loss1 + loss2\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "        # apply \n",
    "        trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        # update metrics\n",
    "        cls_metric.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_train, val1_train = cls_metric.get()\n",
    "    name2_train, val2_train = box_metric.get()\n",
    "\n",
    "    cls_metric_test.reset()\n",
    "    box_metric_test.reset()\n",
    "    tic = time.time()\n",
    "    test_loss = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        # record gradients\n",
    "        x = batch[0].as_in_context(ctx)\n",
    "        y = batch[1].as_in_context(ctx)\n",
    "        lbl = convertlbl(batch[1])\n",
    "        default_anchors, class_predictions, box_predictions = net(x)\n",
    "        box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "        # losses\n",
    "        loss1 = cls_loss(class_predictions, cls_target)\n",
    "        loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "        # sum all losses\n",
    "        loss = loss1 + loss2\n",
    "        test_loss += nd.sum(loss).asscalar()\n",
    "        # update metrics\n",
    "        cls_metric_test.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric_test.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_test, val1_test = cls_metric_test.get()\n",
    "    name2_test, val2_test = box_metric_test.get()\n",
    "    print('epoch:%3d;\\t train:%.6e;%f;%.6e;\\t test:%.6e;%f;%.6e'\n",
    "          %(epoch, train_loss/len(train_dataset), val1_train, val2_train, test_loss/len(test_dataset), val1_test, val2_test))\n",
    "\n",
    "    net.save_parameters('process/ssd_%d.params' % epoch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# detached\n",
    "epoch:  1;\t train:1.142976e-02;0.982709;2.026818e-03;\t test:5.950928e-03;0.994906;1.982350e-03\n",
    "epoch:  2;\t train:5.194239e-03;0.996069;2.009641e-03;\t test:4.234332e-03;0.996789;1.951143e-03\n",
    "epoch:  3;\t train:4.158913e-03;0.996979;2.006346e-03;\t test:3.657099e-03;0.997238;1.947551e-03\n",
    "epoch:  4;\t train:3.677135e-03;0.997343;1.992840e-03;\t test:3.354415e-03;0.997519;1.946653e-03\n",
    "epoch:  5;\t train:3.401528e-03;0.997572;1.996442e-03;\t test:3.117369e-03;0.997727;1.929599e-03\n",
    "epoch:  6;\t train:3.208723e-03;0.997733;1.996436e-03;\t test:2.934627e-03;0.997840;1.941947e-03\n",
    "epoch:  7;\t train:3.043061e-03;0.997848;1.982956e-03;\t test:2.810119e-03;0.997902;1.946837e-03\n",
    "epoch:  8;\t train:2.934248e-03;0.997902;1.986833e-03;\t test:2.702097e-03;0.997948;1.948930e-03\n",
    "epoch:  9;\t train:2.830745e-03;0.997944;1.982301e-03;\t test:2.612207e-03;0.998003;1.936180e-03\n",
    "epoch: 10;\t train:2.762063e-03;0.997970;1.982813e-03;\t test:2.515140e-03;0.998042;1.909976e-03\n",
    "epoch: 11;\t train:2.703538e-03;0.997994;1.982351e-03;\t test:2.459528e-03;0.998043;1.915174e-03\n",
    "epoch: 12;\t train:2.639759e-03;0.998017;1.974574e-03;\t test:2.436586e-03;0.998037;1.936900e-03\n",
    "epoch: 13;\t train:2.602960e-03;0.998031;1.978068e-03;\t test:2.354643e-03;0.998098;1.884615e-03\n",
    "epoch: 14;\t train:2.561904e-03;0.998052;1.968779e-03;\t test:2.341381e-03;0.998092;1.909436e-03\n",
    "epoch: 15;\t train:2.534079e-03;0.998057;1.975983e-03;\t test:2.291877e-03;0.998124;1.888833e-03\n",
    "epoch: 16;\t train:2.511962e-03;0.998066;1.976526e-03;\t test:2.308066e-03;0.998085;1.939067e-03\n",
    "epoch: 17;\t train:2.472376e-03;0.998087;1.965343e-03;\t test:2.222756e-03;0.998167;1.863912e-03\n",
    "epoch: 18;\t train:2.446823e-03;0.998099;1.961130e-03;\t test:2.255968e-03;0.998114;1.925121e-03\n",
    "epoch: 19;\t train:2.441774e-03;0.998097;1.973160e-03;\t test:2.205283e-03;0.998153;1.892387e-03\n",
    "epoch: 20;\t train:2.409007e-03;0.998111;1.963464e-03;\t test:2.190169e-03;0.998156;1.897249e-03\n",
    "epoch: 21;\t train:2.388927e-03;0.998122;1.960642e-03;\t test:2.196097e-03;0.998152;1.914378e-03\n",
    "epoch: 22;\t train:2.383896e-03;0.998123;1.964754e-03;\t test:2.152177e-03;0.998178;1.888872e-03\n",
    "epoch: 23;\t train:2.350935e-03;0.998136;1.954015e-03;\t test:2.194683e-03;0.998135;1.946209e-03\n",
    "epoch: 24;\t train:2.346067e-03;0.998131;1.962783e-03;\t test:2.134656e-03;0.998188;1.898344e-03\n",
    "epoch: 25;\t train:2.323595e-03;0.998135;1.961664e-03;\t test:2.117964e-03;0.998200;1.878940e-03\n",
    "epoch: 26;\t train:2.311218e-03;0.998142;1.957463e-03;\t test:2.139356e-03;0.998164;1.924112e-03\n",
    "epoch: 27;\t train:2.299307e-03;0.998146;1.951310e-03;\t test:2.107378e-03;0.998181;1.900978e-03\n",
    "epoch: 28;\t train:2.291833e-03;0.998144;1.961066e-03;\t test:2.061676e-03;0.998218;1.863837e-03\n",
    "epoch: 29;\t train:2.292896e-03;0.998135;1.970330e-03;\t test:2.082349e-03;0.998193;1.895989e-03\n",
    "epoch: 30;\t train:2.280118e-03;0.998142;1.961631e-03;\t test:2.078096e-03;0.998178;1.903263e-03\n",
    "epoch: 31;\t train:2.260903e-03;0.998153;1.953065e-03;\t test:2.095484e-03;0.998155;1.931414e-03\n",
    "epoch: 32;\t train:2.245266e-03;0.998158;1.945520e-03;\t test:2.075902e-03;0.998171;1.919849e-03\n",
    "epoch: 33;\t train:2.241738e-03;0.998156;1.950018e-03;\t test:2.025873e-03;0.998217;1.870370e-03\n",
    "epoch: 34;\t train:2.236749e-03;0.998151;1.953592e-03;\t test:2.039586e-03;0.998205;1.889697e-03\n",
    "epoch: 35;\t train:2.229998e-03;0.998150;1.952954e-03;\t test:2.038359e-03;0.998191;1.898776e-03\n",
    "epoch: 36;\t train:2.207965e-03;0.998168;1.935480e-03;\t test:2.021230e-03;0.998181;1.890462e-03\n",
    "epoch: 37;\t train:2.212996e-03;0.998156;1.948612e-03;\t test:1.988168e-03;0.998223;1.857528e-03\n",
    "epoch: 38;\t train:2.200792e-03;0.998167;1.935721e-03;\t test:1.979053e-03;0.998227;1.852233e-03\n",
    "epoch: 39;\t train:2.204709e-03;0.998154;1.949604e-03;\t test:1.961526e-03;0.998240;1.840145e-03\n",
    "epoch: 40;\t train:2.201101e-03;0.998155;1.949461e-03;\t test:1.993912e-03;0.998193;1.878827e-03\n",
    "epoch: 41;\t train:2.189703e-03;0.998157;1.942740e-03;\t test:1.989849e-03;0.998205;1.878617e-03\n",
    "epoch: 42;\t train:2.179876e-03;0.998158;1.939266e-03;\t test:1.997957e-03;0.998173;1.895185e-03\n",
    "epoch: 43;\t train:2.182080e-03;0.998161;1.938708e-03;\t test:1.984803e-03;0.998196;1.882469e-03\n",
    "epoch: 44;\t train:2.184544e-03;0.998147;1.949377e-03;\t test:1.989881e-03;0.998181;1.893322e-03\n",
    "epoch: 45;\t train:2.185554e-03;0.998136;1.956369e-03;\t test:1.960152e-03;0.998200;1.862091e-03\n",
    "epoch: 46;\t train:2.164659e-03;0.998157;1.936824e-03;\t test:1.966413e-03;0.998174;1.885044e-03\n",
    "epoch: 47;\t train:2.153890e-03;0.998157;1.933145e-03;\t test:1.974376e-03;0.998161;1.894998e-03\n",
    "epoch: 48;\t train:2.150771e-03;0.998156;1.933808e-03;\t test:1.935181e-03;0.998191;1.860862e-03\n",
    "epoch: 49;\t train:2.151517e-03;0.998146;1.941098e-03;\t test:1.932593e-03;0.998188;1.860501e-03\n",
    "epoch: 50;\t train:2.139217e-03;0.998146;1.936553e-03;\t test:1.944881e-03;0.998170;1.879081e-03\n",
    "epoch: 51;\t train:2.135019e-03;0.998147;1.932027e-03;\t test:1.923505e-03;0.998177;1.860166e-03\n",
    "epoch: 52;\t train:2.126196e-03;0.998150;1.928114e-03;\t test:1.914599e-03;0.998184;1.862451e-03\n",
    "epoch: 53;\t train:2.120271e-03;0.998151;1.927391e-03;\t test:1.909548e-03;0.998175;1.858101e-03\n",
    "epoch: 54;\t train:2.116308e-03;0.998154;1.924875e-03;\t test:1.922235e-03;0.998154;1.878517e-03\n",
    "epoch: 55;\t train:2.117089e-03;0.998140;1.935076e-03;\t test:1.909469e-03;0.998174;1.861899e-03\n",
    "epoch: 56;\t train:2.111890e-03;0.998147;1.925975e-03;\t test:1.872411e-03;0.998188;1.833489e-03\n",
    "epoch: 57;\t train:2.089831e-03;0.998159;1.913233e-03;\t test:1.881727e-03;0.998178;1.851595e-03\n",
    "epoch: 58;\t train:2.100960e-03;0.998139;1.930461e-03;\t test:1.895495e-03;0.998137;1.860015e-03\n",
    "epoch: 59;\t train:2.090515e-03;0.998156;1.915678e-03;\t test:1.872900e-03;0.998178;1.848193e-03\n",
    "epoch: 60;\t train:2.089127e-03;0.998145;1.925394e-03;\t test:1.853789e-03;0.998173;1.833551e-03\n",
    "epoch: 61;\t train:2.083314e-03;0.998145;1.920246e-03;\t test:1.895247e-03;0.998151;1.876001e-03\n",
    "epoch: 62;\t train:2.079127e-03;0.998146;1.917338e-03;\t test:1.861901e-03;0.998168;1.846308e-03\n",
    "epoch: 63;\t train:2.076532e-03;0.998141;1.919341e-03;\t test:1.826680e-03;0.998202;1.811602e-03\n",
    "epoch: 64;\t train:2.057156e-03;0.998160;1.900993e-03;\t test:1.880602e-03;0.998147;1.871994e-03\n",
    "epoch: 65;\t train:2.059280e-03;0.998146;1.911460e-03;\t test:1.861245e-03;0.998140;1.861563e-03\n",
    "epoch: 66;\t train:2.052574e-03;0.998149;1.907208e-03;\t test:1.856796e-03;0.998171;1.857425e-03\n",
    "epoch: 67;\t train:2.054312e-03;0.998149;1.908779e-03;\t test:1.830579e-03;0.998162;1.831204e-03\n",
    "epoch: 68;\t train:2.052651e-03;0.998148;1.911483e-03;\t test:1.830642e-03;0.998190;1.828219e-03\n",
    "epoch: 69;\t train:2.054516e-03;0.998134;1.919294e-03;\t test:1.859446e-03;0.998129;1.862922e-03\n",
    "epoch: 70;\t train:2.051749e-03;0.998140;1.915174e-03;\t test:1.859713e-03;0.998121;1.870316e-03\n",
    "epoch: 71;\t train:2.047911e-03;0.998135;1.918965e-03;\t test:1.840182e-03;0.998153;1.855044e-03\n",
    "epoch: 72;\t train:2.034744e-03;0.998148;1.901537e-03;\t test:1.786083e-03;0.998187;1.790220e-03\n",
    "epoch: 73;\t train:2.030020e-03;0.998154;1.900152e-03;\t test:1.804264e-03;0.998194;1.815234e-03\n",
    "epoch: 74;\t train:2.022393e-03;0.998153;1.896800e-03;\t test:1.794604e-03;0.998169;1.820397e-03\n",
    "epoch: 75;\t train:2.022016e-03;0.998149;1.896774e-03;\t test:1.823751e-03;0.998152;1.845379e-03\n",
    "epoch: 76;\t train:2.010884e-03;0.998161;1.891553e-03;\t test:1.795239e-03;0.998185;1.818036e-03\n",
    "epoch: 77;\t train:2.024395e-03;0.998145;1.904379e-03;\t test:1.776688e-03;0.998192;1.799722e-03\n",
    "epoch: 78;\t train:2.022971e-03;0.998150;1.899538e-03;\t test:1.809304e-03;0.998147;1.835538e-03\n",
    "epoch: 79;\t train:2.014794e-03;0.998145;1.900646e-03;\t test:1.805820e-03;0.998141;1.841452e-03\n",
    "epoch: 80;\t train:2.000818e-03;0.998161;1.885137e-03;\t test:1.795081e-03;0.998174;1.828082e-03\n",
    "epoch: 81;\t train:1.995158e-03;0.998157;1.884493e-03;\t test:1.776539e-03;0.998177;1.811125e-03\n",
    "epoch: 82;\t train:1.994048e-03;0.998159;1.884103e-03;\t test:1.783530e-03;0.998168;1.821404e-03\n",
    "epoch: 83;\t train:2.004568e-03;0.998151;1.893677e-03;\t test:1.766683e-03;0.998176;1.797622e-03\n",
    "epoch: 84;\t train:1.997345e-03;0.998149;1.890405e-03;\t test:1.775889e-03;0.998187;1.813888e-03\n",
    "epoch: 85;\t train:1.987978e-03;0.998158;1.883820e-03;\t test:1.773353e-03;0.998164;1.810636e-03\n",
    "epoch: 86;\t train:1.999321e-03;0.998142;1.894635e-03;\t test:1.759538e-03;0.998166;1.808876e-03\n",
    "epoch: 87;\t train:1.997971e-03;0.998138;1.898382e-03;\t test:1.779130e-03;0.998151;1.819722e-03\n",
    "epoch: 88;\t train:1.980534e-03;0.998158;1.880547e-03;\t test:1.742700e-03;0.998196;1.791956e-03\n",
    "epoch: 89;\t train:1.978422e-03;0.998151;1.882145e-03;\t test:1.766175e-03;0.998152;1.817176e-03\n",
    "epoch: 90;\t train:1.975442e-03;0.998154;1.881371e-03;\t test:1.758351e-03;0.998176;1.795834e-03\n",
    "epoch: 91;\t train:1.984499e-03;0.998144;1.889861e-03;\t test:1.752967e-03;0.998124;1.793375e-03\n",
    "epoch: 92;\t train:1.973727e-03;0.998147;1.887329e-03;\t test:1.756537e-03;0.998158;1.804491e-03\n",
    "epoch: 93;\t train:1.969905e-03;0.998149;1.880354e-03;\t test:1.750057e-03;0.998147;1.799334e-03\n",
    "epoch: 94;\t train:1.970800e-03;0.998150;1.881868e-03;\t test:1.755878e-03;0.998159;1.810874e-03\n",
    "epoch: 95;\t train:1.962106e-03;0.998156;1.874172e-03;\t test:1.722353e-03;0.998183;1.777675e-03\n",
    "epoch: 96;\t train:1.973104e-03;0.998149;1.883841e-03;\t test:1.700250e-03;0.998205;1.753466e-03\n",
    "epoch: 97;\t train:1.961292e-03;0.998153;1.878947e-03;\t test:1.769909e-03;0.998138;1.836047e-03\n",
    "epoch: 98;\t train:1.952597e-03;0.998162;1.866912e-03;\t test:1.752998e-03;0.998167;1.816530e-03\n",
    "epoch: 99;\t train:1.956106e-03;0.998156;1.872133e-03;\t test:1.697247e-03;0.998205;1.750872e-03\n",
    "\n",
    "# attached\n",
    "epoch:100;\t train:1.941430e-03;0.998161;1.863020e-03;\t test:1.739691e-03;0.998193;1.808003e-03\n",
    "epoch:101;\t train:1.936580e-03;0.998168;1.857441e-03;\t test:1.714077e-03;0.998162;1.774645e-03\n",
    "epoch:102;\t train:1.947499e-03;0.998152;1.871150e-03;\t test:1.698287e-03;0.998197;1.758418e-03\n",
    "epoch:103;\t train:1.944474e-03;0.998151;1.874674e-03;\t test:1.702367e-03;0.998172;1.768250e-03\n",
    "epoch:104;\t train:1.924715e-03;0.998168;1.856019e-03;\t test:1.724661e-03;0.998179;1.801624e-03\n",
    "epoch:105;\t train:1.939456e-03;0.998152;1.870907e-03;\t test:1.714528e-03;0.998169;1.785630e-03\n",
    "epoch:106;\t train:1.927463e-03;0.998155;1.863835e-03;\t test:1.718806e-03;0.998184;1.800176e-03\n",
    "epoch:107;\t train:1.916820e-03;0.998167;1.850669e-03;\t test:1.685695e-03;0.998145;1.745721e-03\n",
    "epoch:108;\t train:1.913406e-03;0.998162;1.853086e-03;\t test:1.686859e-03;0.998219;1.774500e-03\n",
    "epoch:109;\t train:1.920340e-03;0.998162;1.857672e-03;\t test:1.684705e-03;0.998210;1.765106e-03\n",
    "epoch:110;\t train:1.909942e-03;0.998173;1.845541e-03;\t test:1.647863e-03;0.998279;1.733184e-03\n",
    "epoch:111;\t train:1.909420e-03;0.998166;1.852941e-03;\t test:1.686377e-03;0.998183;1.768925e-03\n",
    "epoch:112;\t train:1.893877e-03;0.998173;1.837463e-03;\t test:1.673676e-03;0.998234;1.759512e-03\n",
    "epoch:113;\t train:1.905638e-03;0.998164;1.849512e-03;\t test:1.654255e-03;0.998240;1.741051e-03\n",
    "epoch:114;\t train:1.904039e-03;0.998164;1.847921e-03;\t test:1.658849e-03;0.998243;1.751753e-03\n",
    "epoch:115;\t train:1.894228e-03;0.998170;1.840012e-03;\t test:1.690349e-03;0.998203;1.780372e-03\n",
    "epoch:116;\t train:1.887090e-03;0.998184;1.831074e-03;\t test:1.656691e-03;0.998231;1.743756e-03\n",
    "epoch:117;\t train:1.888524e-03;0.998175;1.838113e-03;\t test:1.672543e-03;0.998202;1.759107e-03\n",
    "epoch:118;\t train:1.881834e-03;0.998179;1.831527e-03;\t test:1.661895e-03;0.998232;1.754573e-03\n",
    "epoch:119;\t train:1.884258e-03;0.998177;1.836630e-03;\t test:1.649608e-03;0.998238;1.748253e-03\n",
    "epoch:120;\t train:1.873708e-03;0.998178;1.829041e-03;\t test:1.646181e-03;0.998264;1.757325e-03\n",
    "epoch:121;\t train:1.874073e-03;0.998181;1.825581e-03;\t test:1.636852e-03;0.998259;1.750387e-03\n",
    "epoch:122;\t train:1.868644e-03;0.998180;1.825202e-03;\t test:1.652274e-03;0.998227;1.747894e-03\n",
    "epoch:123;\t train:1.881636e-03;0.998174;1.835390e-03;\t test:1.623525e-03;0.998247;1.714875e-03\n",
    "epoch:124;\t train:1.864054e-03;0.998184;1.820850e-03;\t test:1.625001e-03;0.998273;1.718847e-03\n",
    "epoch:125;\t train:1.863748e-03;0.998184;1.821219e-03;\t test:1.609891e-03;0.998277;1.707407e-03\n",
    "epoch:126;\t train:1.870754e-03;0.998174;1.828245e-03;\t test:1.654340e-03;0.998249;1.765044e-03\n",
    "epoch:127;\t train:1.854250e-03;0.998184;1.813799e-03;\t test:1.626063e-03;0.998243;1.723711e-03\n",
    "epoch:128;\t train:1.850847e-03;0.998183;1.813095e-03;\t test:1.631441e-03;0.998225;1.730116e-03\n",
    "epoch:129;\t train:1.854354e-03;0.998186;1.814591e-03;\t test:1.622610e-03;0.998264;1.725540e-03\n",
    "epoch:130;\t train:1.858623e-03;0.998180;1.820084e-03;\t test:1.639518e-03;0.998227;1.747886e-03\n",
    "epoch:131;\t train:1.853384e-03;0.998186;1.811805e-03;\t test:1.577884e-03;0.998309;1.676688e-03\n",
    "epoch:132;\t train:1.849472e-03;0.998180;1.815238e-03;\t test:1.596427e-03;0.998271;1.706395e-03\n",
    "epoch:133;\t train:1.842326e-03;0.998188;1.809325e-03;\t test:1.600355e-03;0.998297;1.700598e-03\n",
    "epoch:134;\t train:1.849689e-03;0.998173;1.816845e-03;\t test:1.617905e-03;0.998268;1.727452e-03\n",
    "epoch:135;\t train:1.836732e-03;0.998187;1.801475e-03;\t test:1.599201e-03;0.998262;1.700689e-03\n",
    "epoch:136;\t train:1.847632e-03;0.998175;1.820610e-03;\t test:1.581050e-03;0.998304;1.693631e-03\n",
    "epoch:137;\t train:1.843884e-03;0.998186;1.807994e-03;\t test:1.590779e-03;0.998294;1.697701e-03\n",
    "epoch:138;\t train:1.826498e-03;0.998194;1.796218e-03;\t test:1.598732e-03;0.998272;1.693337e-03\n",
    "epoch:139;\t train:1.840782e-03;0.998177;1.812786e-03;\t test:1.607012e-03;0.998275;1.712516e-03\n",
    "epoch:140;\t train:1.822266e-03;0.998197;1.792532e-03;\t test:1.591641e-03;0.998290;1.700585e-03\n",
    "epoch:141;\t train:1.821947e-03;0.998196;1.791506e-03;\t test:1.585708e-03;0.998289;1.689861e-03\n",
    "epoch:142;\t train:1.823303e-03;0.998191;1.793924e-03;\t test:1.594933e-03;0.998285;1.715050e-03\n",
    "epoch:143;\t train:1.818241e-03;0.998198;1.788694e-03;\t test:1.591116e-03;0.998284;1.716457e-03\n",
    "epoch:144;\t train:1.814444e-03;0.998197;1.788319e-03;\t test:1.589159e-03;0.998285;1.703942e-03\n",
    "epoch:145;\t train:1.825136e-03;0.998188;1.799593e-03;\t test:1.580114e-03;0.998274;1.682870e-03\n",
    "epoch:146;\t train:1.826343e-03;0.998179;1.801926e-03;\t test:1.582201e-03;0.998280;1.701998e-03\n",
    "epoch:147;\t train:1.819998e-03;0.998194;1.792539e-03;\t test:1.614316e-03;0.998275;1.740828e-03\n",
    "epoch:148;\t train:1.824873e-03;0.998184;1.801563e-03;\t test:1.590007e-03;0.998258;1.707298e-03\n",
    "epoch:149;\t train:1.806517e-03;0.998200;1.783375e-03;\t test:1.567962e-03;0.998296;1.682338e-03\n",
    "epoch:150;\t train:1.803075e-03;0.998205;1.780478e-03;\t test:1.557305e-03;0.998298;1.669522e-03\n",
    "epoch:151;\t train:1.796908e-03;0.998200;1.776197e-03;\t test:1.561606e-03;0.998295;1.666160e-03\n",
    "epoch:152;\t train:1.814071e-03;0.998191;1.794864e-03;\t test:1.580231e-03;0.998275;1.692163e-03\n",
    "epoch:153;\t train:1.817135e-03;0.998181;1.794486e-03;\t test:1.573217e-03;0.998309;1.678252e-03\n",
    "epoch:154;\t train:1.797870e-03;0.998205;1.775585e-03;\t test:1.574518e-03;0.998282;1.683839e-03\n",
    "epoch:155;\t train:1.794358e-03;0.998200;1.773321e-03;\t test:1.571699e-03;0.998268;1.687480e-03\n",
    "epoch:156;\t train:1.790311e-03;0.998205;1.770916e-03;\t test:1.557074e-03;0.998321;1.672021e-03\n",
    "epoch:157;\t train:1.800432e-03;0.998186;1.783788e-03;\t test:1.552186e-03;0.998314;1.678471e-03\n",
    "epoch:158;\t train:1.793350e-03;0.998199;1.772712e-03;\t test:1.522993e-03;0.998340;1.649229e-03\n",
    "epoch:159;\t train:1.793456e-03;0.998195;1.781079e-03;\t test:1.543520e-03;0.998317;1.663595e-03\n",
    "epoch:160;\t train:1.781776e-03;0.998200;1.767049e-03;\t test:1.549995e-03;0.998320;1.663697e-03\n",
    "epoch:161;\t train:1.789008e-03;0.998209;1.770230e-03;\t test:1.542672e-03;0.998309;1.654156e-03\n",
    "epoch:162;\t train:1.789989e-03;0.998196;1.777983e-03;\t test:1.570431e-03;0.998293;1.691194e-03\n",
    "epoch:163;\t train:1.781384e-03;0.998203;1.766183e-03;\t test:1.538859e-03;0.998318;1.657582e-03\n",
    "epoch:164;\t train:1.781036e-03;0.998200;1.770582e-03;\t test:1.569136e-03;0.998285;1.696522e-03\n",
    "epoch:165;\t train:1.778529e-03;0.998203;1.771213e-03;\t test:1.549137e-03;0.998307;1.671316e-03\n",
    "epoch:166;\t train:1.773874e-03;0.998212;1.758400e-03;\t test:1.522223e-03;0.998329;1.641856e-03\n",
    "epoch:167;\t train:1.763386e-03;0.998219;1.749996e-03;\t test:1.544159e-03;0.998314;1.672284e-03\n",
    "epoch:168;\t train:1.787032e-03;0.998188;1.778918e-03;\t test:1.509532e-03;0.998347;1.631359e-03\n",
    "epoch:169;\t train:1.771837e-03;0.998206;1.763885e-03;\t test:1.523542e-03;0.998337;1.637520e-03\n",
    "epoch:170;\t train:1.767236e-03;0.998207;1.756558e-03;\t test:1.540684e-03;0.998303;1.667455e-03\n",
    "epoch:171;\t train:1.774860e-03;0.998201;1.764439e-03;\t test:1.534371e-03;0.998329;1.658521e-03\n",
    "epoch:172;\t train:1.766205e-03;0.998206;1.757632e-03;\t test:1.525448e-03;0.998333;1.650515e-03\n",
    "epoch:173;\t train:1.759938e-03;0.998205;1.756532e-03;\t test:1.514179e-03;0.998336;1.651046e-03\n",
    "epoch:174;\t train:1.763764e-03;0.998206;1.755375e-03;\t test:1.539898e-03;0.998294;1.661927e-03\n",
    "epoch:175;\t train:1.758914e-03;0.998213;1.754088e-03;\t test:1.514967e-03;0.998325;1.636344e-03\n",
    "epoch:176;\t train:1.754947e-03;0.998209;1.750761e-03;\t test:1.536947e-03;0.998301;1.660294e-03\n",
    "epoch:177;\t train:1.758797e-03;0.998209;1.753039e-03;\t test:1.522332e-03;0.998316;1.654774e-03\n",
    "epoch:178;\t train:1.748391e-03;0.998217;1.746467e-03;\t test:1.515855e-03;0.998317;1.632936e-03\n",
    "epoch:179;\t train:1.754528e-03;0.998216;1.747804e-03;\t test:1.530515e-03;0.998305;1.659779e-03\n",
    "epoch:180;\t train:1.754437e-03;0.998210;1.753479e-03;\t test:1.506400e-03;0.998335;1.636308e-03\n",
    "epoch:181;\t train:1.753712e-03;0.998211;1.750837e-03;\t test:1.500162e-03;0.998326;1.623050e-03\n",
    "epoch:182;\t train:1.746209e-03;0.998218;1.741456e-03;\t test:1.514605e-03;0.998313;1.652245e-03\n",
    "epoch:183;\t train:1.754528e-03;0.998205;1.752917e-03;\t test:1.537649e-03;0.998297;1.666600e-03\n",
    "epoch:184;\t train:1.747727e-03;0.998207;1.748563e-03;\t test:1.504216e-03;0.998327;1.636001e-03\n",
    "epoch:185;\t train:1.746592e-03;0.998218;1.744850e-03;\t test:1.501689e-03;0.998336;1.630552e-03\n",
    "epoch:186;\t train:1.747857e-03;0.998208;1.748528e-03;\t test:1.503047e-03;0.998320;1.627961e-03\n",
    "epoch:187;\t train:1.739748e-03;0.998218;1.738399e-03;\t test:1.488855e-03;0.998354;1.613676e-03\n",
    "epoch:188;\t train:1.742050e-03;0.998219;1.742508e-03;\t test:1.535039e-03;0.998290;1.670418e-03\n",
    "epoch:189;\t train:1.747617e-03;0.998206;1.752949e-03;\t test:1.510470e-03;0.998326;1.646046e-03\n",
    "epoch:190;\t train:1.748650e-03;0.998211;1.748507e-03;\t test:1.499574e-03;0.998325;1.632144e-03\n",
    "epoch:191;\t train:1.738402e-03;0.998214;1.744992e-03;\t test:1.514086e-03;0.998313;1.649120e-03\n",
    "epoch:192;\t train:1.729223e-03;0.998225;1.731975e-03;\t test:1.517066e-03;0.998315;1.646859e-03\n",
    "epoch:193;\t train:1.732172e-03;0.998217;1.738077e-03;\t test:1.487612e-03;0.998338;1.621335e-03\n",
    "epoch:194;\t train:1.730872e-03;0.998215;1.736637e-03;\t test:1.506796e-03;0.998338;1.641642e-03\n",
    "epoch:195;\t train:1.731216e-03;0.998217;1.737694e-03;\t test:1.507848e-03;0.998337;1.637851e-03\n",
    "epoch:196;\t train:1.738757e-03;0.998208;1.746222e-03;\t test:1.486031e-03;0.998343;1.614452e-03\n",
    "epoch:197;\t train:1.731191e-03;0.998214;1.737514e-03;\t test:1.518419e-03;0.998309;1.667245e-03\n",
    "epoch:198;\t train:1.726013e-03;0.998224;1.732500e-03;\t test:1.492273e-03;0.998320;1.626487e-03\n",
    "epoch:199;\t train:1.721657e-03;0.998221;1.732491e-03;\t test:1.477473e-03;0.998350;1.605542e-03\n",
    "epoch:200;\t train:1.726062e-03;0.998224;1.736643e-03;\t test:1.471630e-03;0.998368;1.594217e-03\n",
    "epoch:201;\t train:1.720155e-03;0.998217;1.733176e-03;\t test:1.500573e-03;0.998311;1.636563e-03\n",
    "epoch:202;\t train:1.720508e-03;0.998222;1.730439e-03;\t test:1.478474e-03;0.998353;1.611709e-03\n",
    "epoch:203;\t train:1.726319e-03;0.998214;1.737019e-03;\t test:1.488788e-03;0.998344;1.621275e-03\n",
    "epoch:204;\t train:1.720292e-03;0.998219;1.732199e-03;\t test:1.504794e-03;0.998317;1.642327e-03\n",
    "epoch:205;\t train:1.712551e-03;0.998234;1.719754e-03;\t test:1.496952e-03;0.998315;1.632780e-03\n",
    "epoch:206;\t train:1.708332e-03;0.998230;1.717224e-03;\t test:1.486048e-03;0.998347;1.619440e-03\n",
    "epoch:207;\t train:1.714243e-03;0.998227;1.724357e-03;\t test:1.479546e-03;0.998349;1.609096e-03\n",
    "epoch:208;\t train:1.708307e-03;0.998230;1.722784e-03;\t test:1.467099e-03;0.998358;1.609097e-03\n",
    "epoch:209;\t train:1.705388e-03;0.998234;1.716380e-03;\t test:1.472132e-03;0.998361;1.604432e-03\n",
    "epoch:210;\t train:1.705172e-03;0.998233;1.716448e-03;\t test:1.476092e-03;0.998344;1.614079e-03\n",
    "epoch:211;\t train:1.705084e-03;0.998232;1.718653e-03;\t test:1.480363e-03;0.998339;1.615023e-03\n",
    "epoch:212;\t train:1.695339e-03;0.998239;1.710009e-03;\t test:1.468863e-03;0.998367;1.592822e-03\n",
    "epoch:213;\t train:1.706923e-03;0.998230;1.716645e-03;\t test:1.460416e-03;0.998352;1.591525e-03\n",
    "epoch:214;\t train:1.708639e-03;0.998227;1.724184e-03;\t test:1.460314e-03;0.998360;1.590578e-03\n",
    "epoch:215;\t train:1.702107e-03;0.998230;1.717600e-03;\t test:1.484194e-03;0.998322;1.624894e-03\n",
    "epoch:216;\t train:1.687253e-03;0.998241;1.705793e-03;\t test:1.469905e-03;0.998351;1.608050e-03\n",
    "epoch:217;\t train:1.693541e-03;0.998235;1.709818e-03;\t test:1.448452e-03;0.998378;1.591549e-03\n",
    "epoch:218;\t train:1.701737e-03;0.998229;1.716765e-03;\t test:1.471089e-03;0.998345;1.605067e-03\n",
    "epoch:219;\t train:1.694545e-03;0.998237;1.711736e-03;\t test:1.462804e-03;0.998359;1.593139e-03\n",
    "epoch:220;\t train:1.706133e-03;0.998220;1.725733e-03;\t test:1.454398e-03;0.998359;1.587005e-03\n",
    "epoch:221;\t train:1.693068e-03;0.998241;1.711797e-03;\t test:1.483605e-03;0.998309;1.626177e-03\n",
    "epoch:222;\t train:1.692873e-03;0.998235;1.715485e-03;\t test:1.465294e-03;0.998345;1.598335e-03\n",
    "epoch:223;\t train:1.690965e-03;0.998232;1.712518e-03;\t test:1.467148e-03;0.998350;1.605152e-03\n",
    "epoch:224;\t train:1.695931e-03;0.998230;1.716884e-03;\t test:1.449940e-03;0.998352;1.590593e-03\n",
    "epoch:225;\t train:1.683786e-03;0.998244;1.702593e-03;\t test:1.417062e-03;0.998405;1.548839e-03\n",
    "epoch:226;\t train:1.696808e-03;0.998235;1.715830e-03;\t test:1.467455e-03;0.998353;1.607996e-03\n",
    "epoch:227;\t train:1.683992e-03;0.998242;1.705066e-03;\t test:1.450981e-03;0.998359;1.596111e-03\n",
    "epoch:228;\t train:1.690170e-03;0.998231;1.712123e-03;\t test:1.437502e-03;0.998375;1.581523e-03\n",
    "epoch:229;\t train:1.681747e-03;0.998243;1.702019e-03;\t test:1.459515e-03;0.998333;1.598052e-03\n",
    "epoch:230;\t train:1.679177e-03;0.998240;1.703009e-03;\t test:1.439953e-03;0.998371;1.566308e-03\n",
    "epoch:231;\t train:1.677801e-03;0.998242;1.699523e-03;\t test:1.459169e-03;0.998334;1.607019e-03\n",
    "epoch:232;\t train:1.682527e-03;0.998242;1.706159e-03;\t test:1.458047e-03;0.998336;1.596785e-03\n",
    "epoch:233;\t train:1.688471e-03;0.998233;1.714950e-03;\t test:1.436940e-03;0.998380;1.570862e-03\n",
    "epoch:234;\t train:1.681078e-03;0.998242;1.705019e-03;\t test:1.451352e-03;0.998361;1.590894e-03\n",
    "epoch:235;\t train:1.685255e-03;0.998236;1.710118e-03;\t test:1.435641e-03;0.998373;1.575659e-03\n",
    "epoch:236;\t train:1.680818e-03;0.998233;1.708146e-03;\t test:1.438214e-03;0.998363;1.582408e-03\n",
    "epoch:237;\t train:1.684345e-03;0.998229;1.711761e-03;\t test:1.441066e-03;0.998367;1.587458e-03\n",
    "epoch:238;\t train:1.675588e-03;0.998245;1.700659e-03;\t test:1.424159e-03;0.998386;1.564341e-03\n",
    "epoch:239;\t train:1.678522e-03;0.998243;1.704180e-03;\t test:1.429886e-03;0.998369;1.572588e-03\n",
    "epoch:240;\t train:1.676034e-03;0.998241;1.701212e-03;\t test:1.447198e-03;0.998365;1.590816e-03\n",
    "epoch:241;\t train:1.663700e-03;0.998253;1.691943e-03;\t test:1.409263e-03;0.998402;1.536137e-03\n",
    "epoch:242;\t train:1.669256e-03;0.998252;1.693386e-03;\t test:1.437979e-03;0.998369;1.594268e-03\n",
    "epoch:243;\t train:1.670016e-03;0.998245;1.698580e-03;\t test:1.429972e-03;0.998374;1.563333e-03\n",
    "epoch:244;\t train:1.675653e-03;0.998239;1.705092e-03;\t test:1.441554e-03;0.998365;1.587856e-03\n",
    "epoch:245;\t train:1.658718e-03;0.998256;1.685259e-03;\t test:1.426848e-03;0.998382;1.570084e-03\n",
    "epoch:246;\t train:1.668271e-03;0.998242;1.697738e-03;\t test:1.458589e-03;0.998336;1.610620e-03\n",
    "epoch:247;\t train:1.669507e-03;0.998247;1.699628e-03;\t test:1.428034e-03;0.998362;1.574192e-03\n",
    "epoch:248;\t train:1.669105e-03;0.998246;1.701488e-03;\t test:1.454480e-03;0.998341;1.599737e-03\n",
    "epoch:249;\t train:1.668724e-03;0.998248;1.698968e-03;\t test:1.413901e-03;0.998392;1.554457e-03\n",
    "epoch:250;\t train:1.658678e-03;0.998257;1.687435e-03;\t test:1.446051e-03;0.998354;1.594351e-03\n",
    "epoch:251;\t train:1.650839e-03;0.998259;1.682951e-03;\t test:1.438184e-03;0.998370;1.573033e-03\n",
    "epoch:252;\t train:1.662553e-03;0.998250;1.694015e-03;\t test:1.425274e-03;0.998358;1.573262e-03\n",
    "epoch:253;\t train:1.661471e-03;0.998249;1.691583e-03;\t test:1.421734e-03;0.998366;1.574520e-03\n",
    "epoch:254;\t train:1.665235e-03;0.998242;1.698188e-03;\t test:1.433295e-03;0.998368;1.580100e-03\n",
    "epoch:255;\t train:1.654626e-03;0.998260;1.682780e-03;\t test:1.415968e-03;0.998393;1.556242e-03\n",
    "epoch:256;\t train:1.650863e-03;0.998256;1.683942e-03;\t test:1.415063e-03;0.998395;1.554382e-03\n",
    "epoch:257;\t train:1.669139e-03;0.998239;1.705967e-03;\t test:1.447707e-03;0.998355;1.592035e-03\n",
    "epoch:258;\t train:1.664779e-03;0.998245;1.695097e-03;\t test:1.417150e-03;0.998390;1.557628e-03\n",
    "epoch:259;\t train:1.648794e-03;0.998257;1.679854e-03;\t test:1.454079e-03;0.998357;1.598937e-03\n",
    "epoch:260;\t train:1.646327e-03;0.998255;1.683553e-03;\t test:1.433468e-03;0.998359;1.592634e-03\n",
    "epoch:261;\t train:1.655534e-03;0.998253;1.687955e-03;\t test:1.432870e-03;0.998372;1.580501e-03\n",
    "epoch:262;\t train:1.659547e-03;0.998246;1.695548e-03;\t test:1.424367e-03;0.998355;1.575158e-03\n",
    "epoch:263;\t train:1.653136e-03;0.998258;1.685300e-03;\t test:1.445048e-03;0.998358;1.600755e-03\n",
    "epoch:264;\t train:1.647382e-03;0.998256;1.680752e-03;\t test:1.417185e-03;0.998378;1.565166e-03\n",
    "epoch:265;\t train:1.652929e-03;0.998250;1.689719e-03;\t test:1.410184e-03;0.998390;1.557093e-03\n",
    "epoch:266;\t train:1.648030e-03;0.998262;1.680720e-03;\t test:1.416665e-03;0.998379;1.566419e-03\n",
    "epoch:267;\t train:1.637248e-03;0.998263;1.673516e-03;\t test:1.427590e-03;0.998375;1.579131e-03\n",
    "epoch:268;\t train:1.643664e-03;0.998260;1.680615e-03;\t test:1.425493e-03;0.998361;1.572676e-03\n",
    "epoch:269;\t train:1.654446e-03;0.998252;1.688323e-03;\t test:1.425478e-03;0.998369;1.571201e-03\n",
    "epoch:270;\t train:1.652106e-03;0.998250;1.687494e-03;\t test:1.418610e-03;0.998376;1.567556e-03\n",
    "epoch:271;\t train:1.648791e-03;0.998253;1.687389e-03;\t test:1.424771e-03;0.998364;1.576824e-03\n",
    "epoch:272;\t train:1.641105e-03;0.998259;1.680395e-03;\t test:1.375966e-03;0.998405;1.523552e-03\n",
    "epoch:273;\t train:1.634332e-03;0.998267;1.671937e-03;\t test:1.449562e-03;0.998348;1.587081e-03\n",
    "epoch:274;\t train:1.640454e-03;0.998261;1.676861e-03;\t test:1.395459e-03;0.998389;1.547720e-03\n",
    "epoch:275;\t train:1.641510e-03;0.998256;1.681663e-03;\t test:1.407487e-03;0.998395;1.552096e-03\n",
    "epoch:276;\t train:1.640667e-03;0.998258;1.682638e-03;\t test:1.428221e-03;0.998342;1.587362e-03\n",
    "epoch:277;\t train:1.636366e-03;0.998266;1.672219e-03;\t test:1.416288e-03;0.998391;1.561464e-03\n",
    "epoch:278;\t train:1.641294e-03;0.998260;1.681747e-03;\t test:1.404862e-03;0.998382;1.550669e-03\n",
    "epoch:279;\t train:1.641776e-03;0.998255;1.680817e-03;\t test:1.375495e-03;0.998415;1.518481e-03\n",
    "epoch:280;\t train:1.638963e-03;0.998257;1.680382e-03;\t test:1.410448e-03;0.998375;1.559388e-03\n",
    "epoch:281;\t train:1.641177e-03;0.998253;1.682830e-03;\t test:1.419746e-03;0.998354;1.574072e-03\n",
    "epoch:282;\t train:1.638149e-03;0.998262;1.677556e-03;\t test:1.391718e-03;0.998406;1.539103e-03\n",
    "epoch:283;\t train:1.641591e-03;0.998254;1.682933e-03;\t test:1.402884e-03;0.998368;1.556622e-03\n",
    "epoch:284;\t train:1.630491e-03;0.998265;1.673363e-03;\t test:1.398663e-03;0.998375;1.550214e-03\n",
    "epoch:285;\t train:1.634652e-03;0.998259;1.676092e-03;\t test:1.381821e-03;0.998395;1.531392e-03\n",
    "epoch:286;\t train:1.633694e-03;0.998261;1.679053e-03;\t test:1.409044e-03;0.998364;1.558868e-03\n",
    "epoch:287;\t train:1.643351e-03;0.998250;1.684679e-03;\t test:1.396459e-03;0.998390;1.549610e-03\n",
    "epoch:288;\t train:1.629854e-03;0.998259;1.673962e-03;\t test:1.396006e-03;0.998385;1.543797e-03\n",
    "epoch:289;\t train:1.626640e-03;0.998265;1.670278e-03;\t test:1.399200e-03;0.998381;1.556042e-03\n",
    "epoch:290;\t train:1.634723e-03;0.998259;1.681324e-03;\t test:1.397704e-03;0.998368;1.553187e-03\n",
    "epoch:291;\t train:1.634319e-03;0.998257;1.681309e-03;\t test:1.384950e-03;0.998406;1.528719e-03\n",
    "epoch:292;\t train:1.624222e-03;0.998266;1.670621e-03;\t test:1.396271e-03;0.998381;1.547505e-03\n",
    "epoch:293;\t train:1.625333e-03;0.998264;1.671422e-03;\t test:1.393696e-03;0.998392;1.545345e-03\n",
    "epoch:294;\t train:1.631527e-03;0.998259;1.675935e-03;\t test:1.371522e-03;0.998395;1.529297e-03\n",
    "epoch:295;\t train:1.626665e-03;0.998265;1.670865e-03;\t test:1.359643e-03;0.998404;1.505784e-03\n",
    "epoch:296;\t train:1.616556e-03;0.998268;1.663102e-03;\t test:1.388109e-03;0.998389;1.539631e-03\n",
    "epoch:297;\t train:1.622301e-03;0.998266;1.667402e-03;\t test:1.391051e-03;0.998385;1.546056e-03\n",
    "epoch:298;\t train:1.614000e-03;0.998275;1.659016e-03;\t test:1.378988e-03;0.998395;1.530024e-03\n",
    "epoch:299;\t train:1.610867e-03;0.998273;1.658164e-03;\t test:1.394352e-03;0.998382;1.547077e-03\n",
    "epoch:300;\t train:1.626397e-03;0.998259;1.676995e-03;\t test:1.371228e-03;0.998397;1.527078e-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = test_dataset[0]\n",
    "test_image2, test_label2 = train_transform(test_image, test_label)\n",
    "test_image2 = nd.expand_dims(test_image2,0)\n",
    "print('tensor shape:', test_image2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, cls_preds, box_preds = net(test_image2.as_in_context(ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predictions to real object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxDetection\n",
    "cls_probs = nd.SoftmaxActivation(nd.transpose(cls_preds, (0, 2, 1)), mode='channel')\n",
    "output = MultiBoxDetection(cls_prob=cls_probs, loc_pred=box_preds, anchor=anchors, force_suppress=True, clip=True, nms_topk=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('cluster')\n",
    "def display(img, out, thresh=0.5):\n",
    "    import random\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    mpl.rcParams['figure.figsize'] = (10,10)\n",
    "    img = img.asnumpy()\n",
    "    img = np.transpose(img,(2,3,1,0))\n",
    "    img = np.squeeze(img)\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    for det in out:\n",
    "        cid = int(det[0])\n",
    "        if cid == 0:\n",
    "            continue\n",
    "        score = det[1]\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        scales = [img.shape[1], img.shape[0]] * 2\n",
    "        xmin, ymin, xmax, ymax = [int(p * s) for p, s in zip(det[2:6].tolist(), scales)]\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False,\n",
    "                             edgecolor='red', linewidth=3)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[cid]\n",
    "        plt.gca().text(xmin, ymin-2, '{:s} {:.3f}'.format(text, score),\n",
    "                       bbox=dict(facecolor='red', alpha=0.5),\n",
    "                       fontsize=12, color='white')\n",
    "\n",
    "display(test_image2, output[0].asnumpy(), thresh=0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
