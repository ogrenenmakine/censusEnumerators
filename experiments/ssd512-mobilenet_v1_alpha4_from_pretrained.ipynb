{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.contrib.ndarray import MultiBoxPrior\n",
    "from mxnet.gluon.contrib import nn as nn_contrib\n",
    "from mxnet.gluon import nn\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict classes\n",
    "- channel `i*(num_class+1)` store the scores for this box contains only background\n",
    "- channel `i*(num_class+1)+1+j` store the scores for this box contains an object from the *j*-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predictor(num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict anchor boxes\n",
    "- $t_x = (Y_x - b_x) / b_{width}$\n",
    "- $t_y = (Y_y - b_y) / b_{height}$\n",
    "- $t_{width} = (Y_{width} - b_{width}) / b_{width}$\n",
    "- $t_{height} = (Y_{height} - b_{height}) / b_{height}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_predictor(num_anchors):\n",
    "    return nn.Conv2D(num_anchors * 4, 3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage preditions from multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    return nd.flatten(nd.transpose(pred, axes=(0, 2, 3, 1)))\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    return nd.concat(*preds, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_layer(nfilters, stride, expension_constant):\n",
    "    out = nn.HybridSequential()\n",
    "    out.add(nn.Conv2D(nfilters, 3, strides=stride, padding=1, groups=nfilters, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(nfilters*expension_constant, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global alpha\n",
    "alpha = 0.25\n",
    "num_filters = int(32*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Body network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "def s16():\n",
    "    out = nn.HybridSequential()\n",
    "    # conv_0 layer\n",
    "    out.add(nn.Conv2D(num_filters, 3, strides=2, padding=1, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_1 layer\n",
    "    out.add(dp_layer(num_filters, 1, 2))\n",
    "    # conv_2 layer\n",
    "    out.add(dp_layer(num_filters*2, 2, 2))\n",
    "    # conv_3 layer\n",
    "    out.add(dp_layer(num_filters*4, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*4, 3, strides=2, padding=1, groups=num_filters*4, use_bias=False))\n",
    "    #out.load_parameters(\"weights/mobilenet_0_25_s16_org.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def s32():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*8, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_4_layer\n",
    "    out.add(dp_layer(num_filters*8, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*8, 3, strides=2, padding=1, groups=num_filters*8, use_bias=False))\n",
    "    #out.load_parameters(\"weights/mobilenet_0_25_s32_org.params\", ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b1():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_6_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    #out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b2():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_7_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    #out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b3():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_8_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    #out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out\n",
    "\n",
    "def b4():\n",
    "    out = nn.HybridSequential()\n",
    "    # from last layer\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    out.add(nn.Conv2D(num_filters*16, 1, strides=1, padding=0, use_bias=False))\n",
    "    out.add(nn.BatchNorm(use_global_stats=False, epsilon=1e-05, momentum=0.9, axis=1))\n",
    "    out.add(nn.Activation('relu'))\n",
    "    # conv_9_layer\n",
    "    out.add(dp_layer(num_filters*16, 1, 1))\n",
    "    out.add(nn.Conv2D(num_filters*16, 3, strides=2, padding=1, groups=num_filters*16, use_bias=False))\n",
    "    #out.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    out.hybridize()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_model(num_anchors, num_classes):\n",
    "    class_preds = nn.Sequential()\n",
    "    box_preds = nn.Sequential()\n",
    "    \n",
    "    for scale in range(6):\n",
    "        class_preds.add(class_predictor(num_anchors, num_classes))\n",
    "        box_preds.add(box_predictor(num_anchors))\n",
    "    \n",
    "    #class_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    #box_preds.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "    return s16(), s32(), b1(), b2(), b3(), b4(), class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_forward(x, s16, s32, b1, b2, b3, b4, class_preds, box_preds, sizes, ratios):\n",
    "    default_anchors = []\n",
    "    predicted_boxes = []  \n",
    "    predicted_classes = []\n",
    "\n",
    "    x = s16(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[0], ratios=ratios[0]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[0](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[0](x)))\n",
    "    \n",
    "    x = s32(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[1], ratios=ratios[1]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[1](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[1](x)))\n",
    "    \n",
    "    x = b1(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[2], ratios=ratios[2]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[2](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[2](x)))\n",
    "    \n",
    "    x = b2(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[3], ratios=ratios[3]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[3](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[3](x)))\n",
    "    \n",
    "    x = b3(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[4], ratios=ratios[4]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[4](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[4](x)))\n",
    "    \n",
    "    x = b4(x)\n",
    "    default_anchors.append(MultiBoxPrior(x, sizes=sizes[5], ratios=ratios[5]))\n",
    "    predicted_boxes.append(flatten_prediction(box_preds[5](x)))\n",
    "    predicted_classes.append(flatten_prediction(class_preds[5](x)))\n",
    "\n",
    "    return default_anchors, predicted_classes, predicted_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "class SSD(gluon.Block):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(SSD, self).__init__(**kwargs)\n",
    "        self.anchor_sizes = [[0.04, 0.1],[0.1,0.26],[0.26,0.42],[0.42,0.58],[0.58,0.74],[0.74,0.9],[0.9,1.06]]\n",
    "        self.anchor_ratios = [[1, 2, .5]] * 6\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.s16, self.s32, self.b1, self.b2, self.b3, self.b4, self.class_preds, self.box_preds = ssd_model(4, num_classes)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        default_anchors, predicted_classes, predicted_boxes = ssd_forward(x, self.s16, self.s32, self.b1, self.b2, self.b3, self.b4,\n",
    "            self.class_preds, self.box_preds, self.anchor_sizes, self.anchor_ratios)\n",
    "        anchors = concat_predictions(default_anchors)\n",
    "        box_preds = concat_predictions(predicted_boxes)\n",
    "        class_preds = concat_predictions(predicted_classes)\n",
    "        class_preds = nd.reshape(class_preds, shape=(0, -1, self.num_classes + 1))\n",
    "        \n",
    "        return anchors, class_preds, box_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs of SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = SSD(2)\n",
    "#net.initialize(mx.init.Xavier(magnitude=2), ctx=ctx)\n",
    "net.load_parameters(\"process/ssd_99.params\",ctx=ctx)\n",
    "x = nd.zeros((1, 3, 512, 512),ctx=ctx)\n",
    "default_anchors, class_predictions, box_predictions = net(x)\n",
    "print('Outputs:', 'anchors', default_anchors.shape, 'class prediction', class_predictions.shape, 'box prediction', box_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.NACDDetection import NACDDetection\n",
    "\n",
    "train_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'train')])\n",
    "test_dataset = NACDDetection(splits=[('NACDwNegswAugCropped', 'test')])\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Test images:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source import NACDTransform\n",
    "width, height = 512, 512\n",
    "train_transform = NACDTransform.NACDDefaultTransform(width, height, False)\n",
    "test_transform = NACDTransform.NACDDefaultTransform(width, height, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd\n",
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label = test_dataset[0]\n",
    "bboxes = train_label[:, :4]\n",
    "cids = train_label[:, 4:5]\n",
    "print('image:', train_image.shape)\n",
    "print('bboxes:', bboxes.shape, 'class ids:', cids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2, train_label2 = train_transform(train_image, train_label)\n",
    "print('tensor shape:', train_image2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 4\n",
    "\n",
    "batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset.transform(test_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(test_loader):\n",
    "    if ib > 3:\n",
    "        break\n",
    "    print('data:', batch[0].shape, 'label:', batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array((0.485, 0.456, 0.406))\n",
    "train_image2 = (train_image2 * 255).clip(0, 255)\n",
    "ax = viz.plot_bbox(train_image2.asnumpy(), train_label2[:, :4],\n",
    "                   labels=train_label2[:, 4:5],\n",
    "                   class_names=train_dataset.classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxTarget\n",
    "def training_targets(default_anchors, class_predicts, labels):\n",
    "    class_predicts = nd.transpose(class_predicts, axes=(0, 2, 1))\n",
    "    z = MultiBoxTarget(anchor=default_anchors.as_in_context(mx.cpu()), label=labels.as_in_context(mx.cpu()), cls_pred=class_predicts.as_in_context(mx.cpu()))\n",
    "    box_target = z[0].as_in_context(ctx)  # box offset target for (x, y, width, height)\n",
    "    box_mask = z[1].as_in_context(ctx)  # mask is used to ignore box offsets we don't want to penalize, e.g. negative samples\n",
    "    cls_target = z[2].as_in_context(ctx)  # cls_target is an array of labels for all anchors boxes\n",
    "    return box_target, box_mask, cls_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertlbl(y):\n",
    "    mtrx = y[:,:,0:4]\n",
    "    mtrx = mtrx.asnumpy()\n",
    "    mtrx[mtrx == -1] = -width\n",
    "    mtrx = mtrx/512\n",
    "    return mx.nd.concat(nd.expand_dims(y[:,:,4],2),mx.nd.array(mtrx),dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(gluon.loss.Loss):\n",
    "    def __init__(self, axis=-1, alpha=0.25, gamma=2, batch_axis=0, **kwargs):\n",
    "        super(FocalLoss, self).__init__(None, batch_axis, **kwargs)\n",
    "        self._axis = axis\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label):\n",
    "        output = F.softmax(output)\n",
    "        pt = F.pick(output, label, axis=self._axis, keepdims=True)\n",
    "        loss = -self._alpha * ((1 - pt) ** self._gamma) * F.log(pt)\n",
    "        return F.mean(loss, axis=self._batch_axis, exclude=True)\n",
    "\n",
    "# cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "cls_loss = FocalLoss()\n",
    "print(cls_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(gluon.loss.Loss):\n",
    "    def __init__(self, batch_axis=0, **kwargs):\n",
    "        super(SmoothL1Loss, self).__init__(None, batch_axis, **kwargs)\n",
    "    \n",
    "    def hybrid_forward(self, F, output, label, mask):\n",
    "        loss = F.smooth_l1((output - label) * mask, scalar=1.0)\n",
    "        return F.mean(loss, self._batch_axis, exclude=True)\n",
    "\n",
    "box_loss = SmoothL1Loss()\n",
    "print(box_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mxnet import autograd as ag\n",
    "from gluoncv.loss import SSDMultiBoxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop params\n",
    "epochs = 301\n",
    "start_epoch = 100\n",
    "\n",
    "# initialize trainer\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 1e-1, 'wd': 4e-5})\n",
    "\n",
    "# evaluation metrics\n",
    "cls_metric = mx.metric.Accuracy()\n",
    "box_metric = mx.metric.MAE()\n",
    "cls_metric_test = mx.metric.Accuracy()\n",
    "box_metric_test = mx.metric.MAE()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    # reset iterator and tick\n",
    "    #train_data.reset()\n",
    "    cls_metric.reset()\n",
    "    box_metric.reset()\n",
    "    tic = time.time()\n",
    "    train_loss = 0\n",
    "    # iterate through all batch\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # record gradients\n",
    "        with ag.record():\n",
    "            x = batch[0].as_in_context(ctx)\n",
    "            y = batch[1].as_in_context(ctx)\n",
    "            lbl = convertlbl(batch[1])\n",
    "            default_anchors, class_predictions, box_predictions = net(x)\n",
    "            box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "            # losses\n",
    "            loss1 = cls_loss(class_predictions, cls_target)\n",
    "            loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "            # sum all losses\n",
    "            loss = loss1 + loss2\n",
    "            train_loss += nd.sum(loss).asscalar()\n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "        # apply \n",
    "        trainer.step(batch_size, ignore_stale_grad=True)\n",
    "        # update metrics\n",
    "        cls_metric.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_train, val1_train = cls_metric.get()\n",
    "    name2_train, val2_train = box_metric.get()\n",
    "\n",
    "    cls_metric_test.reset()\n",
    "    box_metric_test.reset()\n",
    "    tic = time.time()\n",
    "    test_loss = 0\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        # record gradients\n",
    "        x = batch[0].as_in_context(ctx)\n",
    "        y = batch[1].as_in_context(ctx)\n",
    "        lbl = convertlbl(batch[1])\n",
    "        default_anchors, class_predictions, box_predictions = net(x)\n",
    "        box_target, box_mask, cls_target = training_targets(default_anchors, class_predictions, lbl)\n",
    "        # losses\n",
    "        loss1 = cls_loss(class_predictions, cls_target)\n",
    "        loss2 = box_loss(box_predictions, box_target, box_mask)\n",
    "        # sum all losses\n",
    "        loss = loss1 + loss2\n",
    "        test_loss += nd.sum(loss).asscalar()\n",
    "        # update metrics\n",
    "        cls_metric_test.update([cls_target], [nd.transpose(class_predictions, (0, 2, 1))])\n",
    "        box_metric_test.update([box_target], [box_predictions * box_mask])\n",
    "        #if (i + 1) % log_interval == 0:\n",
    "    toc = time.time()\n",
    "    name1_test, val1_test = cls_metric_test.get()\n",
    "    name2_test, val2_test = box_metric_test.get()\n",
    "    print('epoch:%3d;\\t train:%.6e;%f;%.6e;\\t test:%.6e;%f;%.6e'\n",
    "          %(epoch, train_loss/len(train_dataset), val1_train, val2_train, test_loss/len(test_dataset), val1_test, val2_test))\n",
    "\n",
    "    net.save_parameters('process/ssd_%d.params' % epoch)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# detached\n",
    "epoch:  1;\t train:1.001266e-02;0.984155;2.032372e-03;\t test:5.732625e-03;0.994700;1.984987e-03\n",
    "epoch:  2;\t train:5.120501e-03;0.996053;2.014282e-03;\t test:4.127182e-03;0.996767;1.949473e-03\n",
    "epoch:  3;\t train:4.122311e-03;0.996992;2.012809e-03;\t test:3.556125e-03;0.997248;1.940761e-03\n",
    "epoch:  4;\t train:3.636892e-03;0.997359;1.987458e-03;\t test:3.210800e-03;0.997552;1.911640e-03\n",
    "epoch:  5;\t train:3.374393e-03;0.997584;2.000231e-03;\t test:3.056045e-03;0.997712;1.950544e-03\n",
    "epoch:  6;\t train:3.164508e-03;0.997750;1.976378e-03;\t test:2.876841e-03;0.997825;1.952521e-03\n",
    "epoch:  7;\t train:3.030090e-03;0.997833;1.998359e-03;\t test:2.721244e-03;0.997914;1.922196e-03\n",
    "epoch:  8;\t train:2.900282e-03;0.997913;1.978325e-03;\t test:2.651506e-03;0.997946;1.944521e-03\n",
    "epoch:  9;\t train:2.804570e-03;0.997950;1.975061e-03;\t test:2.539959e-03;0.998009;1.898470e-03\n",
    "epoch: 10;\t train:2.731558e-03;0.997979;1.973836e-03;\t test:2.467860e-03;0.998023;1.911131e-03\n",
    "epoch: 11;\t train:2.687887e-03;0.997986;1.985265e-03;\t test:2.432113e-03;0.998032;1.916359e-03\n",
    "epoch: 12;\t train:2.612627e-03;0.998029;1.956787e-03;\t test:2.378936e-03;0.998067;1.902372e-03\n",
    "epoch: 13;\t train:2.570337e-03;0.998039;1.960790e-03;\t test:2.336689e-03;0.998068;1.891659e-03\n",
    "epoch: 14;\t train:2.546000e-03;0.998048;1.963187e-03;\t test:2.324488e-03;0.998085;1.901829e-03\n",
    "epoch: 15;\t train:2.527890e-03;0.998044;1.981261e-03;\t test:2.296326e-03;0.998089;1.904335e-03\n",
    "epoch: 16;\t train:2.491980e-03;0.998064;1.966874e-03;\t test:2.299503e-03;0.998076;1.935773e-03\n",
    "epoch: 17;\t train:2.463299e-03;0.998074;1.966689e-03;\t test:2.254260e-03;0.998107;1.898027e-03\n",
    "epoch: 18;\t train:2.427322e-03;0.998098;1.951102e-03;\t test:2.213851e-03;0.998113;1.901429e-03\n",
    "epoch: 19;\t train:2.430852e-03;0.998086;1.970465e-03;\t test:2.207221e-03;0.998104;1.896203e-03\n",
    "epoch: 20;\t train:2.397970e-03;0.998101;1.959993e-03;\t test:2.196195e-03;0.998103;1.908798e-03\n",
    "epoch: 21;\t train:2.373805e-03;0.998118;1.947101e-03;\t test:2.181175e-03;0.998114;1.904463e-03\n",
    "epoch: 22;\t train:2.374206e-03;0.998110;1.961062e-03;\t test:2.174052e-03;0.998103;1.921109e-03\n",
    "epoch: 23;\t train:2.350889e-03;0.998119;1.953275e-03;\t test:2.130931e-03;0.998138;1.886516e-03\n",
    "epoch: 24;\t train:2.322411e-03;0.998143;1.933944e-03;\t test:2.138517e-03;0.998088;1.890237e-03\n",
    "epoch: 25;\t train:2.317038e-03;0.998130;1.946364e-03;\t test:2.109564e-03;0.998133;1.901280e-03\n",
    "epoch: 26;\t train:2.312693e-03;0.998125;1.954780e-03;\t test:2.122649e-03;0.998114;1.906843e-03\n",
    "epoch: 27;\t train:2.293859e-03;0.998139;1.940016e-03;\t test:2.083759e-03;0.998141;1.890969e-03\n",
    "epoch: 28;\t train:2.285103e-03;0.998138;1.945760e-03;\t test:2.090514e-03;0.998135;1.908057e-03\n",
    "epoch: 29;\t train:2.273012e-03;0.998143;1.938448e-03;\t test:2.099330e-03;0.998101;1.926664e-03\n",
    "epoch: 30;\t train:2.274895e-03;0.998130;1.951958e-03;\t test:2.071556e-03;0.998132;1.907718e-03\n",
    "epoch: 31;\t train:2.256111e-03;0.998142;1.942688e-03;\t test:2.024822e-03;0.998169;1.869222e-03\n",
    "epoch: 32;\t train:2.261380e-03;0.998128;1.954577e-03;\t test:2.026774e-03;0.998146;1.885906e-03\n",
    "epoch: 33;\t train:2.229771e-03;0.998148;1.934087e-03;\t test:2.042962e-03;0.998139;1.896527e-03\n",
    "epoch: 34;\t train:2.220095e-03;0.998148;1.930079e-03;\t test:2.037466e-03;0.998110;1.901083e-03\n",
    "epoch: 35;\t train:2.201268e-03;0.998159;1.920369e-03;\t test:1.970277e-03;0.998184;1.835757e-03\n",
    "epoch: 36;\t train:2.224403e-03;0.998129;1.951787e-03;\t test:1.996823e-03;0.998157;1.891889e-03\n",
    "epoch: 37;\t train:2.188285e-03;0.998155;1.923164e-03;\t test:1.979791e-03;0.998162;1.867085e-03\n",
    "epoch: 38;\t train:2.191095e-03;0.998150;1.924373e-03;\t test:1.965207e-03;0.998184;1.859279e-03\n",
    "epoch: 39;\t train:2.183460e-03;0.998146;1.929899e-03;\t test:1.944639e-03;0.998188;1.850490e-03\n",
    "epoch: 40;\t train:2.176271e-03;0.998152;1.923851e-03;\t test:1.947217e-03;0.998164;1.863451e-03\n",
    "epoch: 41;\t train:2.151246e-03;0.998160;1.912269e-03;\t test:1.944347e-03;0.998180;1.854406e-03\n",
    "epoch: 42;\t train:2.166190e-03;0.998140;1.931192e-03;\t test:1.921073e-03;0.998193;1.836454e-03\n",
    "epoch: 43;\t train:2.156894e-03;0.998148;1.924784e-03;\t test:1.929648e-03;0.998154;1.852354e-03\n",
    "epoch: 44;\t train:2.144693e-03;0.998151;1.917427e-03;\t test:1.931367e-03;0.998167;1.863991e-03\n",
    "epoch: 45;\t train:2.139204e-03;0.998148;1.922139e-03;\t test:1.905710e-03;0.998186;1.824944e-03\n",
    "epoch: 46;\t train:2.123749e-03;0.998164;1.903440e-03;\t test:1.912816e-03;0.998146;1.858856e-03\n",
    "epoch: 47;\t train:2.119983e-03;0.998157;1.909495e-03;\t test:1.912952e-03;0.998142;1.862613e-03\n",
    "epoch: 48;\t train:2.120950e-03;0.998152;1.909915e-03;\t test:1.893442e-03;0.998189;1.847756e-03\n",
    "epoch: 49;\t train:2.104314e-03;0.998164;1.897683e-03;\t test:1.890288e-03;0.998140;1.840661e-03\n",
    "epoch: 50;\t train:2.098852e-03;0.998165;1.899062e-03;\t test:1.890986e-03;0.998170;1.849720e-03\n",
    "epoch: 51;\t train:2.111653e-03;0.998146;1.915555e-03;\t test:1.869209e-03;0.998196;1.840772e-03\n",
    "epoch: 52;\t train:2.099230e-03;0.998153;1.906699e-03;\t test:1.870650e-03;0.998174;1.844617e-03\n",
    "epoch: 53;\t train:2.084981e-03;0.998162;1.896908e-03;\t test:1.870360e-03;0.998177;1.851225e-03\n",
    "epoch: 54;\t train:2.088300e-03;0.998152;1.901735e-03;\t test:1.843517e-03;0.998180;1.815933e-03\n",
    "epoch: 55;\t train:2.079003e-03;0.998160;1.900220e-03;\t test:1.847287e-03;0.998194;1.826076e-03\n",
    "epoch: 56;\t train:2.086031e-03;0.998148;1.907361e-03;\t test:1.846134e-03;0.998180;1.833750e-03\n",
    "epoch: 57;\t train:2.072646e-03;0.998158;1.900521e-03;\t test:1.859005e-03;0.998171;1.849457e-03\n",
    "epoch: 58;\t train:2.059987e-03;0.998159;1.891644e-03;\t test:1.843028e-03;0.998199;1.835447e-03\n",
    "epoch: 59;\t train:2.053848e-03;0.998167;1.884219e-03;\t test:1.802440e-03;0.998209;1.806149e-03\n",
    "epoch: 60;\t train:2.056891e-03;0.998158;1.894453e-03;\t test:1.824590e-03;0.998182;1.819263e-03\n",
    "epoch: 61;\t train:2.050194e-03;0.998161;1.888873e-03;\t test:1.822308e-03;0.998162;1.827195e-03\n",
    "epoch: 62;\t train:2.038226e-03;0.998171;1.877896e-03;\t test:1.800006e-03;0.998205;1.794535e-03\n",
    "epoch: 63;\t train:2.056224e-03;0.998149;1.900075e-03;\t test:1.815658e-03;0.998181;1.826404e-03\n",
    "epoch: 64;\t train:2.033639e-03;0.998176;1.874783e-03;\t test:1.776639e-03;0.998233;1.781841e-03\n",
    "epoch: 65;\t train:2.039801e-03;0.998158;1.891194e-03;\t test:1.803631e-03;0.998192;1.816708e-03\n",
    "epoch: 66;\t train:2.036258e-03;0.998157;1.889365e-03;\t test:1.815557e-03;0.998191;1.834493e-03\n",
    "epoch: 67;\t train:2.031238e-03;0.998159;1.886378e-03;\t test:1.791125e-03;0.998209;1.804665e-03\n",
    "epoch: 68;\t train:2.010085e-03;0.998174;1.869770e-03;\t test:1.810881e-03;0.998171;1.839291e-03\n",
    "epoch: 69;\t train:2.029778e-03;0.998155;1.890761e-03;\t test:1.765321e-03;0.998217;1.788120e-03\n",
    "epoch: 70;\t train:2.025199e-03;0.998155;1.886516e-03;\t test:1.780477e-03;0.998201;1.798282e-03\n",
    "epoch: 71;\t train:2.009057e-03;0.998164;1.875826e-03;\t test:1.798283e-03;0.998165;1.831140e-03\n",
    "epoch: 72;\t train:2.016786e-03;0.998154;1.883575e-03;\t test:1.760589e-03;0.998221;1.798192e-03\n",
    "epoch: 73;\t train:2.014181e-03;0.998155;1.885419e-03;\t test:1.743417e-03;0.998233;1.770734e-03\n",
    "epoch: 74;\t train:2.012017e-03;0.998152;1.887173e-03;\t test:1.774627e-03;0.998197;1.803923e-03\n",
    "epoch: 75;\t train:2.003420e-03;0.998163;1.873634e-03;\t test:1.759012e-03;0.998198;1.796845e-03\n",
    "epoch: 76;\t train:1.994859e-03;0.998164;1.871180e-03;\t test:1.758688e-03;0.998212;1.787812e-03\n",
    "epoch: 77;\t train:1.987331e-03;0.998169;1.862051e-03;\t test:1.787538e-03;0.998196;1.815022e-03\n",
    "epoch: 78;\t train:1.997764e-03;0.998162;1.872364e-03;\t test:1.768498e-03;0.998198;1.800287e-03\n",
    "epoch: 79;\t train:1.994794e-03;0.998159;1.873840e-03;\t test:1.776826e-03;0.998175;1.821190e-03\n",
    "epoch: 80;\t train:1.988056e-03;0.998165;1.864188e-03;\t test:1.747875e-03;0.998216;1.777778e-03\n",
    "epoch: 81;\t train:1.997613e-03;0.998153;1.878985e-03;\t test:1.795630e-03;0.998167;1.834754e-03\n",
    "epoch: 82;\t train:1.989304e-03;0.998159;1.869948e-03;\t test:1.769618e-03;0.998165;1.806798e-03\n",
    "epoch: 83;\t train:1.986149e-03;0.998164;1.866689e-03;\t test:1.749401e-03;0.998203;1.784504e-03\n",
    "epoch: 84;\t train:1.995354e-03;0.998149;1.878399e-03;\t test:1.759871e-03;0.998179;1.799716e-03\n",
    "epoch: 85;\t train:1.983537e-03;0.998157;1.867815e-03;\t test:1.771091e-03;0.998179;1.815349e-03\n",
    "epoch: 86;\t train:1.988703e-03;0.998160;1.867523e-03;\t test:1.745843e-03;0.998177;1.791954e-03\n",
    "epoch: 87;\t train:1.985198e-03;0.998147;1.871742e-03;\t test:1.769053e-03;0.998164;1.809436e-03\n",
    "epoch: 88;\t train:1.985415e-03;0.998151;1.866633e-03;\t test:1.759758e-03;0.998174;1.798616e-03\n",
    "epoch: 89;\t train:1.984447e-03;0.998150;1.866765e-03;\t test:1.740241e-03;0.998171;1.779739e-03\n",
    "epoch: 90;\t train:1.981238e-03;0.998150;1.867177e-03;\t test:1.724097e-03;0.998151;1.759882e-03\n",
    "epoch: 91;\t train:1.969968e-03;0.998157;1.854491e-03;\t test:1.754766e-03;0.998162;1.796570e-03\n",
    "epoch: 92;\t train:1.975332e-03;0.998147;1.863345e-03;\t test:1.747086e-03;0.998151;1.792243e-03\n",
    "epoch: 93;\t train:1.968769e-03;0.998153;1.857487e-03;\t test:1.723140e-03;0.998179;1.763075e-03\n",
    "epoch: 94;\t train:1.969985e-03;0.998146;1.857762e-03;\t test:1.737155e-03;0.998162;1.774579e-03\n",
    "epoch: 95;\t train:1.968654e-03;0.998144;1.859341e-03;\t test:1.725399e-03;0.998175;1.775840e-03\n",
    "epoch: 96;\t train:1.971241e-03;0.998141;1.861107e-03;\t test:1.732033e-03;0.998141;1.782098e-03\n",
    "epoch: 97;\t train:1.961820e-03;0.998141;1.859279e-03;\t test:1.740648e-03;0.998118;1.795335e-03\n",
    "epoch: 98;\t train:1.952922e-03;0.998145;1.848978e-03;\t test:1.743723e-03;0.998152;1.795088e-03\n",
    "epoch: 99;\t train:1.952288e-03;0.998145;1.846168e-03;\t test:1.727805e-03;0.998140;1.773551e-03\n",
    "\n",
    "# attached\n",
    "epoch:100;\t train:1.945540e-03;0.998133;1.852406e-03;\t test:1.709062e-03;0.998148;1.779730e-03\n",
    "epoch:101;\t train:1.928352e-03;0.998151;1.840944e-03;\t test:1.703641e-03;0.998135;1.774058e-03\n",
    "epoch:102;\t train:1.923391e-03;0.998158;1.837315e-03;\t test:1.698818e-03;0.998172;1.770586e-03\n",
    "epoch:103;\t train:1.926474e-03;0.998154;1.841268e-03;\t test:1.684855e-03;0.998211;1.763347e-03\n",
    "epoch:104;\t train:1.922539e-03;0.998151;1.841850e-03;\t test:1.702389e-03;0.998168;1.789704e-03\n",
    "epoch:105;\t train:1.921878e-03;0.998153;1.839671e-03;\t test:1.652292e-03;0.998249;1.727045e-03\n",
    "epoch:106;\t train:1.911147e-03;0.998161;1.832879e-03;\t test:1.698744e-03;0.998189;1.779990e-03\n",
    "epoch:107;\t train:1.908664e-03;0.998165;1.830717e-03;\t test:1.669434e-03;0.998220;1.746677e-03\n",
    "epoch:108;\t train:1.903686e-03;0.998163;1.830302e-03;\t test:1.655598e-03;0.998210;1.741336e-03\n",
    "epoch:109;\t train:1.910479e-03;0.998153;1.837731e-03;\t test:1.649060e-03;0.998221;1.730038e-03\n",
    "epoch:110;\t train:1.907956e-03;0.998159;1.834564e-03;\t test:1.661483e-03;0.998208;1.756296e-03\n",
    "epoch:111;\t train:1.891809e-03;0.998172;1.823435e-03;\t test:1.648709e-03;0.998239;1.725627e-03\n",
    "epoch:112;\t train:1.893809e-03;0.998168;1.824061e-03;\t test:1.638480e-03;0.998254;1.728638e-03\n",
    "epoch:113;\t train:1.907290e-03;0.998157;1.836647e-03;\t test:1.657767e-03;0.998191;1.738182e-03\n",
    "epoch:114;\t train:1.893390e-03;0.998162;1.826805e-03;\t test:1.652062e-03;0.998241;1.738502e-03\n",
    "epoch:115;\t train:1.905292e-03;0.998160;1.834395e-03;\t test:1.655677e-03;0.998227;1.749889e-03\n",
    "epoch:116;\t train:1.878638e-03;0.998172;1.818069e-03;\t test:1.607683e-03;0.998257;1.703202e-03\n",
    "epoch:117;\t train:1.877312e-03;0.998171;1.816736e-03;\t test:1.626392e-03;0.998239;1.722178e-03\n",
    "epoch:118;\t train:1.869370e-03;0.998176;1.807410e-03;\t test:1.611863e-03;0.998269;1.709355e-03\n",
    "epoch:119;\t train:1.886540e-03;0.998158;1.831403e-03;\t test:1.620552e-03;0.998244;1.715972e-03\n",
    "epoch:120;\t train:1.869290e-03;0.998170;1.809815e-03;\t test:1.629003e-03;0.998261;1.718689e-03\n",
    "epoch:121;\t train:1.862726e-03;0.998183;1.801965e-03;\t test:1.620778e-03;0.998264;1.721264e-03\n",
    "epoch:122;\t train:1.871497e-03;0.998171;1.814205e-03;\t test:1.647933e-03;0.998232;1.754014e-03\n",
    "epoch:123;\t train:1.871373e-03;0.998167;1.815408e-03;\t test:1.612605e-03;0.998255;1.718749e-03\n",
    "epoch:124;\t train:1.862061e-03;0.998170;1.808686e-03;\t test:1.591241e-03;0.998286;1.695825e-03\n",
    "epoch:125;\t train:1.859422e-03;0.998179;1.805901e-03;\t test:1.595947e-03;0.998265;1.699939e-03\n",
    "epoch:126;\t train:1.860078e-03;0.998167;1.807325e-03;\t test:1.619341e-03;0.998260;1.717776e-03\n",
    "epoch:127;\t train:1.855137e-03;0.998175;1.805826e-03;\t test:1.585562e-03;0.998288;1.680556e-03\n",
    "epoch:128;\t train:1.845960e-03;0.998181;1.796402e-03;\t test:1.602106e-03;0.998269;1.704652e-03\n",
    "epoch:129;\t train:1.851627e-03;0.998174;1.802243e-03;\t test:1.610194e-03;0.998245;1.715494e-03\n",
    "epoch:130;\t train:1.858079e-03;0.998165;1.813958e-03;\t test:1.604254e-03;0.998257;1.720182e-03\n",
    "epoch:131;\t train:1.844704e-03;0.998178;1.795411e-03;\t test:1.594463e-03;0.998261;1.702140e-03\n",
    "epoch:132;\t train:1.839700e-03;0.998177;1.795320e-03;\t test:1.600735e-03;0.998259;1.710783e-03\n",
    "epoch:133;\t train:1.839298e-03;0.998178;1.795933e-03;\t test:1.572245e-03;0.998268;1.678756e-03\n",
    "epoch:134;\t train:1.851683e-03;0.998163;1.808439e-03;\t test:1.575930e-03;0.998288;1.678384e-03\n",
    "epoch:135;\t train:1.837295e-03;0.998180;1.795895e-03;\t test:1.595881e-03;0.998259;1.701325e-03\n",
    "epoch:136;\t train:1.836549e-03;0.998178;1.794393e-03;\t test:1.559337e-03;0.998316;1.664402e-03\n",
    "epoch:137;\t train:1.830292e-03;0.998183;1.789395e-03;\t test:1.591588e-03;0.998270;1.698390e-03\n",
    "epoch:138;\t train:1.821879e-03;0.998186;1.784486e-03;\t test:1.579662e-03;0.998293;1.677771e-03\n",
    "epoch:139;\t train:1.818268e-03;0.998192;1.776288e-03;\t test:1.581341e-03;0.998269;1.695885e-03\n",
    "epoch:140;\t train:1.820522e-03;0.998188;1.781994e-03;\t test:1.561109e-03;0.998294;1.668083e-03\n",
    "epoch:141;\t train:1.816395e-03;0.998184;1.781030e-03;\t test:1.554535e-03;0.998301;1.665535e-03\n",
    "epoch:142;\t train:1.828224e-03;0.998182;1.787939e-03;\t test:1.583781e-03;0.998266;1.686883e-03\n",
    "epoch:143;\t train:1.814807e-03;0.998191;1.778269e-03;\t test:1.573407e-03;0.998273;1.690703e-03\n",
    "epoch:144;\t train:1.803595e-03;0.998195;1.771850e-03;\t test:1.573454e-03;0.998257;1.685382e-03\n",
    "epoch:145;\t train:1.811718e-03;0.998191;1.774695e-03;\t test:1.555627e-03;0.998288;1.672068e-03\n",
    "epoch:146;\t train:1.808300e-03;0.998185;1.778566e-03;\t test:1.584553e-03;0.998239;1.699656e-03\n",
    "epoch:147;\t train:1.808551e-03;0.998184;1.777183e-03;\t test:1.551427e-03;0.998293;1.657250e-03\n",
    "epoch:148;\t train:1.807341e-03;0.998182;1.776618e-03;\t test:1.551719e-03;0.998310;1.661716e-03\n",
    "epoch:149;\t train:1.811933e-03;0.998181;1.786461e-03;\t test:1.591109e-03;0.998235;1.712884e-03\n",
    "epoch:150;\t train:1.797299e-03;0.998191;1.768630e-03;\t test:1.573613e-03;0.998270;1.698108e-03\n",
    "epoch:151;\t train:1.804654e-03;0.998188;1.775825e-03;\t test:1.548662e-03;0.998298;1.660921e-03\n",
    "epoch:152;\t train:1.813078e-03;0.998176;1.786178e-03;\t test:1.556197e-03;0.998264;1.667133e-03\n",
    "epoch:153;\t train:1.796383e-03;0.998194;1.765493e-03;\t test:1.568297e-03;0.998276;1.684297e-03\n",
    "epoch:154;\t train:1.786185e-03;0.998202;1.758198e-03;\t test:1.560950e-03;0.998285;1.674123e-03\n",
    "epoch:155;\t train:1.800627e-03;0.998186;1.776039e-03;\t test:1.553530e-03;0.998269;1.678157e-03\n",
    "epoch:156;\t train:1.786341e-03;0.998200;1.759664e-03;\t test:1.531426e-03;0.998292;1.644130e-03\n",
    "epoch:157;\t train:1.796138e-03;0.998187;1.774755e-03;\t test:1.537272e-03;0.998285;1.655228e-03\n",
    "epoch:158;\t train:1.787906e-03;0.998193;1.765643e-03;\t test:1.550466e-03;0.998274;1.672952e-03\n",
    "epoch:159;\t train:1.778393e-03;0.998204;1.753414e-03;\t test:1.552242e-03;0.998269;1.677123e-03\n",
    "epoch:160;\t train:1.783622e-03;0.998192;1.762056e-03;\t test:1.566988e-03;0.998259;1.691016e-03\n",
    "epoch:161;\t train:1.786314e-03;0.998185;1.768122e-03;\t test:1.550775e-03;0.998273;1.678410e-03\n",
    "epoch:162;\t train:1.772154e-03;0.998197;1.755225e-03;\t test:1.577360e-03;0.998254;1.699081e-03\n",
    "epoch:163;\t train:1.775477e-03;0.998203;1.753242e-03;\t test:1.545798e-03;0.998276;1.674088e-03\n",
    "epoch:164;\t train:1.784540e-03;0.998193;1.765002e-03;\t test:1.536275e-03;0.998269;1.655255e-03\n",
    "epoch:165;\t train:1.784266e-03;0.998183;1.768547e-03;\t test:1.522119e-03;0.998285;1.643926e-03\n",
    "epoch:166;\t train:1.777216e-03;0.998195;1.762796e-03;\t test:1.543966e-03;0.998280;1.663503e-03\n",
    "epoch:167;\t train:1.783529e-03;0.998188;1.767834e-03;\t test:1.524646e-03;0.998294;1.650707e-03\n",
    "epoch:168;\t train:1.778943e-03;0.998188;1.764825e-03;\t test:1.541891e-03;0.998274;1.664232e-03\n",
    "epoch:169;\t train:1.776793e-03;0.998189;1.762680e-03;\t test:1.528585e-03;0.998294;1.654274e-03\n",
    "epoch:170;\t train:1.766728e-03;0.998199;1.755008e-03;\t test:1.555720e-03;0.998248;1.685892e-03\n",
    "epoch:171;\t train:1.772209e-03;0.998194;1.759329e-03;\t test:1.521299e-03;0.998284;1.647604e-03\n",
    "epoch:172;\t train:1.760014e-03;0.998206;1.746630e-03;\t test:1.517720e-03;0.998282;1.636467e-03\n",
    "epoch:173;\t train:1.760490e-03;0.998203;1.751063e-03;\t test:1.505749e-03;0.998327;1.623275e-03\n",
    "epoch:174;\t train:1.762421e-03;0.998206;1.747649e-03;\t test:1.519936e-03;0.998294;1.648789e-03\n",
    "epoch:175;\t train:1.754200e-03;0.998203;1.744851e-03;\t test:1.520666e-03;0.998289;1.652629e-03\n",
    "epoch:176;\t train:1.768720e-03;0.998193;1.759902e-03;\t test:1.505334e-03;0.998307;1.636338e-03\n",
    "epoch:177;\t train:1.758681e-03;0.998202;1.747964e-03;\t test:1.534903e-03;0.998265;1.665093e-03\n",
    "epoch:178;\t train:1.750731e-03;0.998204;1.744141e-03;\t test:1.517998e-03;0.998298;1.647741e-03\n",
    "epoch:179;\t train:1.754090e-03;0.998206;1.745233e-03;\t test:1.522601e-03;0.998294;1.645049e-03\n",
    "epoch:180;\t train:1.765455e-03;0.998187;1.763317e-03;\t test:1.507510e-03;0.998304;1.632789e-03\n",
    "epoch:181;\t train:1.752694e-03;0.998201;1.747817e-03;\t test:1.525849e-03;0.998279;1.653258e-03\n",
    "epoch:182;\t train:1.754633e-03;0.998204;1.745969e-03;\t test:1.505693e-03;0.998310;1.633687e-03\n",
    "epoch:183;\t train:1.751081e-03;0.998200;1.746061e-03;\t test:1.519494e-03;0.998299;1.647019e-03\n",
    "epoch:184;\t train:1.737531e-03;0.998215;1.733695e-03;\t test:1.518300e-03;0.998285;1.652718e-03\n",
    "epoch:185;\t train:1.747280e-03;0.998210;1.740231e-03;\t test:1.510267e-03;0.998285;1.642559e-03\n",
    "epoch:186;\t train:1.751199e-03;0.998197;1.753699e-03;\t test:1.502631e-03;0.998311;1.634803e-03\n",
    "epoch:187;\t train:1.748265e-03;0.998204;1.744746e-03;\t test:1.500889e-03;0.998294;1.630127e-03\n",
    "epoch:188;\t train:1.739974e-03;0.998202;1.743123e-03;\t test:1.492181e-03;0.998305;1.620693e-03\n",
    "epoch:189;\t train:1.729117e-03;0.998219;1.729045e-03;\t test:1.509370e-03;0.998299;1.639974e-03\n",
    "epoch:190;\t train:1.733879e-03;0.998218;1.730574e-03;\t test:1.504636e-03;0.998307;1.630513e-03\n",
    "epoch:191;\t train:1.741160e-03;0.998206;1.744102e-03;\t test:1.497866e-03;0.998291;1.617307e-03\n",
    "epoch:192;\t train:1.738206e-03;0.998203;1.739315e-03;\t test:1.485287e-03;0.998313;1.618900e-03\n",
    "epoch:193;\t train:1.734702e-03;0.998207;1.733849e-03;\t test:1.519703e-03;0.998268;1.660137e-03\n",
    "epoch:194;\t train:1.732679e-03;0.998206;1.735451e-03;\t test:1.498075e-03;0.998292;1.635249e-03\n",
    "epoch:195;\t train:1.729843e-03;0.998213;1.732070e-03;\t test:1.501108e-03;0.998303;1.634294e-03\n",
    "epoch:196;\t train:1.729585e-03;0.998212;1.734648e-03;\t test:1.479053e-03;0.998326;1.603968e-03\n",
    "epoch:197;\t train:1.736361e-03;0.998205;1.742472e-03;\t test:1.501012e-03;0.998295;1.639826e-03\n",
    "epoch:198;\t train:1.750149e-03;0.998182;1.757979e-03;\t test:1.525166e-03;0.998296;1.658272e-03\n",
    "epoch:199;\t train:1.729645e-03;0.998206;1.735098e-03;\t test:1.523029e-03;0.998284;1.658579e-03\n",
    "epoch:200;\t train:1.723338e-03;0.998214;1.729133e-03;\t test:1.486910e-03;0.998307;1.620159e-03\n",
    "epoch:201;\t train:1.723366e-03;0.998216;1.730009e-03;\t test:1.486352e-03;0.998325;1.620656e-03\n",
    "epoch:202;\t train:1.721369e-03;0.998213;1.728993e-03;\t test:1.493970e-03;0.998306;1.631652e-03\n",
    "epoch:203;\t train:1.712719e-03;0.998222;1.719575e-03;\t test:1.503543e-03;0.998296;1.634319e-03\n",
    "epoch:204;\t train:1.715297e-03;0.998218;1.722153e-03;\t test:1.484073e-03;0.998317;1.616644e-03\n",
    "epoch:205;\t train:1.719271e-03;0.998215;1.730536e-03;\t test:1.500838e-03;0.998289;1.636281e-03\n",
    "epoch:206;\t train:1.712577e-03;0.998219;1.722264e-03;\t test:1.478563e-03;0.998323;1.613252e-03\n",
    "epoch:207;\t train:1.718518e-03;0.998209;1.729611e-03;\t test:1.503982e-03;0.998304;1.638553e-03\n",
    "epoch:208;\t train:1.711803e-03;0.998219;1.724238e-03;\t test:1.466368e-03;0.998321;1.598367e-03\n",
    "epoch:209;\t train:1.711383e-03;0.998220;1.718973e-03;\t test:1.495988e-03;0.998299;1.631474e-03\n",
    "epoch:210;\t train:1.706100e-03;0.998218;1.718392e-03;\t test:1.479922e-03;0.998314;1.617359e-03\n",
    "epoch:211;\t train:1.712957e-03;0.998218;1.724028e-03;\t test:1.483145e-03;0.998302;1.620934e-03\n",
    "epoch:212;\t train:1.714396e-03;0.998209;1.729006e-03;\t test:1.472519e-03;0.998328;1.600948e-03\n",
    "epoch:213;\t train:1.717080e-03;0.998209;1.730701e-03;\t test:1.483689e-03;0.998314;1.620427e-03\n",
    "epoch:214;\t train:1.700631e-03;0.998222;1.715381e-03;\t test:1.468275e-03;0.998329;1.598542e-03\n",
    "epoch:215;\t train:1.713823e-03;0.998214;1.730006e-03;\t test:1.478936e-03;0.998312;1.621438e-03\n",
    "epoch:216;\t train:1.702846e-03;0.998216;1.721404e-03;\t test:1.486732e-03;0.998303;1.634319e-03\n",
    "epoch:217;\t train:1.709390e-03;0.998214;1.723455e-03;\t test:1.443195e-03;0.998354;1.569572e-03\n",
    "epoch:218;\t train:1.694295e-03;0.998230;1.707724e-03;\t test:1.481961e-03;0.998304;1.624121e-03\n",
    "epoch:219;\t train:1.700916e-03;0.998225;1.714284e-03;\t test:1.469205e-03;0.998316;1.606839e-03\n",
    "epoch:220;\t train:1.698523e-03;0.998227;1.712610e-03;\t test:1.466880e-03;0.998322;1.599742e-03\n",
    "epoch:221;\t train:1.704598e-03;0.998215;1.724769e-03;\t test:1.463260e-03;0.998325;1.595798e-03\n",
    "epoch:222;\t train:1.702041e-03;0.998219;1.718294e-03;\t test:1.475872e-03;0.998312;1.614253e-03\n",
    "epoch:223;\t train:1.703368e-03;0.998219;1.721371e-03;\t test:1.466058e-03;0.998324;1.611658e-03\n",
    "epoch:224;\t train:1.690677e-03;0.998227;1.709060e-03;\t test:1.482623e-03;0.998306;1.616089e-03\n",
    "epoch:225;\t train:1.692243e-03;0.998228;1.706712e-03;\t test:1.451196e-03;0.998332;1.595873e-03\n",
    "epoch:226;\t train:1.695380e-03;0.998224;1.714617e-03;\t test:1.457066e-03;0.998339;1.590118e-03\n",
    "epoch:227;\t train:1.699871e-03;0.998220;1.716440e-03;\t test:1.463486e-03;0.998313;1.596968e-03\n",
    "epoch:228;\t train:1.679091e-03;0.998236;1.699372e-03;\t test:1.468233e-03;0.998319;1.614644e-03\n",
    "epoch:229;\t train:1.688893e-03;0.998223;1.712977e-03;\t test:1.480426e-03;0.998322;1.624614e-03\n",
    "epoch:230;\t train:1.692175e-03;0.998224;1.713310e-03;\t test:1.464084e-03;0.998319;1.607331e-03\n",
    "epoch:231;\t train:1.683488e-03;0.998231;1.704362e-03;\t test:1.454602e-03;0.998333;1.588746e-03\n",
    "epoch:232;\t train:1.687334e-03;0.998223;1.711938e-03;\t test:1.448516e-03;0.998347;1.583391e-03\n",
    "epoch:233;\t train:1.683772e-03;0.998233;1.706239e-03;\t test:1.457772e-03;0.998322;1.600801e-03\n",
    "epoch:234;\t train:1.679059e-03;0.998227;1.705806e-03;\t test:1.456034e-03;0.998331;1.591426e-03\n",
    "epoch:235;\t train:1.688384e-03;0.998224;1.713709e-03;\t test:1.461803e-03;0.998300;1.602715e-03\n",
    "epoch:236;\t train:1.678069e-03;0.998234;1.700433e-03;\t test:1.474535e-03;0.998298;1.619516e-03\n",
    "epoch:237;\t train:1.684684e-03;0.998225;1.709714e-03;\t test:1.449240e-03;0.998347;1.583763e-03\n",
    "epoch:238;\t train:1.677466e-03;0.998228;1.701171e-03;\t test:1.463571e-03;0.998309;1.601898e-03\n",
    "epoch:239;\t train:1.679555e-03;0.998228;1.706706e-03;\t test:1.448777e-03;0.998315;1.590767e-03\n",
    "epoch:240;\t train:1.678567e-03;0.998227;1.705981e-03;\t test:1.453264e-03;0.998321;1.592681e-03\n",
    "epoch:241;\t train:1.676959e-03;0.998232;1.701424e-03;\t test:1.454557e-03;0.998334;1.591647e-03\n",
    "epoch:242;\t train:1.679115e-03;0.998232;1.705054e-03;\t test:1.441662e-03;0.998350;1.577487e-03\n",
    "epoch:243;\t train:1.690581e-03;0.998217;1.714399e-03;\t test:1.451531e-03;0.998322;1.599585e-03\n",
    "epoch:244;\t train:1.677974e-03;0.998232;1.706434e-03;\t test:1.444561e-03;0.998319;1.587290e-03\n",
    "epoch:245;\t train:1.683709e-03;0.998219;1.712117e-03;\t test:1.429527e-03;0.998345;1.561798e-03\n",
    "epoch:246;\t train:1.677374e-03;0.998229;1.703959e-03;\t test:1.435834e-03;0.998359;1.564646e-03\n",
    "epoch:247;\t train:1.667531e-03;0.998233;1.697009e-03;\t test:1.450893e-03;0.998338;1.594639e-03\n",
    "epoch:248;\t train:1.675073e-03;0.998234;1.701676e-03;\t test:1.437212e-03;0.998322;1.573733e-03\n",
    "epoch:249;\t train:1.672176e-03;0.998236;1.697988e-03;\t test:1.449092e-03;0.998331;1.589870e-03\n",
    "epoch:250;\t train:1.669396e-03;0.998230;1.702740e-03;\t test:1.453659e-03;0.998333;1.597864e-03\n",
    "epoch:251;\t train:1.665555e-03;0.998237;1.693435e-03;\t test:1.444478e-03;0.998354;1.584418e-03\n",
    "epoch:252;\t train:1.661339e-03;0.998238;1.692776e-03;\t test:1.430212e-03;0.998356;1.572120e-03\n",
    "epoch:253;\t train:1.657710e-03;0.998243;1.690354e-03;\t test:1.451867e-03;0.998334;1.591867e-03\n",
    "epoch:254;\t train:1.671713e-03;0.998238;1.701711e-03;\t test:1.438614e-03;0.998331;1.583067e-03\n",
    "epoch:255;\t train:1.658817e-03;0.998237;1.693744e-03;\t test:1.458111e-03;0.998312;1.604751e-03\n",
    "epoch:256;\t train:1.662301e-03;0.998236;1.697157e-03;\t test:1.427051e-03;0.998347;1.574362e-03\n",
    "epoch:257;\t train:1.660937e-03;0.998241;1.689705e-03;\t test:1.438953e-03;0.998323;1.584600e-03\n",
    "epoch:258;\t train:1.664787e-03;0.998230;1.699983e-03;\t test:1.426664e-03;0.998358;1.566249e-03\n",
    "epoch:259;\t train:1.652828e-03;0.998241;1.686037e-03;\t test:1.406809e-03;0.998362;1.548376e-03\n",
    "epoch:260;\t train:1.657538e-03;0.998233;1.695781e-03;\t test:1.441713e-03;0.998330;1.587521e-03\n",
    "epoch:261;\t train:1.653766e-03;0.998239;1.689579e-03;\t test:1.419708e-03;0.998362;1.562929e-03\n",
    "epoch:262;\t train:1.657747e-03;0.998232;1.692516e-03;\t test:1.410530e-03;0.998355;1.553655e-03\n",
    "epoch:263;\t train:1.649204e-03;0.998241;1.685323e-03;\t test:1.438873e-03;0.998334;1.584378e-03\n",
    "epoch:264;\t train:1.658762e-03;0.998234;1.693504e-03;\t test:1.425825e-03;0.998348;1.575226e-03\n",
    "epoch:265;\t train:1.657098e-03;0.998236;1.692496e-03;\t test:1.435764e-03;0.998348;1.580022e-03\n",
    "epoch:266;\t train:1.652565e-03;0.998240;1.688612e-03;\t test:1.422508e-03;0.998344;1.563107e-03\n",
    "epoch:267;\t train:1.650305e-03;0.998240;1.688892e-03;\t test:1.431926e-03;0.998350;1.573391e-03\n",
    "epoch:268;\t train:1.643645e-03;0.998247;1.680590e-03;\t test:1.429333e-03;0.998322;1.577480e-03\n",
    "epoch:269;\t train:1.650631e-03;0.998239;1.688128e-03;\t test:1.417393e-03;0.998347;1.562594e-03\n",
    "epoch:270;\t train:1.636563e-03;0.998253;1.673308e-03;\t test:1.420961e-03;0.998356;1.562865e-03\n",
    "epoch:271;\t train:1.640995e-03;0.998250;1.677382e-03;\t test:1.448978e-03;0.998321;1.596266e-03\n",
    "epoch:272;\t train:1.645745e-03;0.998245;1.684626e-03;\t test:1.438713e-03;0.998326;1.585455e-03\n",
    "epoch:273;\t train:1.645963e-03;0.998245;1.682622e-03;\t test:1.422199e-03;0.998339;1.569791e-03\n",
    "epoch:274;\t train:1.637135e-03;0.998247;1.675520e-03;\t test:1.417566e-03;0.998348;1.565101e-03\n",
    "epoch:275;\t train:1.645756e-03;0.998241;1.685950e-03;\t test:1.419482e-03;0.998356;1.570535e-03\n",
    "epoch:276;\t train:1.638202e-03;0.998241;1.682525e-03;\t test:1.405921e-03;0.998349;1.550824e-03\n",
    "epoch:277;\t train:1.644346e-03;0.998243;1.685995e-03;\t test:1.406805e-03;0.998371;1.554122e-03\n",
    "epoch:278;\t train:1.640084e-03;0.998242;1.681634e-03;\t test:1.420676e-03;0.998352;1.564897e-03\n",
    "epoch:279;\t train:1.638201e-03;0.998251;1.681708e-03;\t test:1.437352e-03;0.998339;1.590171e-03\n",
    "epoch:280;\t train:1.630737e-03;0.998254;1.672497e-03;\t test:1.432668e-03;0.998339;1.581328e-03\n",
    "epoch:281;\t train:1.641301e-03;0.998246;1.681769e-03;\t test:1.401187e-03;0.998372;1.537706e-03\n",
    "epoch:282;\t train:1.635377e-03;0.998243;1.680307e-03;\t test:1.422268e-03;0.998331;1.567054e-03\n",
    "epoch:283;\t train:1.635243e-03;0.998254;1.674266e-03;\t test:1.432003e-03;0.998337;1.579446e-03\n",
    "epoch:284;\t train:1.631299e-03;0.998248;1.673741e-03;\t test:1.424213e-03;0.998342;1.574320e-03\n",
    "epoch:285;\t train:1.627356e-03;0.998249;1.672364e-03;\t test:1.417369e-03;0.998334;1.563156e-03\n",
    "epoch:286;\t train:1.635396e-03;0.998248;1.677819e-03;\t test:1.412340e-03;0.998355;1.563764e-03\n",
    "epoch:287;\t train:1.630666e-03;0.998245;1.676988e-03;\t test:1.411512e-03;0.998361;1.556500e-03\n",
    "epoch:288;\t train:1.644201e-03;0.998229;1.693589e-03;\t test:1.418285e-03;0.998342;1.561955e-03\n",
    "epoch:289;\t train:1.631479e-03;0.998247;1.674546e-03;\t test:1.398462e-03;0.998354;1.550139e-03\n",
    "epoch:290;\t train:1.639591e-03;0.998236;1.688160e-03;\t test:1.406701e-03;0.998342;1.554558e-03\n",
    "epoch:291;\t train:1.623646e-03;0.998254;1.666319e-03;\t test:1.390682e-03;0.998374;1.536456e-03\n",
    "epoch:292;\t train:1.629442e-03;0.998245;1.676500e-03;\t test:1.393333e-03;0.998369;1.539018e-03\n",
    "epoch:293;\t train:1.628091e-03;0.998251;1.675390e-03;\t test:1.405844e-03;0.998346;1.557259e-03\n",
    "epoch:294;\t train:1.631534e-03;0.998242;1.674700e-03;\t test:1.401640e-03;0.998357;1.542579e-03\n",
    "epoch:295;\t train:1.626105e-03;0.998247;1.671092e-03;\t test:1.397843e-03;0.998365;1.547492e-03\n",
    "epoch:296;\t train:1.620808e-03;0.998259;1.667938e-03;\t test:1.421335e-03;0.998337;1.572542e-03\n",
    "epoch:297;\t train:1.620527e-03;0.998251;1.667518e-03;\t test:1.400187e-03;0.998333;1.538090e-03\n",
    "epoch:298;\t train:1.615565e-03;0.998258;1.661521e-03;\t test:1.401611e-03;0.998342;1.559463e-03\n",
    "epoch:299;\t train:1.617638e-03;0.998249;1.664530e-03;\t test:1.399007e-03;0.998351;1.549592e-03\n",
    "epoch:300;\t train:1.615109e-03;0.998255;1.664476e-03;\t test:1.386487e-03;0.998371;1.536563e-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = test_dataset[0]\n",
    "test_image2, test_label2 = train_transform(test_image, test_label)\n",
    "test_image2 = nd.expand_dims(test_image2,0)\n",
    "print('tensor shape:', test_image2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors, cls_preds, box_preds = net(test_image2.as_in_context(ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert predictions to real object detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.contrib.ndarray import MultiBoxDetection\n",
    "cls_probs = nd.SoftmaxActivation(nd.transpose(cls_preds, (0, 2, 1)), mode='channel')\n",
    "output = MultiBoxDetection(cls_prob=cls_probs, loc_pred=box_preds, anchor=anchors, force_suppress=True, clip=True, nms_topk=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ('cluster')\n",
    "def display(img, out, thresh=0.5):\n",
    "    import random\n",
    "    import matplotlib as mpl\n",
    "    import numpy as np\n",
    "    mpl.rcParams['figure.figsize'] = (10,10)\n",
    "    img = img.asnumpy()\n",
    "    img = np.transpose(img,(2,3,1,0))\n",
    "    img = np.squeeze(img)\n",
    "    plt.clf()\n",
    "    plt.imshow(img)\n",
    "    for det in out:\n",
    "        cid = int(det[0])\n",
    "        if cid == 0:\n",
    "            continue\n",
    "        score = det[1]\n",
    "        if score < thresh:\n",
    "            continue\n",
    "        scales = [img.shape[1], img.shape[0]] * 2\n",
    "        xmin, ymin, xmax, ymax = [int(p * s) for p, s in zip(det[2:6].tolist(), scales)]\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False,\n",
    "                             edgecolor='red', linewidth=3)\n",
    "        plt.gca().add_patch(rect)\n",
    "        text = class_names[cid]\n",
    "        plt.gca().text(xmin, ymin-2, '{:s} {:.3f}'.format(text, score),\n",
    "                       bbox=dict(facecolor='red', alpha=0.5),\n",
    "                       fontsize=12, color='white')\n",
    "\n",
    "display(test_image2, output[0].asnumpy(), thresh=0.52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
